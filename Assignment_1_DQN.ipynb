{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1: Deep Q-Networks (DQN) with [PufferLib Ocean Environments](https://puffer.ai/ocean.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*This assignment must be completed solely by the members of your group. Sharing of code between groups is strictly prohibited. However, you are allowed to discuss general solution approaches or share publicly available resources with members of other groups. Therefore, clearly indicate which public resources you consulted and/or copied code from. Any plagiarism between groups will result in the initiation of a fraud procedure with the director of education.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "In this notebook, your group will implement a fully vectorized Deep Q-Network (DQN) agent and several of its key improvements, inspired by the [\"Rainbow\"](https://arxiv.org/abs/1710.02298) paper. The project is divided into a foundational group task followed by individual implementation of specific DQN extensions. Your goal is to compare the performance of these components and then combine them into the ultimate rainbow-lite agent.\n",
    "\n",
    "### Project Structure:\n",
    "1.  **Foundational (Group Task):** The entire group will first collaborate to right the necessary code to  make simple DQN work.\n",
    "2.  **Individual Extensions:** Once the baseline is complete, each group member will be assigned one of the following algorithmic extensions to implement:\n",
    "    *   **Extension A:** Double DQN & Dueling DQN (Architectural Improvements)\n",
    "    *   **Extension B:** Prioritized Experience Replay (PER) (Advanced Sampling)\n",
    "    *   **Extension C:** N-Step Returns & Noisy Networks (Target & Exploration Improvements)\n",
    "3.  **Analysis (Group Task):** The group will integrate all components, run a comparative analysis on the `breakout` environment, and collaboratively answer the conceptual and reflection questions in a final report.\n",
    "\n",
    "Cells with `TODO` indicate where you must add or adjust code. However feel free to modify any part of the code to improve clarity, efficiency, or performance. You are encouraged to experiment with hyperparameters and other design choices to optimize your agent's learning.\n",
    "\n",
    "---\n",
    "\n",
    "### Environment Tiers:\n",
    "You will use three types of environments from `pufferlib.ocean`:\n",
    "1.  **Debug:** `squared` (fast iterations; verify your code  is correct, your agent should learn within seconds and become optimal around 2 minutes, max episode score is 1).\n",
    "2.  **Comparing:** `cartpole` (fast iterations; verify your extensions and evaluate the agent, learning signs should be clear within 2 minutes but optimal agent could take 30+ minutes, max episode score is 199).\n",
    "3. **Challenge:** `breakout` (focus on performance, and diagnostics, an optimal agent can take multiple hours, you will get bonus points if you find an optimal agent in this environment, max episode score is 864).\n",
    "\n",
    "You are allowed to experiment with other environments from `pufferlib.ocean` if you wish, but the above three are mandatory.\n",
    "\n",
    "---\n",
    "\n",
    "### Report\n",
    "We expect you to write the report in a separate document and submit both the notebook and the report on Ufora. Remember that a plot often tells more than a thousand words. When you explain somethings or you show your results, try to add figures to accompany the text. Go beyond just souly describing what you did or how the techniques work but additionally, share your observations, reflect on why things turned out the way they did, and help us understand the story behind your findings.\n",
    "\n",
    "The report has a soft limit of 8 pages. You are welcome to go over this limit if you keep your writing concise and make sure any extra content is relevant. If you are highly motivated and want to test many different things, feel free to share your findings as long as you follow the requirements mentioned earlier.\n",
    "\n",
    "If your group is smaller than 3 people, you can choose to implement more than one extension per person. In that case, please clearly indicate who did what in the report.\n",
    "\n",
    "**Deadline:** October 26, 2025, 23:59\n",
    "\n",
    "\n",
    "### Office hours\n",
    "We will hold weekly office hours to help you with questions about the assignment. Your are more then welcome between 13:30 and 16:00. We reserved IDLab9 (IGent) for you. If we are not there, you can find us in our offices on the 10th floor (IGent): 200.026 (Elias and Thibault), 200.031 (Ciem).\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### Docker\n",
    "To ensure a consistent and hassle-free setup, we have prepared a Docker image with all necessary dependencies pre-installed. This is the recommended way to run the notebook to avoid issues with package versions.\n",
    "\n",
    "You can also use there container more information at [puffertank](https://github.com/PufferAI/PufferTank).\n",
    "\n",
    "The Docker image is available on Docker Hub:\n",
    "*   **Image:** `ciemcornelissen/puffer-notebook:latest`\n",
    "*   **URL:** [https://hub.docker.com/r/ciemcornelissen/puffer-notebook](https://hub.docker.com/r/ciemcornelissen/puffer-notebook)\n",
    "\n",
    "You can use this image in several ways. Below are instructions for three common setups: VS Code (recommended for local use), Deepnote for a cloud-based environment, and locally with classic Jupyter.\n",
    "\n",
    "\n",
    "### Option 1: Local Development with VS Code\n",
    "\n",
    "This is the most seamless way to work locally. VS Code's \"Dev Containers\" extension allows you to open your project folder directly inside the running container, giving you access to a fully integrated terminal, file editor, and Jupyter renderer.\n",
    "\n",
    "**Prerequisites:**\n",
    "*   [Docker Desktop](https://www.docker.com/products/docker-desktop/) installed and running.\n",
    "*   [Visual Studio Code](https://code.visualstudio.com/) installed.\n",
    "*   The [Dev Containers extension](https://marketplace.visualstudio.com/items?itemName=ms-vscode-remote.remote-containers) installed in VS Code.\n",
    "\n",
    "**Instructions:**\n",
    "\n",
    "1.  **Pull and Run the Container:** Open a terminal in your project directory (where this notebook is located) and run the appropriate command below. **Keep this terminal window open.**\n",
    "\n",
    "    *   **For Linux, Windows (WSL), and Intel-based Macs:**\n",
    "        ```bash\n",
    "        docker run -it --rm -p 8888:8888 -v \"$(pwd)\":/app --name puffer-dev ciemcornelissen/puffer-notebook:latest\n",
    "\n",
    "    *   **For Apple Silicon (M1/M2/M3) Macs:** You must add the `--platform` flag to emulate the correct architecture.\n",
    "        ```bash\n",
    "        docker run -it --rm --platform linux/amd64 -p 8888:8888 -v \"$(pwd)\":/app --name puffer-dev ciemcornelissen/puffer-notebook:latest\n",
    "\n",
    "    *   *(Note: We use `--name puffer-dev` to give the container an easy-to-find name).*\n",
    "\n",
    "2.  **Attach VS Code to the Container:**\n",
    "    *   Open VS Code.\n",
    "    *   Attach vsual studio code to the running container:\n",
    "        1.  Click on the container extension in VS Code.\n",
    "        2.  Select **\"Attach to Running Container...\"**.\n",
    "        3.  Choose the container named `puffer-dev`.\n",
    "    *   Or open the Command Palette (`Ctrl+Shift+P` on Windows/Linux, `Cmd+Shift+P` on Mac).\n",
    "    *   Type and select **\"Dev Containers: Attach to Running Container...\"**.\n",
    "    *   Choose the right container from the list.\n",
    "\n",
    "\n",
    "3.  **Start Working:** A new VS Code window will open, connected to the container. Click **\"Open Folder\"** to open your project files. You can now edit code, run the notebook, and use the terminal as if you were running natively inside the correct environment.\n",
    "\n",
    "**If you can not select a kernel when trying to run code in the notebook then you need to update the jupyter extension of vsc. This can be done by clicking on the extensions tab on the left and searching for jupyter. Then click on the little gear icon and select install a specific version and choose the newest version.**\n",
    "\n",
    "### Option 2: Cloud Development with [Deepnote](https://deepnote.com/)\n",
    "\n",
    "If you prefer not to install Docker locally, you can use Deepnote to run the environment in the cloud.\n",
    "\n",
    "**Instructions:**\n",
    "\n",
    "1.  In your Deepnote project, navigate to the **Environment** tab in the left sidebar at the bottem under machine.\n",
    "2.  Click on the **\"Set up a new Docker image\"**.\n",
    "3.  In the \"Docker image\" field, paste the image name:\n",
    "    ```\n",
    "    ciemcornelissen/puffer-notebook:latest\n",
    "\n",
    "4.  Deepnote will pull the image. Once it's ready, you'll be ready to work.\n",
    "\n",
    "\n",
    "\n",
    "### Option 3: Local Development with Classic Jupyter\n",
    "\n",
    "This method uses the command line to start a Jupyter server, which you access through your web browser.\n",
    "\n",
    "**Instructions:**\n",
    "\n",
    "1.  **Pull the Image:**\n",
    "    ```bash\n",
    "    docker pull ciemcornelissen/puffer-notebook:latest\n",
    "\n",
    "2.  **Run the Container:** Run the command below from your project directory.\n",
    "\n",
    "    *   **For Linux, Windows (WSL), and Intel-based Macs:**\n",
    "        ```bash\n",
    "        docker run -it --rm -p 8888:8888 -v \"$(pwd)\":/app ciemcornelissen/puffer-notebook:latest\n",
    "\n",
    "    *   **For Apple Silicon (M1/M2/M3) Macs:**\n",
    "        ```bash\n",
    "        docker run -it --rm --platform linux/amd64 -p 8888:8888 -v \"$(pwd)\":/app ciemcornelissen/puffer-notebook:latest\n",
    "\n",
    "3.  **Access Jupyter:** Your terminal will display a URL (e.g., `http://127.0.0.1:8888/lab?token=...`). Copy and paste this full URL into your web browser to start the notebook. Your files will be in the `/app` directory.\n",
    "\n",
    "\n",
    "\n",
    "### **Important Note for Apple Silicon (M1/M2/M3/M4) Users**\n",
    "\n",
    "The Docker image is built for the `amd64` (Intel/AMD) architecture. If you are using a Mac with Apple Silicon, you must tell Docker to emulate this architecture by adding the `--platform linux/amd64` flag.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#when using deepnote and you get the error no module pufferlib run this cell\n",
    "\n",
    "# try:\n",
    "#     import __editable___pufferlib_3_0_0_finder as _pf\n",
    "#     _pf.install()          # registers the module loader\n",
    "#     import pufferlib\n",
    "#     print(\"Loaded:\", pufferlib.__file__)\n",
    "# except Exception as e:\n",
    "#     print(\"Failed to manually activate editable hook:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "import random, dataclasses\n",
    "import math\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Optional, Dict, Any\n",
    "from collections import deque\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Import ocean environments, you can import more if you want to experiment with other envs\n",
    "from pufferlib.ocean.squared import squared\n",
    "from pufferlib.ocean.pong import pong\n",
    "from pufferlib.ocean.pacman import pacman\n",
    "from pufferlib.ocean.enduro import enduro\n",
    "from pufferlib.ocean.tetris import tetris\n",
    "from pufferlib.ocean.breakout import breakout\n",
    "from pufferlib.ocean.cartpole import cartpole\n",
    "\n",
    "import gymnasium as gym\n",
    "\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding PufferLib and Ocean Environments\n",
    "\n",
    "### What is [PufferLib](https://puffer.ai/ocean.html)?\n",
    "\n",
    "**PufferLib** is a high-performance reinforcement learning framework designed to bridge the gap between research and production RL. It was created to address common pain points in RL development:\n",
    "\n",
    "- **Speed:** PufferLib environments are highly optimized, often running 10-100x faster than traditional implementations\n",
    "- **Scalability:** Built-in support for massive parallelization across thousands of environments\n",
    "- **Simplicity:** Clean, minimal API that follows Gymnasium standards\n",
    "- **GPU-Native:** Environments can run directly on GPU, eliminating CPU-GPU transfer bottlenecks\n",
    "\n",
    "\n",
    "### PufferLib Ocean: Educational RL Environments\n",
    "\n",
    "**Ocean** is PufferLib's collection of lightweight, educational environments. The name reflects its purpose: a \"sea\" of diverse environments for learning and experimentation. Ocean environments are specifically designed for:\n",
    "\n",
    "1. **Fast Debugging:** Quickly verify your algorithm works before scaling up\n",
    "2. **Rapid Prototyping:** Test new ideas without waiting hours for results\n",
    "3. **Educational Clarity:** Simpler codebases that are easier to understand and modify\n",
    "4. **Vectorization by Default:** Learn modern RL practices from the start\n",
    "\n",
    "### Available Ocean Environments\n",
    "\n",
    "Ocean includes a variety of environments with different characteristics (more can be found in the [docs](https://github.com/PufferAI/PufferLib/tree/3.0/pufferlib/ocean)):\n",
    "\n",
    "- **Classic Control:** `cartpole`\n",
    "  - Simple physics simulations\n",
    "  - Low-dimensional observations\n",
    "  - Great for debugging and initial testing\n",
    "\n",
    "- **Grid Worlds:** `squared`, `minigrid_variants`\n",
    "  - Discrete state/action spaces\n",
    "  - Fast iteration times\n",
    "  - Perfect for verifying algorithm correctness\n",
    "\n",
    "- **Atari-Style:** `breakout`, `pong`, `pacman`, `enduro`\n",
    "  - More complex visual observations\n",
    "  - Longer training times\n",
    "  - Closer to real-world RL challenges\n",
    "\n",
    "- **Puzzle Games:** `tetris`\n",
    "  - Strategic planning required\n",
    "  - Sparse rewards\n",
    "  - Advanced challenge tasks\n",
    "\n",
    "### Why PufferLib for This Assignment?\n",
    "\n",
    "We chose PufferLib Ocean for several pedagogical reasons:\n",
    "\n",
    "1. **Immediate Feedback:** Fast environments mean you can iterate quickly on your code\n",
    "2. **Realistic Scale:** The environment enables easy vectorized environments like in real RL research\n",
    "3. **Low Hardware Requirements:** Efficient implementation means you don't need expensive GPUs because of vectorised observations\n",
    "4. **Clear Progression:** From simple (Squared) to complex (Breakout) in the same framework\n",
    "5. **Industry Relevance:** PufferLib is used in actual RL research and applications\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Environment Factories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Debug env observation shape: (1, 121)\n",
      "Action space: MultiDiscrete([5])\n",
      "Observation space: Box(0, 1, (1, 121), uint8)\n"
     ]
    }
   ],
   "source": [
    "class TimeLimitVec:\n",
    "    \"\"\"\n",
    "    Generic vector env time-limit wrapper for PufferLib/Gymnasium-like envs.docker run -it --rm -p 8888:8888 -v \"$(pwd)\":/app ciemcornelissen/puffer-notebook:latest\n",
    "    - Marks truncations True when per-env step count reaches max_episode_steps.\n",
    "    - Preserves existing terminals/truncations from the underlying env.\n",
    "    - Resets per-env counters on reset or when an env ends.\n",
    "    \"\"\"\n",
    "    def __init__(self, env: Any, max_episode_steps: int, info_key: str = \"time_limit\"):\n",
    "        assert max_episode_steps and max_episode_steps > 0\n",
    "        self.env = env\n",
    "        self.max_episode_steps = int(max_episode_steps)\n",
    "        self.info_key = info_key\n",
    "\n",
    "        # Mirror common attributes for compatibility\n",
    "        self.single_observation_space = getattr(env, \"single_observation_space\", None)\n",
    "        self.single_action_space = getattr(env, \"single_action_space\", None)\n",
    "        self.observation_space = getattr(env, \"observation_space\", None)\n",
    "        self.action_space = getattr(env, \"action_space\", None)\n",
    "        self.num_agents = getattr(env, \"num_agents\", 1)\n",
    "\n",
    "        self._steps = np.zeros(self.num_agents, dtype=np.int64)\n",
    "\n",
    "    def reset(self, seed: Optional[int] = 0):\n",
    "        obs, infos = self.env.reset(seed)\n",
    "        self._steps[...] = 0\n",
    "        return obs, infos\n",
    "\n",
    "    # def reset(self, seed: Optional[int] = None): \n",
    "    #     if seed is None:\n",
    "    #         seed = random.randint(1, 2**32 - 1)\n",
    "    #     obs, infos = self.env.reset(seed)\n",
    "    #     self._steps[...] = 0\n",
    "    #     return obs, infos\n",
    "\n",
    "\n",
    "    def step(self, actions):\n",
    "        obs, rewards, terminals, truncations, infos = self.env.step(actions)\n",
    "\n",
    "        t = np.asarray(terminals, dtype=bool)\n",
    "        tr = np.asarray(truncations, dtype=bool)\n",
    "        if t.ndim == 0:\n",
    "            t = t.reshape(1)\n",
    "        if tr.ndim == 0:\n",
    "            tr = tr.reshape(1)\n",
    "\n",
    "        active = ~(t | tr)\n",
    "        # Increment only for envs still active before this step's end flags\n",
    "        self._steps[active] += 1\n",
    "\n",
    "        # Apply time limit where not already ended this step\n",
    "        timeouts = (self._steps >= self.max_episode_steps) & active\n",
    "        if np.any(timeouts):\n",
    "            tr = np.logical_or(tr, timeouts)\n",
    "            self._steps[timeouts] = 0\n",
    "            if infos is None:\n",
    "                infos = []\n",
    "            if not isinstance(infos, list):\n",
    "                infos = [infos]\n",
    "            infos.append({self.info_key: {\"timeouts\": timeouts.copy()}})\n",
    "\n",
    "        # Also reset counters for any envs that ended naturally\n",
    "        ended = t | tr\n",
    "        if np.any(ended):\n",
    "            self._steps[ended] = 0\n",
    "\n",
    "        return obs, rewards, t, tr, infos\n",
    "\n",
    "    def render(self, *args, **kwargs):\n",
    "        return getattr(self.env, \"render\")(*args, **kwargs)\n",
    "\n",
    "    def close(self):\n",
    "        return getattr(self.env, \"close\")()\n",
    "def make_env(name: str, seed: int = 0):\n",
    "    env_map = {\n",
    "        'squared': squared,\n",
    "        'pong': pong,\n",
    "        'pacman': pacman,\n",
    "        'enduro': enduro,\n",
    "        'tetris': tetris,\n",
    "        'breakout': breakout,\n",
    "        'cartpole': cartpole\n",
    "    }\n",
    "    if name not in env_map:\n",
    "        raise ValueError(f'Unknown environment {name}')\n",
    "\n",
    "    def thunk():\n",
    "        # Get the module from the map\n",
    "        env_module = env_map[name]\n",
    "\n",
    "        # For example, in the 'squared' module, there is a 'Squared' class.\n",
    "        env_class_name = name.capitalize()\n",
    "        env_class = getattr(env_module, env_class_name)\n",
    "\n",
    "        raw_env = env_class(num_envs=1, render_mode=None, seed=seed)  # Instantiate the class\n",
    "        env = TimeLimitVec(raw_env, max_episode_steps=10_000)\n",
    "        env.reset(seed=seed)\n",
    "        return env\n",
    "    return thunk\n",
    "\n",
    "debug_env_name = 'squared'\n",
    "intermediate_env_name = 'cartpole'  # Change to 'pacman', 'enduro', or 'tetris' if desired\n",
    "challenge_env_name = 'breakout'\n",
    "\n",
    "test_env = make_env(debug_env_name)()\n",
    "obs, info = test_env.reset()\n",
    "print('Debug env observation shape:', np.array(obs).shape)\n",
    "print('Action space:', test_env.action_space)\n",
    "print('Observation space:', test_env.observation_space)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Configuration\n",
    "Adjust hyperparameters and add hyperparameters as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Config(seed=7, gamma=0.95, lr=0.001, batch_size=128, buffer_capacity=1000, min_buffer_size=200, max_steps_per_env=60000, max_episode_len=500, epsilon_start=1.0, epsilon_end=0.01, epsilon_decay_steps=5000, target_update_interval=100, train_freq=4, gradient_clip=10.0, reward_clip=None, use_double_dqn=True, use_dueling=True, vector_envs=1, log_interval_episodes=10, eval_episodes=3, curriculum=['cartpole'], hidden_size=256, n_steps=3, use_noisy_layer=False)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@dataclass\n",
    "class Config:\n",
    "    seed: int = 7\n",
    "    gamma: float = 0.95                # prioritize short-term rewards\n",
    "    lr: float = 1e-3                   # can reduce to 1e-4 if unstable\n",
    "    batch_size: int = 128\n",
    "    buffer_capacity: int = 1000      # larger buffer for diverse data\n",
    "    min_buffer_size: int = 200       # start training with more data\n",
    "    max_steps_per_env: int = 60_000\n",
    "    max_episode_len: int = 500\n",
    "    epsilon_start: float = 1.0\n",
    "    epsilon_end: float = 0.01\n",
    "    epsilon_decay_steps: int = 5000\n",
    "    target_update_interval: int = 100\n",
    "    train_freq: int = 4\n",
    "    gradient_clip: float = 10.0\n",
    "    reward_clip: Optional[float] = None\n",
    "    use_double_dqn: bool = True\n",
    "    use_dueling: bool = True           # enabled for better value estimation\n",
    "    vector_envs: int = 1\n",
    "    log_interval_episodes: int = 10\n",
    "    eval_episodes: int = 3\n",
    "    curriculum: List[str] = None\n",
    "    hidden_size: int = 256             # larger network for complex patterns\n",
    "\n",
    "    n_steps: int = 3\n",
    "    use_noisy_layer: bool = False\n",
    "\n",
    "cfg = Config()\n",
    "cfg.curriculum = [intermediate_env_name]\n",
    "random.seed(cfg.seed)\n",
    "np.random.seed(cfg.seed)\n",
    "torch.manual_seed(cfg.seed)\n",
    "cfg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Replay Buffer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing ReplayBuffer...\n",
      "✓ ReplayBuffer tests passed!\n"
     ]
    }
   ],
   "source": [
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity: int, obs_shape: tuple, device):\n",
    "        self.capacity = capacity\n",
    "        self.device = device\n",
    "        self.obs_shape = obs_shape\n",
    "        self.position = 0\n",
    "        self.size = 0\n",
    "        \n",
    "        # Pre-allocate numpy arrays for storage\n",
    "        self.obs = np.zeros((capacity, *obs_shape), dtype=np.float32)\n",
    "        self.actions = np.zeros((capacity, 1), dtype=np.int64)  # Shape: (capacity, 1)\n",
    "        self.rewards = np.zeros((capacity, 1), dtype=np.float32)\n",
    "        self.next_obs = np.zeros((capacity, *obs_shape), dtype=np.float32)\n",
    "        self.dones = np.zeros((capacity, 1), dtype=np.float32)\n",
    "    \n",
    "    def push(self, obs, action, reward, next_obs, done):\n",
    "        \"\"\"Store a transition\"\"\"\n",
    "        self.obs[self.position] = obs\n",
    "        self.actions[self.position, 0] = action  # Store as scalar in (1,) shaped array\n",
    "        self.rewards[self.position, 0] = reward\n",
    "        self.next_obs[self.position] = next_obs\n",
    "        self.dones[self.position, 0] = float(done)\n",
    "        \n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "        self.size = min(self.size + 1, self.capacity)\n",
    "    \n",
    "    def sample(self, batch_size: int):\n",
    "        \"\"\"Sample a batch of transitions and convert to PyTorch tensors\"\"\"\n",
    "        indices = np.random.choice(self.size, batch_size, replace=False)\n",
    "        \n",
    "        # Convert to tensors - actions will have shape (batch_size, 1)\n",
    "        obs = torch.FloatTensor(self.obs[indices]).to(self.device)\n",
    "        actions = torch.LongTensor(self.actions[indices]).to(self.device)  # (batch_size, 1)\n",
    "        rewards = torch.FloatTensor(self.rewards[indices]).to(self.device)\n",
    "        next_obs = torch.FloatTensor(self.next_obs[indices]).to(self.device)\n",
    "        dones = torch.FloatTensor(self.dones[indices]).to(self.device)\n",
    "        \n",
    "        return obs, actions, rewards, next_obs, dones\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.size\n",
    "\n",
    "# Test ReplayBuffer\n",
    "print(\"Testing ReplayBuffer...\")\n",
    "test_buffer = ReplayBuffer(100, (4,), DEVICE)\n",
    "test_buffer.push(np.array([1,2,3,4]), 0, 1.0, np.array([2,3,4,5]), False)\n",
    "assert len(test_buffer) == 1, \"Buffer should have 1 element\"\n",
    "obs, actions, rewards, next_obs, dones = test_buffer.sample(1)\n",
    "assert obs.shape == (1, 4), f\"Expected shape (1, 4), got {obs.shape}\"\n",
    "print(\"✓ ReplayBuffer tests passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5 Epsilon Schedule\n",
    "Linear decay from start to end over `epsilon_decay_steps`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epsilon(0)= 1.0 Epsilon(mid)= 0.505 Epsilon(end)= 0.01\n"
     ]
    }
   ],
   "source": [
    "def epsilon_by_step(step: int, cfg: Config) -> float:\n",
    "    per_step = (cfg.epsilon_start-cfg.epsilon_end)/cfg.epsilon_decay_steps\n",
    "    if step < cfg.epsilon_decay_steps:\n",
    "        eps = cfg.epsilon_start - (step*per_step)\n",
    "    else:\n",
    "        eps = cfg.epsilon_end\n",
    "    return eps\n",
    "\n",
    "print('Epsilon(0)=', epsilon_by_step(0, cfg), 'Epsilon(mid)=', epsilon_by_step(cfg.epsilon_decay_steps//2, cfg), 'Epsilon(end)=', epsilon_by_step(cfg.epsilon_decay_steps*2, cfg))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.6 Q-Network Architectures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class NoisyLinear(nn.Module):\n",
    "    \"\"\"\n",
    "    Factorized NoisyNet linear layer (Fortunato et al., 2018).\n",
    "    Samples noise during training; uses deterministic weights in eval mode.\n",
    "\n",
    "    Args:\n",
    "        in_features (int): input feature size\n",
    "        out_features (int): output feature size\n",
    "        sigma0 (float): initial scale for noise parameters\n",
    "        bias (bool): include bias term\n",
    "        auto_reset (bool): resample noise every forward() in training mode\n",
    "    \"\"\"\n",
    "    def __init__(self, in_features: int, out_features: int, sigma0: float = 0.5, bias: bool = True, auto_reset: bool = True):\n",
    "        super().__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.sigma0 = sigma0\n",
    "        self.auto_reset = auto_reset\n",
    "\n",
    "        # Learnable parameters (mu and sigma)\n",
    "        self.weight_mu = nn.Parameter(torch.empty(out_features, in_features))\n",
    "        self.weight_sigma = nn.Parameter(torch.empty(out_features, in_features))\n",
    "        if bias:\n",
    "            self.bias_mu = nn.Parameter(torch.empty(out_features))\n",
    "            self.bias_sigma = nn.Parameter(torch.empty(out_features))\n",
    "        else:\n",
    "            self.register_parameter(\"bias_mu\", None)\n",
    "            self.register_parameter(\"bias_sigma\", None)\n",
    "\n",
    "        # Buffers for factorized noise\n",
    "        self.register_buffer(\"eps_in\", torch.empty(1, in_features))\n",
    "        self.register_buffer(\"eps_out\", torch.empty(out_features, 1))\n",
    "\n",
    "        self.reset_parameters()\n",
    "        self.reset_noise()\n",
    "\n",
    "    def reset_parameters(self) -> None:\n",
    "        mu_range = 1.0 / math.sqrt(self.in_features)\n",
    "        self.weight_mu.data.uniform_(-mu_range, mu_range)\n",
    "        self.weight_sigma.data.fill_(self.sigma0 / math.sqrt(self.in_features))\n",
    "        if self.bias_mu is not None:\n",
    "            self.bias_mu.data.uniform_(-mu_range, mu_range)\n",
    "            self.bias_sigma.data.fill_(self.sigma0 / math.sqrt(self.out_features))\n",
    "\n",
    "    @staticmethod\n",
    "    def _scale_noise(size, device):\n",
    "        # f(epsilon) = sign(epsilon) * sqrt(|epsilon|)\n",
    "        x = torch.randn(size, device=device)\n",
    "        return x.sign().mul_(x.abs().sqrt_())\n",
    "\n",
    "    def reset_noise(self) -> None:\n",
    "        device = self.weight_mu.device\n",
    "        self.eps_in.copy_(self._scale_noise((1, self.in_features), device))\n",
    "        self.eps_out.copy_(self._scale_noise((self.out_features, 1), device))\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        if self.training:\n",
    "            if self.auto_reset:\n",
    "                self.reset_noise()\n",
    "            weight = self.weight_mu + self.weight_sigma * (self.eps_out @ self.eps_in)\n",
    "            bias = None\n",
    "            if self.bias_mu is not None:\n",
    "                bias = self.bias_mu + self.bias_sigma * self.eps_out.squeeze(1)\n",
    "        else:\n",
    "            weight = self.weight_mu\n",
    "            bias = self.bias_mu if self.bias_mu is not None else None\n",
    "\n",
    "        return F.linear(x, weight, bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obs space: 121\n",
      "MLPQ(\n",
      "  (feature): Sequential(\n",
      "    (0): Linear(in_features=121, out_features=256, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=256, out_features=256, bias=True)\n",
      "    (3): ReLU()\n",
      "  )\n",
      "  (value_stream): Sequential(\n",
      "    (0): Linear(in_features=256, out_features=256, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=256, out_features=1, bias=True)\n",
      "  )\n",
      "  (advantage_stream): Sequential(\n",
      "    (0): Linear(in_features=256, out_features=256, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=256, out_features=5, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class MLPQ(nn.Module):\n",
    "    def __init__(self, obs_dim: int, action_dim: int, dueling=False):\n",
    "        super().__init__()\n",
    "        self.hidden_size = cfg.hidden_size  # Typically 128, 256, or 512\n",
    "        self.dueling = dueling\n",
    "        self.action_dim = action_dim\n",
    "\n",
    "        final_layer = (\n",
    "            NoisyLinear(self.hidden_size, action_dim)\n",
    "            if cfg.use_noisy_layer\n",
    "            else nn.Linear(self.hidden_size, action_dim)\n",
    "        )\n",
    "\n",
    "        # Shared feature extractor\n",
    "        self.feature = nn.Sequential(\n",
    "            nn.Linear(obs_dim, self.hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(self.hidden_size, self.hidden_size),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        # Define output heads\n",
    "        if not dueling:\n",
    "            self.q_head = final_layer\n",
    "        else:\n",
    "            self.value_stream = nn.Sequential(\n",
    "                nn.Linear(self.hidden_size, self.hidden_size),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(self.hidden_size, 1),\n",
    "            )\n",
    "            self.advantage_stream = nn.Sequential(\n",
    "                nn.Linear(self.hidden_size, self.hidden_size),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(self.hidden_size, action_dim),\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = self.feature(x)\n",
    "        if not self.dueling:\n",
    "            return self.q_head(features)\n",
    "        else:\n",
    "            value = self.value_stream(features)\n",
    "            advantages = self.advantage_stream(features)\n",
    "            q_values = value + (advantages - advantages.mean(dim=1, keepdim=True))\n",
    "            return q_values\n",
    "\n",
    "\n",
    "def build_q_network(env, cfg: Config):\n",
    "    obs_space = env.single_observation_space\n",
    "    print('Obs space:', obs_space.shape[0])\n",
    "    act_space = env.single_action_space\n",
    "    action_dim = act_space.n\n",
    "    assert isinstance(act_space, gym.spaces.Discrete), 'DQN requires discrete actions'\n",
    "    if len(obs_space.shape) == 1:\n",
    "        return MLPQ(obs_space.shape[0], action_dim, dueling=cfg.use_dueling)\n",
    "\n",
    "\n",
    "# Quick test on debug env\n",
    "net_test_env = test_env\n",
    "test_net = build_q_network(net_test_env, cfg).to(DEVICE)\n",
    "print(test_net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.7 Action Selection\n",
    "Standard epsilon-greedy policy. (Later you might incorporate noisy networks or exploration bonuses.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random/greedy test action: [2]\n"
     ]
    }
   ],
   "source": [
    "@torch.no_grad()\n",
    "def select_action(q_net, obs, epsilon: float, env):\n",
    "    if random.random() < epsilon:\n",
    "        return env.action_space.sample()\n",
    "    if isinstance(obs, torch.Tensor):\n",
    "        obs_t = obs.to(DEVICE, dtype=torch.float32, non_blocking=True)\n",
    "    else:\n",
    "        obs_t = torch.as_tensor(obs, dtype=torch.float32, device=DEVICE)\n",
    "    q_vals = q_net(obs_t)\n",
    "    return int(q_vals.argmax(dim=1).item())\n",
    "\n",
    "# Test call\n",
    "dummy_action = select_action(test_net, obs, 1.0, test_env)\n",
    "print('Random/greedy test action:', dummy_action)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.8 Training Step\n",
    "Compute standard DQN loss or Double DQN target if `cfg.use_double_dqn=True`.\n",
    "\n",
    "Target for vanilla DQN:\n",
    "$$ y = r + (1-d) \\gamma \\max_{a'} Q_{target}(s', a') $$\n",
    "\n",
    "Double DQN variant:\n",
    "$$ a^* = \\arg\\max_{a'} Q_{online}(s', a') \\quad; \\quad y = r + (1-d) \\gamma Q_{target}(s', a^*) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.SmoothL1Loss()\n",
    "\n",
    "def train_step(q_net, target_net, optimizer, replay: ReplayBuffer, cfg: Config):\n",
    "    \"\"\"\n",
    "    Performs one training step of DQN.\n",
    "    \n",
    "    Args:\n",
    "        q_net: Online Q-network (being trained)\n",
    "        target_net: Target Q-network (frozen, updated periodically)\n",
    "        optimizer: Optimizer for q_net\n",
    "        replay: Replay buffer containing transitions\n",
    "        cfg: Configuration object\n",
    "    \n",
    "    Returns:\n",
    "        Loss value or None if buffer not ready\n",
    "    \"\"\"\n",
    "    # Don't train until we have enough experiences\n",
    "    if len(replay) < cfg.min_buffer_size:\n",
    "        return None\n",
    "    \n",
    "    obs, actions, rewards, next_obs, dones = replay.sample(cfg.batch_size)\n",
    "\n",
    "    q_values = q_net(obs)  # Shape: (batch_size, action_dim)\n",
    "\n",
    "    q_sa = q_values.gather(1, actions)  # Shape: (batch_size, 1)\n",
    "    \n",
    "    # ============= Compute Target Q-values =============\n",
    "    with torch.no_grad():  # Don't compute gradients for target\n",
    "        if cfg.use_double_dqn:\n",
    "            \n",
    "            # Step 1: Use online network to find best action in next state\n",
    "            next_q_values_online = q_net(next_obs)  # Shape: (batch_size, action_dim)\n",
    "            best_actions = next_q_values_online.argmax(dim=1, keepdim=True)  # Shape: (batch_size, 1)\n",
    "            \n",
    "            # Step 2: Use target network to evaluate that action\n",
    "            next_q_values_target = target_net(next_obs)  # Shape: (batch_size, action_dim)\n",
    "            next_q_value = next_q_values_target.gather(1, best_actions)  # Shape: (batch_size, 1)\n",
    "            \n",
    "        else:\n",
    "            # Vanilla DQN: Use target network for both selection and evaluation\n",
    "            next_q_values = target_net(next_obs)  # Shape: (batch_size, action_dim)\n",
    "            next_q_value = next_q_values.max(dim=1, keepdim=True)[0]  # Shape: (batch_size, 1)\n",
    "            # Note: max returns (values, indices), we take [0] for values\n",
    "        \n",
    "        # Compute TD target: y = r + γ * max Q(s', a') * (1 - done)\n",
    "        # The (1 - dones) term zeros out the next_q_value if episode ended\n",
    "        target = rewards + (cfg.gamma ** cfg.n_steps) * next_q_value * (1 - dones)  # Shape: (batch_size, 1)\n",
    "\n",
    "    loss = loss_fn(q_sa, target)\n",
    "    \n",
    "    # Standard PyTorch training loop\n",
    "    optimizer.zero_grad()  # Clear previous gradients\n",
    "    loss.backward()  # Compute gradients\n",
    "    nn.utils.clip_grad_norm_(q_net.parameters(), cfg.gradient_clip)  # Prevent exploding gradients\n",
    "    optimizer.step()  # Update weights\n",
    "    \n",
    "    return float(loss.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.9 Reward Processing\n",
    "Add optional reward clipping to stabilize training on environments with high or varied reward magnitudes.\n",
    "\n",
    "If `cfg.reward_clip` is set, clip reward to `[-cfg.reward_clip, cfg.reward_clip]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reward test (no clip): 5.0\n",
      "Reward test (clip=1.0): 1.0\n"
     ]
    }
   ],
   "source": [
    "def process_reward(r: float, cfg: Config):\n",
    "    if cfg.reward_clip is not None:\n",
    "        return max(-cfg.reward_clip, min(cfg.reward_clip, r))\n",
    "    return r\n",
    "\n",
    "# Quick test\n",
    "cfg.reward_clip = None  # Set to 1.0 to try clipping later\n",
    "print('Reward test (no clip):', process_reward(5.0, cfg))\n",
    "cfg.reward_clip = 1.0\n",
    "print('Reward test (clip=1.0):', process_reward(5.0, cfg))\n",
    "cfg.reward_clip = None  # Reset for training; modify if desired"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.10 Evaluation Utilities\n",
    "Evaluation runs with greedy policy (`epsilon=0`).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval utility ready.\n"
     ]
    }
   ],
   "source": [
    "def evaluate(env_fn, q_net, cfg: Config, episodes: int):\n",
    "    env = env_fn\n",
    "    returns = []\n",
    "    for _ in range(episodes):\n",
    "        obs, _ = env.reset()\n",
    "        done = False\n",
    "        ep_ret = 0\n",
    "        steps = 0\n",
    "        while not done and steps < cfg.max_episode_len:\n",
    "            a = select_action(q_net, obs, 0.0, env)\n",
    "            obs2, r, terminated, truncated, _ = env.step(a)\n",
    "            done = terminated or truncated\n",
    "            ep_ret += r\n",
    "            obs = obs2\n",
    "            steps += 1\n",
    "        returns.append(ep_ret)\n",
    "    return float(np.mean(returns))\n",
    "\n",
    "def hard_update(target, source):\n",
    "    target.load_state_dict(source.state_dict())\n",
    "\n",
    "print('Eval utility ready.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.11 Training Loop\n",
    "We iterate over the environments in.\n",
    "Logging per stage:\n",
    "- `episode_rewards`\n",
    "- `losses`\n",
    "- `eps_history`\n",
    "- `eval` (periodic greedy evaluation)\n",
    "\n",
    "Increase `max_steps_per_env` for stronger performance. For quick debugging, you may temporarily reduce it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Stage 1: cartpole ===\n",
      "Obs space: 4\n",
      "Obs space: 4\n",
      "Starting training...\n",
      "[DEBUG] Step      9 | Ep    1 | Eps 0.998 | MeanReward(10ep):   8.000 | MeanLoss(100it):  0.00000 | MeanQ:  -0.001 | GradNorm:  0.0000\n",
      "[DEBUG] Step     20 | Ep    2 | Eps 0.996 | MeanReward(10ep):   9.000 | MeanLoss(100it):  0.00000 | MeanQ:  -0.001 | GradNorm:  0.0000\n",
      "[DEBUG] Step     31 | Ep    3 | Eps 0.994 | MeanReward(10ep):   9.333 | MeanLoss(100it):  0.00000 | MeanQ:  -0.001 | GradNorm:  0.0000\n",
      "[DEBUG] Step     38 | Ep    4 | Eps 0.993 | MeanReward(10ep):   8.500 | MeanLoss(100it):  0.00000 | MeanQ:  -0.001 | GradNorm:  0.0000\n",
      "[DEBUG] Step     54 | Ep    5 | Eps 0.990 | MeanReward(10ep):   9.800 | MeanLoss(100it):  0.00000 | MeanQ:  -0.001 | GradNorm:  0.0000\n",
      "[DEBUG] Step     67 | Ep    6 | Eps 0.987 | MeanReward(10ep):  10.167 | MeanLoss(100it):  0.00000 | MeanQ:  -0.001 | GradNorm:  0.0000\n",
      "[DEBUG] Step     85 | Ep    7 | Eps 0.983 | MeanReward(10ep):  11.143 | MeanLoss(100it):  0.00000 | MeanQ:  -0.001 | GradNorm:  0.0000\n",
      "[DEBUG] Step     93 | Ep    8 | Eps 0.982 | MeanReward(10ep):  10.625 | MeanLoss(100it):  0.00000 | MeanQ:  -0.001 | GradNorm:  0.0000\n",
      "[DEBUG] Step     98 | Ep    9 | Eps 0.981 | MeanReward(10ep):   9.889 | MeanLoss(100it):  0.00000 | MeanQ:  -0.001 | GradNorm:  0.0000\n",
      "[DEBUG] Step    117 | Ep   10 | Eps 0.977 | MeanReward(10ep):  10.700 | MeanLoss(100it):  0.00000 | MeanQ:  -0.001 | GradNorm:  0.0000\n",
      "[cartpole] Ep 10 Steps 117 RecentMean 10.70 Eps 0.977\n",
      "[DEBUG] Step    123 | Ep   11 | Eps 0.976 | MeanReward(10ep):  10.400 | MeanLoss(100it):  0.00000 | MeanQ:  -0.001 | GradNorm:  0.0000\n",
      "[DEBUG] Step    131 | Ep   12 | Eps 0.974 | MeanReward(10ep):  10.100 | MeanLoss(100it):  0.00000 | MeanQ:  -0.001 | GradNorm:  0.0000\n",
      "[DEBUG] Step    149 | Ep   13 | Eps 0.971 | MeanReward(10ep):  10.800 | MeanLoss(100it):  0.00000 | MeanQ:  -0.001 | GradNorm:  0.0000\n",
      "[DEBUG] Step    154 | Ep   14 | Eps 0.970 | MeanReward(10ep):  10.600 | MeanLoss(100it):  0.00000 | MeanQ:  -0.001 | GradNorm:  0.0000\n",
      "[DEBUG] Step    162 | Ep   15 | Eps 0.968 | MeanReward(10ep):   9.800 | MeanLoss(100it):  0.00000 | MeanQ:  -0.001 | GradNorm:  0.0000\n",
      "[DEBUG] Step    168 | Ep   16 | Eps 0.967 | MeanReward(10ep):   9.100 | MeanLoss(100it):  0.00000 | MeanQ:  -0.001 | GradNorm:  0.0000\n",
      "[DEBUG] Step    184 | Ep   17 | Eps 0.964 | MeanReward(10ep):   8.900 | MeanLoss(100it):  0.00000 | MeanQ:  -0.001 | GradNorm:  0.0000\n",
      "[DEBUG] Step    192 | Ep   18 | Eps 0.962 | MeanReward(10ep):   8.900 | MeanLoss(100it):  0.00000 | MeanQ:  -0.001 | GradNorm:  0.0000\n",
      "[DEBUG] Step    202 | Ep   19 | Eps 0.960 | MeanReward(10ep):   9.400 | MeanLoss(100it):  0.00000 | MeanQ:  -0.001 | GradNorm:  0.0000\n",
      "[DEBUG] Step    211 | Ep   20 | Eps 0.958 | MeanReward(10ep):   8.400 | MeanLoss(100it):  0.00000 | MeanQ:  -0.001 | GradNorm:  0.0000\n",
      "[cartpole] Ep 20 Steps 211 RecentMean 8.40 Eps 0.958\n",
      "Eval @ Ep 20 and 211 steps => mean return 5.00\n",
      "[DEBUG] Step    219 | Ep   21 | Eps 0.957 | MeanReward(10ep):   8.600 | MeanLoss(100it):  0.00000 | MeanQ:  -0.003 | GradNorm:  0.0000\n",
      "[DEBUG] Step    228 | Ep   22 | Eps 0.955 | MeanReward(10ep):   8.700 | MeanLoss(100it):  0.00000 | MeanQ:  -0.001 | GradNorm:  0.0000\n",
      "[DEBUG] Step    233 | Ep   23 | Eps 0.954 | MeanReward(10ep):   7.400 | MeanLoss(100it):  0.00000 | MeanQ:  -0.001 | GradNorm:  0.0000\n",
      "[DEBUG] Step    243 | Ep   24 | Eps 0.952 | MeanReward(10ep):   7.900 | MeanLoss(100it):  0.00000 | MeanQ:  -0.001 | GradNorm:  0.0000\n",
      "[DEBUG] Step    250 | Ep   25 | Eps 0.951 | MeanReward(10ep):   7.800 | MeanLoss(100it):  0.00000 | MeanQ:  -0.001 | GradNorm:  0.0000\n",
      "[DEBUG] Step    259 | Ep   26 | Eps 0.949 | MeanReward(10ep):   8.100 | MeanLoss(100it):  2.14826 | MeanQ:   0.201 | GradNorm:  0.4072\n",
      "[DEBUG] Step    268 | Ep   27 | Eps 0.947 | MeanReward(10ep):   7.400 | MeanLoss(100it):  1.80099 | MeanQ:   0.640 | GradNorm:  0.5854\n",
      "[DEBUG] Step    275 | Ep   28 | Eps 0.946 | MeanReward(10ep):   7.300 | MeanLoss(100it):  1.66736 | MeanQ:   0.863 | GradNorm:  0.5553\n",
      "[DEBUG] Step    281 | Ep   29 | Eps 0.945 | MeanReward(10ep):   6.900 | MeanLoss(100it):  1.43756 | MeanQ:   1.424 | GradNorm:  0.1567\n",
      "[DEBUG] Step    287 | Ep   30 | Eps 0.943 | MeanReward(10ep):   6.600 | MeanLoss(100it):  1.36504 | MeanQ:   1.656 | GradNorm:  0.3907\n",
      "[cartpole] Ep 30 Steps 287 RecentMean 6.60 Eps 0.943\n",
      "[DEBUG] Step    297 | Ep   31 | Eps 0.941 | MeanReward(10ep):   6.800 | MeanLoss(100it):  1.25159 | MeanQ:   1.743 | GradNorm:  0.7668\n",
      "[DEBUG] Step    303 | Ep   32 | Eps 0.940 | MeanReward(10ep):   6.500 | MeanLoss(100it):  1.20120 | MeanQ:   1.668 | GradNorm:  0.4443\n",
      "[DEBUG] Step    310 | Ep   33 | Eps 0.939 | MeanReward(10ep):   6.700 | MeanLoss(100it):  1.42536 | MeanQ:   1.796 | GradNorm:  0.8730\n",
      "[DEBUG] Step    321 | Ep   34 | Eps 0.937 | MeanReward(10ep):   6.800 | MeanLoss(100it):  1.61212 | MeanQ:   2.457 | GradNorm:  0.8202\n",
      "[DEBUG] Step    333 | Ep   35 | Eps 0.934 | MeanReward(10ep):   7.300 | MeanLoss(100it):  1.66552 | MeanQ:   3.584 | GradNorm:  0.3963\n",
      "[DEBUG] Step    347 | Ep   36 | Eps 0.931 | MeanReward(10ep):   7.800 | MeanLoss(100it):  1.67544 | MeanQ:   4.584 | GradNorm:  0.8861\n",
      "[DEBUG] Step    356 | Ep   37 | Eps 0.930 | MeanReward(10ep):   7.800 | MeanLoss(100it):  1.69037 | MeanQ:   4.493 | GradNorm:  1.1996\n",
      "[DEBUG] Step    362 | Ep   38 | Eps 0.929 | MeanReward(10ep):   7.700 | MeanLoss(100it):  1.68159 | MeanQ:   4.294 | GradNorm:  0.7355\n",
      "[DEBUG] Step    384 | Ep   39 | Eps 0.924 | MeanReward(10ep):   9.300 | MeanLoss(100it):  1.60308 | MeanQ:   4.239 | GradNorm:  0.7283\n",
      "[DEBUG] Step    390 | Ep   40 | Eps 0.923 | MeanReward(10ep):   9.300 | MeanLoss(100it):  1.59382 | MeanQ:   4.495 | GradNorm:  0.7503\n",
      "[cartpole] Ep 40 Steps 390 RecentMean 9.30 Eps 0.923\n",
      "Eval @ Ep 40 and 390 steps => mean return 5.00\n",
      "[DEBUG] Step    410 | Ep   41 | Eps 0.919 | MeanReward(10ep):  10.300 | MeanLoss(100it):  1.55840 | MeanQ:   6.445 | GradNorm:  1.4434\n",
      "[DEBUG] Step    421 | Ep   42 | Eps 0.917 | MeanReward(10ep):  10.800 | MeanLoss(100it):  1.50148 | MeanQ:   8.577 | GradNorm:  1.5627\n",
      "[DEBUG] Step    428 | Ep   43 | Eps 0.915 | MeanReward(10ep):  10.800 | MeanLoss(100it):  1.49313 | MeanQ:   8.836 | GradNorm:  1.4404\n",
      "[DEBUG] Step    439 | Ep   44 | Eps 0.913 | MeanReward(10ep):  10.800 | MeanLoss(100it):  1.47148 | MeanQ:   8.308 | GradNorm:  1.5528\n",
      "[DEBUG] Step    473 | Ep   45 | Eps 0.907 | MeanReward(10ep):  13.000 | MeanLoss(100it):  1.31843 | MeanQ:   8.737 | GradNorm:  1.4458\n",
      "[DEBUG] Step    490 | Ep   46 | Eps 0.903 | MeanReward(10ep):  13.300 | MeanLoss(100it):  1.25308 | MeanQ:   7.578 | GradNorm:  0.9476\n",
      "[DEBUG] Step    502 | Ep   47 | Eps 0.901 | MeanReward(10ep):  13.600 | MeanLoss(100it):  1.20999 | MeanQ:   7.824 | GradNorm:  1.1981\n",
      "[DEBUG] Step    509 | Ep   48 | Eps 0.899 | MeanReward(10ep):  13.700 | MeanLoss(100it):  1.20092 | MeanQ:   8.640 | GradNorm:  1.4667\n",
      "[DEBUG] Step    515 | Ep   49 | Eps 0.898 | MeanReward(10ep):  12.100 | MeanLoss(100it):  1.18606 | MeanQ:   9.203 | GradNorm:  1.1625\n",
      "[DEBUG] Step    521 | Ep   50 | Eps 0.897 | MeanReward(10ep):  12.100 | MeanLoss(100it):  1.16438 | MeanQ:   9.747 | GradNorm:  1.4711\n",
      "[cartpole] Ep 50 Steps 521 RecentMean 12.10 Eps 0.897\n",
      "[DEBUG] Step    532 | Ep   51 | Eps 0.895 | MeanReward(10ep):  11.200 | MeanLoss(100it):  1.14754 | MeanQ:   9.242 | GradNorm:  1.4520\n",
      "[DEBUG] Step    537 | Ep   52 | Eps 0.894 | MeanReward(10ep):  10.600 | MeanLoss(100it):  1.13649 | MeanQ:   8.808 | GradNorm:  1.5202\n",
      "[DEBUG] Step    544 | Ep   53 | Eps 0.892 | MeanReward(10ep):  10.600 | MeanLoss(100it):  1.11250 | MeanQ:   8.283 | GradNorm:  1.5084\n",
      "[DEBUG] Step    555 | Ep   54 | Eps 0.890 | MeanReward(10ep):  10.600 | MeanLoss(100it):  1.09584 | MeanQ:   8.422 | GradNorm:  1.4943\n",
      "[DEBUG] Step    563 | Ep   55 | Eps 0.889 | MeanReward(10ep):   8.000 | MeanLoss(100it):  1.07397 | MeanQ:   9.054 | GradNorm:  0.9859\n",
      "[DEBUG] Step    571 | Ep   56 | Eps 0.887 | MeanReward(10ep):   7.100 | MeanLoss(100it):  1.05229 | MeanQ:   9.477 | GradNorm:  1.3732\n",
      "[DEBUG] Step    580 | Ep   57 | Eps 0.885 | MeanReward(10ep):   6.800 | MeanLoss(100it):  1.02078 | MeanQ:   9.087 | GradNorm:  0.3913\n",
      "[DEBUG] Step    593 | Ep   58 | Eps 0.883 | MeanReward(10ep):   7.400 | MeanLoss(100it):  0.99103 | MeanQ:   9.210 | GradNorm:  1.2005\n",
      "[DEBUG] Step    612 | Ep   59 | Eps 0.879 | MeanReward(10ep):   8.700 | MeanLoss(100it):  0.95923 | MeanQ:  10.815 | GradNorm:  1.3944\n",
      "[DEBUG] Step    628 | Ep   60 | Eps 0.876 | MeanReward(10ep):   9.700 | MeanLoss(100it):  0.93522 | MeanQ:  11.189 | GradNorm:  1.4596\n",
      "[cartpole] Ep 60 Steps 628 RecentMean 9.70 Eps 0.876\n",
      "Eval @ Ep 60 and 628 steps => mean return 18.00\n",
      "[DEBUG] Step    634 | Ep   61 | Eps 0.875 | MeanReward(10ep):   9.200 | MeanLoss(100it):  0.92776 | MeanQ:  10.746 | GradNorm:  1.5198\n",
      "[DEBUG] Step    640 | Ep   62 | Eps 0.873 | MeanReward(10ep):   9.300 | MeanLoss(100it):  0.91282 | MeanQ:  10.406 | GradNorm:  1.4273\n",
      "[DEBUG] Step    646 | Ep   63 | Eps 0.872 | MeanReward(10ep):   9.200 | MeanLoss(100it):  0.90676 | MeanQ:  10.423 | GradNorm:  1.5847\n",
      "[DEBUG] Step    659 | Ep   64 | Eps 0.870 | MeanReward(10ep):   9.400 | MeanLoss(100it):  0.86132 | MeanQ:  11.283 | GradNorm:  0.6633\n",
      "[DEBUG] Step    665 | Ep   65 | Eps 0.869 | MeanReward(10ep):   9.200 | MeanLoss(100it):  0.83231 | MeanQ:  11.470 | GradNorm:  1.5621\n",
      "[DEBUG] Step    676 | Ep   66 | Eps 0.866 | MeanReward(10ep):   9.500 | MeanLoss(100it):  0.80479 | MeanQ:  10.850 | GradNorm:  0.6183\n",
      "[DEBUG] Step    688 | Ep   67 | Eps 0.864 | MeanReward(10ep):   9.800 | MeanLoss(100it):  0.78412 | MeanQ:  11.069 | GradNorm:  0.8477\n",
      "[DEBUG] Step    697 | Ep   68 | Eps 0.862 | MeanReward(10ep):   9.400 | MeanLoss(100it):  0.76819 | MeanQ:  11.286 | GradNorm:  1.2207\n",
      "[DEBUG] Step    703 | Ep   69 | Eps 0.861 | MeanReward(10ep):   8.100 | MeanLoss(100it):  0.76322 | MeanQ:  11.173 | GradNorm:  1.3064\n",
      "[DEBUG] Step    712 | Ep   70 | Eps 0.859 | MeanReward(10ep):   7.400 | MeanLoss(100it):  0.69334 | MeanQ:  11.850 | GradNorm:  1.4109\n",
      "[cartpole] Ep 70 Steps 712 RecentMean 7.40 Eps 0.859\n",
      "[DEBUG] Step    729 | Ep   71 | Eps 0.856 | MeanReward(10ep):   8.500 | MeanLoss(100it):  0.61343 | MeanQ:  12.579 | GradNorm:  1.4280\n",
      "[DEBUG] Step    763 | Ep   72 | Eps 0.849 | MeanReward(10ep):  11.300 | MeanLoss(100it):  0.48704 | MeanQ:  12.901 | GradNorm:  1.4212\n",
      "[DEBUG] Step    772 | Ep   73 | Eps 0.847 | MeanReward(10ep):  11.600 | MeanLoss(100it):  0.45173 | MeanQ:  12.278 | GradNorm:  0.2761\n",
      "[DEBUG] Step    780 | Ep   74 | Eps 0.846 | MeanReward(10ep):  11.100 | MeanLoss(100it):  0.43118 | MeanQ:  12.360 | GradNorm:  1.3518\n",
      "[DEBUG] Step    785 | Ep   75 | Eps 0.845 | MeanReward(10ep):  11.000 | MeanLoss(100it):  0.42037 | MeanQ:  12.579 | GradNorm:  0.9088\n",
      "[DEBUG] Step    818 | Ep   76 | Eps 0.838 | MeanReward(10ep):  13.200 | MeanLoss(100it):  0.34999 | MeanQ:  13.021 | GradNorm:  1.3919\n",
      "[DEBUG] Step    855 | Ep   77 | Eps 0.831 | MeanReward(10ep):  15.700 | MeanLoss(100it):  0.29122 | MeanQ:  13.185 | GradNorm:  1.4592\n",
      "[DEBUG] Step    862 | Ep   78 | Eps 0.830 | MeanReward(10ep):  15.500 | MeanLoss(100it):  0.28353 | MeanQ:  13.663 | GradNorm:  1.5316\n",
      "[DEBUG] Step    870 | Ep   79 | Eps 0.828 | MeanReward(10ep):  15.700 | MeanLoss(100it):  0.27766 | MeanQ:  13.374 | GradNorm:  1.3971\n",
      "[DEBUG] Step    881 | Ep   80 | Eps 0.826 | MeanReward(10ep):  15.900 | MeanLoss(100it):  0.26595 | MeanQ:  13.146 | GradNorm:  0.9656\n",
      "[cartpole] Ep 80 Steps 881 RecentMean 15.90 Eps 0.826\n",
      "Eval @ Ep 80 and 881 steps => mean return 34.00\n",
      "[DEBUG] Step    887 | Ep   81 | Eps 0.825 | MeanReward(10ep):  14.800 | MeanLoss(100it):  0.26438 | MeanQ:  13.190 | GradNorm:  0.7908\n",
      "[DEBUG] Step    896 | Ep   82 | Eps 0.823 | MeanReward(10ep):  12.300 | MeanLoss(100it):  0.25633 | MeanQ:  13.256 | GradNorm:  1.0930\n",
      "[DEBUG] Step    902 | Ep   83 | Eps 0.822 | MeanReward(10ep):  12.000 | MeanLoss(100it):  0.25375 | MeanQ:  13.092 | GradNorm:  0.3941\n",
      "[DEBUG] Step    910 | Ep   84 | Eps 0.820 | MeanReward(10ep):  12.000 | MeanLoss(100it):  0.24334 | MeanQ:  13.331 | GradNorm:  1.4225\n",
      "[DEBUG] Step    920 | Ep   85 | Eps 0.818 | MeanReward(10ep):  12.500 | MeanLoss(100it):  0.23688 | MeanQ:  14.217 | GradNorm:  1.4564\n",
      "[DEBUG] Step    945 | Ep   86 | Eps 0.813 | MeanReward(10ep):  11.700 | MeanLoss(100it):  0.21350 | MeanQ:  14.055 | GradNorm:  1.3964\n",
      "[DEBUG] Step    971 | Ep   87 | Eps 0.808 | MeanReward(10ep):  10.600 | MeanLoss(100it):  0.19959 | MeanQ:  13.881 | GradNorm:  1.2506\n",
      "[DEBUG] Step    977 | Ep   88 | Eps 0.807 | MeanReward(10ep):  10.500 | MeanLoss(100it):  0.19652 | MeanQ:  14.244 | GradNorm:  0.2582\n",
      "[DEBUG] Step   1003 | Ep   89 | Eps 0.802 | MeanReward(10ep):  12.300 | MeanLoss(100it):  0.19325 | MeanQ:  14.027 | GradNorm:  0.4742\n",
      "[DEBUG] Step   1024 | Ep   90 | Eps 0.797 | MeanReward(10ep):  13.300 | MeanLoss(100it):  0.17504 | MeanQ:  14.671 | GradNorm:  1.4681\n",
      "[cartpole] Ep 90 Steps 1024 RecentMean 13.30 Eps 0.797\n",
      "[DEBUG] Step   1049 | Ep   91 | Eps 0.792 | MeanReward(10ep):  15.200 | MeanLoss(100it):  0.16551 | MeanQ:  15.210 | GradNorm:  1.3899\n",
      "[DEBUG] Step   1060 | Ep   92 | Eps 0.790 | MeanReward(10ep):  15.400 | MeanLoss(100it):  0.16253 | MeanQ:  14.604 | GradNorm:  0.7716\n",
      "[DEBUG] Step   1066 | Ep   93 | Eps 0.789 | MeanReward(10ep):  15.400 | MeanLoss(100it):  0.16100 | MeanQ:  14.657 | GradNorm:  1.4497\n",
      "[DEBUG] Step   1078 | Ep   94 | Eps 0.787 | MeanReward(10ep):  15.800 | MeanLoss(100it):  0.15933 | MeanQ:  15.028 | GradNorm:  1.3079\n",
      "[DEBUG] Step   1087 | Ep   95 | Eps 0.785 | MeanReward(10ep):  15.700 | MeanLoss(100it):  0.15753 | MeanQ:  14.608 | GradNorm:  0.3311\n",
      "[DEBUG] Step   1097 | Ep   96 | Eps 0.783 | MeanReward(10ep):  14.200 | MeanLoss(100it):  0.15721 | MeanQ:  14.828 | GradNorm:  0.4548\n",
      "[DEBUG] Step   1104 | Ep   97 | Eps 0.782 | MeanReward(10ep):  12.300 | MeanLoss(100it):  0.15160 | MeanQ:  15.199 | GradNorm:  1.3680\n",
      "[DEBUG] Step   1112 | Ep   98 | Eps 0.780 | MeanReward(10ep):  12.500 | MeanLoss(100it):  0.14356 | MeanQ:  15.870 | GradNorm:  1.3209\n",
      "[DEBUG] Step   1126 | Ep   99 | Eps 0.777 | MeanReward(10ep):  11.300 | MeanLoss(100it):  0.14309 | MeanQ:  15.373 | GradNorm:  1.0703\n",
      "[DEBUG] Step   1138 | Ep  100 | Eps 0.775 | MeanReward(10ep):  10.400 | MeanLoss(100it):  0.13993 | MeanQ:  15.697 | GradNorm:  1.4403\n",
      "[cartpole] Ep 100 Steps 1138 RecentMean 10.40 Eps 0.775\n",
      "Eval @ Ep 100 and 1138 steps => mean return 12.00\n",
      "[DEBUG] Step   1143 | Ep  101 | Eps 0.774 | MeanReward(10ep):   8.400 | MeanLoss(100it):  0.13922 | MeanQ:  15.827 | GradNorm:  0.5385\n",
      "[DEBUG] Step   1151 | Ep  102 | Eps 0.772 | MeanReward(10ep):   8.100 | MeanLoss(100it):  0.13742 | MeanQ:  15.866 | GradNorm:  1.3653\n",
      "[DEBUG] Step   1157 | Ep  103 | Eps 0.771 | MeanReward(10ep):   8.100 | MeanLoss(100it):  0.13687 | MeanQ:  15.514 | GradNorm:  0.9331\n",
      "[DEBUG] Step   1163 | Ep  104 | Eps 0.770 | MeanReward(10ep):   7.500 | MeanLoss(100it):  0.13642 | MeanQ:  15.594 | GradNorm:  1.5611\n",
      "[DEBUG] Step   1183 | Ep  105 | Eps 0.766 | MeanReward(10ep):   8.600 | MeanLoss(100it):  0.13398 | MeanQ:  15.317 | GradNorm:  0.3787\n",
      "[DEBUG] Step   1205 | Ep  106 | Eps 0.762 | MeanReward(10ep):   9.800 | MeanLoss(100it):  0.12942 | MeanQ:  15.702 | GradNorm:  1.3930\n",
      "[DEBUG] Step   1211 | Ep  107 | Eps 0.760 | MeanReward(10ep):   9.700 | MeanLoss(100it):  0.12559 | MeanQ:  15.995 | GradNorm:  1.4513\n",
      "[DEBUG] Step   1223 | Ep  108 | Eps 0.758 | MeanReward(10ep):  10.100 | MeanLoss(100it):  0.12207 | MeanQ:  16.224 | GradNorm:  1.4122\n",
      "[DEBUG] Step   1244 | Ep  109 | Eps 0.754 | MeanReward(10ep):  10.800 | MeanLoss(100it):  0.11547 | MeanQ:  16.400 | GradNorm:  1.4296\n",
      "[DEBUG] Step   1254 | Ep  110 | Eps 0.752 | MeanReward(10ep):  10.600 | MeanLoss(100it):  0.11346 | MeanQ:  16.015 | GradNorm:  0.1045\n",
      "[cartpole] Ep 110 Steps 1254 RecentMean 10.60 Eps 0.752\n",
      "[DEBUG] Step   1268 | Ep  111 | Eps 0.749 | MeanReward(10ep):  11.500 | MeanLoss(100it):  0.11197 | MeanQ:  16.235 | GradNorm:  0.8897\n",
      "[DEBUG] Step   1276 | Ep  112 | Eps 0.748 | MeanReward(10ep):  11.500 | MeanLoss(100it):  0.11158 | MeanQ:  16.045 | GradNorm:  0.4105\n",
      "[DEBUG] Step   1283 | Ep  113 | Eps 0.746 | MeanReward(10ep):  11.600 | MeanLoss(100it):  0.11119 | MeanQ:  16.010 | GradNorm:  0.4500\n",
      "[DEBUG] Step   1291 | Ep  114 | Eps 0.745 | MeanReward(10ep):  11.800 | MeanLoss(100it):  0.11145 | MeanQ:  16.245 | GradNorm:  0.5038\n",
      "[DEBUG] Step   1314 | Ep  115 | Eps 0.740 | MeanReward(10ep):  12.100 | MeanLoss(100it):  0.10470 | MeanQ:  16.589 | GradNorm:  0.9298\n",
      "[DEBUG] Step   1326 | Ep  116 | Eps 0.738 | MeanReward(10ep):  11.100 | MeanLoss(100it):  0.10225 | MeanQ:  16.193 | GradNorm:  0.5977\n",
      "[DEBUG] Step   1332 | Ep  117 | Eps 0.736 | MeanReward(10ep):  11.100 | MeanLoss(100it):  0.10191 | MeanQ:  16.502 | GradNorm:  0.9622\n",
      "[DEBUG] Step   1341 | Ep  118 | Eps 0.735 | MeanReward(10ep):  10.800 | MeanLoss(100it):  0.10052 | MeanQ:  16.545 | GradNorm:  1.2465\n",
      "[DEBUG] Step   1350 | Ep  119 | Eps 0.733 | MeanReward(10ep):   9.600 | MeanLoss(100it):  0.10021 | MeanQ:  16.281 | GradNorm:  0.4625\n",
      "[DEBUG] Step   1372 | Ep  120 | Eps 0.729 | MeanReward(10ep):  10.800 | MeanLoss(100it):  0.09916 | MeanQ:  16.317 | GradNorm:  0.5276\n",
      "[cartpole] Ep 120 Steps 1372 RecentMean 10.80 Eps 0.729\n",
      "Eval @ Ep 120 and 1372 steps => mean return 91.00\n",
      "[DEBUG] Step   1382 | Ep  121 | Eps 0.727 | MeanReward(10ep):  10.400 | MeanLoss(100it):  0.09863 | MeanQ:  16.259 | GradNorm:  0.3962\n",
      "[DEBUG] Step   1406 | Ep  122 | Eps 0.722 | MeanReward(10ep):  12.000 | MeanLoss(100it):  0.09486 | MeanQ:  16.557 | GradNorm:  1.4163\n",
      "[DEBUG] Step   1425 | Ep  123 | Eps 0.718 | MeanReward(10ep):  13.200 | MeanLoss(100it):  0.08955 | MeanQ:  16.572 | GradNorm:  0.8649\n",
      "[DEBUG] Step   1477 | Ep  124 | Eps 0.708 | MeanReward(10ep):  17.600 | MeanLoss(100it):  0.08364 | MeanQ:  16.638 | GradNorm:  0.7371\n",
      "[DEBUG] Step   1483 | Ep  125 | Eps 0.707 | MeanReward(10ep):  15.900 | MeanLoss(100it):  0.08309 | MeanQ:  16.724 | GradNorm:  0.2194\n",
      "[DEBUG] Step   1488 | Ep  126 | Eps 0.706 | MeanReward(10ep):  15.200 | MeanLoss(100it):  0.08300 | MeanQ:  16.710 | GradNorm:  0.9269\n",
      "[DEBUG] Step   1496 | Ep  127 | Eps 0.704 | MeanReward(10ep):  15.400 | MeanLoss(100it):  0.08249 | MeanQ:  16.600 | GradNorm:  0.3317\n",
      "[DEBUG] Step   1516 | Ep  128 | Eps 0.700 | MeanReward(10ep):  16.500 | MeanLoss(100it):  0.07941 | MeanQ:  17.004 | GradNorm:  1.3380\n",
      "[DEBUG] Step   1532 | Ep  129 | Eps 0.697 | MeanReward(10ep):  17.200 | MeanLoss(100it):  0.07602 | MeanQ:  17.339 | GradNorm:  0.4891\n",
      "[DEBUG] Step   1567 | Ep  130 | Eps 0.690 | MeanReward(10ep):  18.500 | MeanLoss(100it):  0.07346 | MeanQ:  16.850 | GradNorm:  0.2595\n",
      "[cartpole] Ep 130 Steps 1567 RecentMean 18.50 Eps 0.690\n",
      "[DEBUG] Step   1579 | Ep  131 | Eps 0.688 | MeanReward(10ep):  18.700 | MeanLoss(100it):  0.07220 | MeanQ:  17.192 | GradNorm:  0.6128\n",
      "[DEBUG] Step   1601 | Ep  132 | Eps 0.683 | MeanReward(10ep):  18.500 | MeanLoss(100it):  0.07071 | MeanQ:  17.209 | GradNorm:  0.8130\n",
      "[DEBUG] Step   1611 | Ep  133 | Eps 0.681 | MeanReward(10ep):  17.600 | MeanLoss(100it):  0.06818 | MeanQ:  17.665 | GradNorm:  0.2221\n",
      "[DEBUG] Step   1642 | Ep  134 | Eps 0.675 | MeanReward(10ep):  15.500 | MeanLoss(100it):  0.06482 | MeanQ:  17.403 | GradNorm:  0.3454\n",
      "[DEBUG] Step   1650 | Ep  135 | Eps 0.673 | MeanReward(10ep):  15.700 | MeanLoss(100it):  0.06364 | MeanQ:  17.556 | GradNorm:  1.1699\n",
      "[DEBUG] Step   1665 | Ep  136 | Eps 0.671 | MeanReward(10ep):  16.700 | MeanLoss(100it):  0.06240 | MeanQ:  17.362 | GradNorm:  0.1828\n",
      "[DEBUG] Step   1672 | Ep  137 | Eps 0.669 | MeanReward(10ep):  16.600 | MeanLoss(100it):  0.06224 | MeanQ:  17.551 | GradNorm:  0.4666\n",
      "[DEBUG] Step   1680 | Ep  138 | Eps 0.668 | MeanReward(10ep):  15.400 | MeanLoss(100it):  0.06179 | MeanQ:  17.483 | GradNorm:  0.5481\n",
      "[DEBUG] Step   1688 | Ep  139 | Eps 0.666 | MeanReward(10ep):  14.600 | MeanLoss(100it):  0.06095 | MeanQ:  17.449 | GradNorm:  0.2568\n",
      "[DEBUG] Step   1711 | Ep  140 | Eps 0.661 | MeanReward(10ep):  13.400 | MeanLoss(100it):  0.05807 | MeanQ:  18.014 | GradNorm:  0.3535\n",
      "[cartpole] Ep 140 Steps 1711 RecentMean 13.40 Eps 0.661\n",
      "Eval @ Ep 140 and 1711 steps => mean return 498.00\n",
      "[DEBUG] Step   1726 | Ep  141 | Eps 0.658 | MeanReward(10ep):  13.700 | MeanLoss(100it):  0.05738 | MeanQ:  17.896 | GradNorm:  1.3182\n",
      "[DEBUG] Step   1740 | Ep  142 | Eps 0.656 | MeanReward(10ep):  12.900 | MeanLoss(100it):  0.05614 | MeanQ:  17.781 | GradNorm:  0.4225\n",
      "[DEBUG] Step   1756 | Ep  143 | Eps 0.653 | MeanReward(10ep):  13.500 | MeanLoss(100it):  0.05543 | MeanQ:  17.866 | GradNorm:  0.6718\n",
      "[DEBUG] Step   1765 | Ep  144 | Eps 0.651 | MeanReward(10ep):  11.300 | MeanLoss(100it):  0.05503 | MeanQ:  17.808 | GradNorm:  0.4902\n",
      "[DEBUG] Step   1785 | Ep  145 | Eps 0.647 | MeanReward(10ep):  12.500 | MeanLoss(100it):  0.05406 | MeanQ:  17.784 | GradNorm:  0.1217\n",
      "[DEBUG] Step   1798 | Ep  146 | Eps 0.644 | MeanReward(10ep):  12.300 | MeanLoss(100it):  0.05366 | MeanQ:  17.888 | GradNorm:  0.0871\n",
      "[DEBUG] Step   1806 | Ep  147 | Eps 0.643 | MeanReward(10ep):  12.400 | MeanLoss(100it):  0.05316 | MeanQ:  18.021 | GradNorm:  0.9249\n",
      "[DEBUG] Step   1824 | Ep  148 | Eps 0.639 | MeanReward(10ep):  13.400 | MeanLoss(100it):  0.05247 | MeanQ:  17.969 | GradNorm:  0.5031\n",
      "[DEBUG] Step   1850 | Ep  149 | Eps 0.634 | MeanReward(10ep):  15.200 | MeanLoss(100it):  0.05222 | MeanQ:  18.041 | GradNorm:  0.5567\n",
      "[DEBUG] Step   1856 | Ep  150 | Eps 0.633 | MeanReward(10ep):  13.500 | MeanLoss(100it):  0.05207 | MeanQ:  18.009 | GradNorm:  0.4608\n",
      "[cartpole] Ep 150 Steps 1856 RecentMean 13.50 Eps 0.633\n",
      "[DEBUG] Step   1866 | Ep  151 | Eps 0.631 | MeanReward(10ep):  13.000 | MeanLoss(100it):  0.05176 | MeanQ:  17.980 | GradNorm:  0.2719\n",
      "[DEBUG] Step   1872 | Ep  152 | Eps 0.630 | MeanReward(10ep):  12.200 | MeanLoss(100it):  0.05168 | MeanQ:  18.025 | GradNorm:  0.1502\n",
      "[DEBUG] Step   1885 | Ep  153 | Eps 0.627 | MeanReward(10ep):  11.900 | MeanLoss(100it):  0.05143 | MeanQ:  18.063 | GradNorm:  0.1476\n",
      "[DEBUG] Step   1928 | Ep  154 | Eps 0.618 | MeanReward(10ep):  15.300 | MeanLoss(100it):  0.04980 | MeanQ:  18.277 | GradNorm:  0.2651\n",
      "[DEBUG] Step   1939 | Ep  155 | Eps 0.616 | MeanReward(10ep):  14.400 | MeanLoss(100it):  0.04906 | MeanQ:  18.205 | GradNorm:  0.5970\n",
      "[DEBUG] Step   1959 | Ep  156 | Eps 0.612 | MeanReward(10ep):  15.100 | MeanLoss(100it):  0.04844 | MeanQ:  18.212 | GradNorm:  0.4927\n",
      "[DEBUG] Step   1981 | Ep  157 | Eps 0.608 | MeanReward(10ep):  16.500 | MeanLoss(100it):  0.04749 | MeanQ:  18.148 | GradNorm:  0.1017\n",
      "[DEBUG] Step   1996 | Ep  158 | Eps 0.605 | MeanReward(10ep):  16.200 | MeanLoss(100it):  0.04712 | MeanQ:  18.159 | GradNorm:  0.3200\n",
      "[DEBUG] Step   2006 | Ep  159 | Eps 0.603 | MeanReward(10ep):  14.600 | MeanLoss(100it):  0.04679 | MeanQ:  18.306 | GradNorm:  0.6504\n",
      "[DEBUG] Step   2030 | Ep  160 | Eps 0.598 | MeanReward(10ep):  16.400 | MeanLoss(100it):  0.04460 | MeanQ:  18.383 | GradNorm:  0.0852\n",
      "[cartpole] Ep 160 Steps 2030 RecentMean 16.40 Eps 0.598\n",
      "Eval @ Ep 160 and 2030 steps => mean return 20.00\n",
      "[DEBUG] Step   2070 | Ep  161 | Eps 0.590 | MeanReward(10ep):  19.400 | MeanLoss(100it):  0.04288 | MeanQ:  18.318 | GradNorm:  0.3333\n",
      "[DEBUG] Step   2075 | Ep  162 | Eps 0.589 | MeanReward(10ep):  19.300 | MeanLoss(100it):  0.04287 | MeanQ:  18.372 | GradNorm:  0.9500\n",
      "[DEBUG] Step   2104 | Ep  163 | Eps 0.584 | MeanReward(10ep):  20.900 | MeanLoss(100it):  0.04345 | MeanQ:  18.312 | GradNorm:  0.7438\n",
      "[DEBUG] Step   2112 | Ep  164 | Eps 0.582 | MeanReward(10ep):  17.400 | MeanLoss(100it):  0.04335 | MeanQ:  18.421 | GradNorm:  0.6824\n",
      "[DEBUG] Step   2158 | Ep  165 | Eps 0.573 | MeanReward(10ep):  20.900 | MeanLoss(100it):  0.04229 | MeanQ:  18.264 | GradNorm:  0.2343\n",
      "[DEBUG] Step   2164 | Ep  166 | Eps 0.572 | MeanReward(10ep):  19.500 | MeanLoss(100it):  0.04236 | MeanQ:  18.224 | GradNorm:  0.4509\n",
      "[DEBUG] Step   2200 | Ep  167 | Eps 0.565 | MeanReward(10ep):  20.900 | MeanLoss(100it):  0.04309 | MeanQ:  18.283 | GradNorm:  0.8537\n",
      "[DEBUG] Step   2224 | Ep  168 | Eps 0.560 | MeanReward(10ep):  21.800 | MeanLoss(100it):  0.04290 | MeanQ:  18.350 | GradNorm:  0.1459\n",
      "[DEBUG] Step   2282 | Ep  169 | Eps 0.548 | MeanReward(10ep):  26.600 | MeanLoss(100it):  0.04238 | MeanQ:  18.349 | GradNorm:  0.1017\n",
      "[DEBUG] Step   2291 | Ep  170 | Eps 0.547 | MeanReward(10ep):  25.100 | MeanLoss(100it):  0.04223 | MeanQ:  18.467 | GradNorm:  0.1789\n",
      "[cartpole] Ep 170 Steps 2291 RecentMean 25.10 Eps 0.547\n",
      "[DEBUG] Step   2316 | Ep  171 | Eps 0.542 | MeanReward(10ep):  23.600 | MeanLoss(100it):  0.04228 | MeanQ:  18.409 | GradNorm:  0.4293\n",
      "[DEBUG] Step   2323 | Ep  172 | Eps 0.540 | MeanReward(10ep):  23.800 | MeanLoss(100it):  0.04263 | MeanQ:  18.571 | GradNorm:  1.3800\n",
      "[DEBUG] Step   2353 | Ep  173 | Eps 0.534 | MeanReward(10ep):  23.900 | MeanLoss(100it):  0.04275 | MeanQ:  18.589 | GradNorm:  0.9616\n",
      "[DEBUG] Step   2368 | Ep  174 | Eps 0.531 | MeanReward(10ep):  24.600 | MeanLoss(100it):  0.04260 | MeanQ:  18.696 | GradNorm:  0.1919\n",
      "[DEBUG] Step   2385 | Ep  175 | Eps 0.528 | MeanReward(10ep):  21.700 | MeanLoss(100it):  0.04249 | MeanQ:  18.717 | GradNorm:  0.4908\n",
      "[DEBUG] Step   2429 | Ep  176 | Eps 0.519 | MeanReward(10ep):  25.500 | MeanLoss(100it):  0.04180 | MeanQ:  18.745 | GradNorm:  0.2299\n",
      "[DEBUG] Step   2474 | Ep  177 | Eps 0.510 | MeanReward(10ep):  26.400 | MeanLoss(100it):  0.04098 | MeanQ:  18.666 | GradNorm:  0.2847\n",
      "[DEBUG] Step   2535 | Ep  178 | Eps 0.498 | MeanReward(10ep):  30.100 | MeanLoss(100it):  0.03928 | MeanQ:  18.839 | GradNorm:  0.7257\n",
      "[DEBUG] Step   2595 | Ep  179 | Eps 0.486 | MeanReward(10ep):  30.300 | MeanLoss(100it):  0.03729 | MeanQ:  18.910 | GradNorm:  0.1112\n",
      "[DEBUG] Step   2627 | Ep  180 | Eps 0.480 | MeanReward(10ep):  32.600 | MeanLoss(100it):  0.03572 | MeanQ:  19.022 | GradNorm:  0.4224\n",
      "[cartpole] Ep 180 Steps 2627 RecentMean 32.60 Eps 0.480\n",
      "Eval @ Ep 180 and 2627 steps => mean return 65.00\n",
      "[DEBUG] Step   2653 | Ep  181 | Eps 0.475 | MeanReward(10ep):  32.700 | MeanLoss(100it):  0.03442 | MeanQ:  18.851 | GradNorm:  0.1031\n",
      "[DEBUG] Step   2691 | Ep  182 | Eps 0.467 | MeanReward(10ep):  35.800 | MeanLoss(100it):  0.03331 | MeanQ:  18.997 | GradNorm:  0.0530\n",
      "[DEBUG] Step   2777 | Ep  183 | Eps 0.450 | MeanReward(10ep):  41.400 | MeanLoss(100it):  0.02907 | MeanQ:  19.031 | GradNorm:  0.1958\n",
      "[DEBUG] Step   2797 | Ep  184 | Eps 0.446 | MeanReward(10ep):  41.900 | MeanLoss(100it):  0.02881 | MeanQ:  19.025 | GradNorm:  0.4627\n",
      "[DEBUG] Step   2811 | Ep  185 | Eps 0.444 | MeanReward(10ep):  41.600 | MeanLoss(100it):  0.02846 | MeanQ:  19.075 | GradNorm:  0.1741\n",
      "[DEBUG] Step   2846 | Ep  186 | Eps 0.437 | MeanReward(10ep):  40.700 | MeanLoss(100it):  0.02787 | MeanQ:  19.134 | GradNorm:  0.2867\n",
      "[DEBUG] Step   2868 | Ep  187 | Eps 0.432 | MeanReward(10ep):  38.400 | MeanLoss(100it):  0.02759 | MeanQ:  19.110 | GradNorm:  0.4271\n",
      "[DEBUG] Step   2950 | Ep  188 | Eps 0.416 | MeanReward(10ep):  40.500 | MeanLoss(100it):  0.02584 | MeanQ:  19.160 | GradNorm:  0.4355\n",
      "[DEBUG] Step   3013 | Ep  189 | Eps 0.404 | MeanReward(10ep):  40.800 | MeanLoss(100it):  0.02542 | MeanQ:  19.130 | GradNorm:  0.6800\n",
      "[DEBUG] Step   3033 | Ep  190 | Eps 0.400 | MeanReward(10ep):  39.600 | MeanLoss(100it):  0.02542 | MeanQ:  19.039 | GradNorm:  0.1837\n",
      "[cartpole] Ep 190 Steps 3033 RecentMean 39.60 Eps 0.400\n",
      "[DEBUG] Step   3053 | Ep  191 | Eps 0.396 | MeanReward(10ep):  39.000 | MeanLoss(100it):  0.02605 | MeanQ:  19.199 | GradNorm:  0.4734\n",
      "[DEBUG] Step   3087 | Ep  192 | Eps 0.389 | MeanReward(10ep):  38.600 | MeanLoss(100it):  0.02595 | MeanQ:  19.120 | GradNorm:  0.4526\n",
      "[DEBUG] Step   3156 | Ep  193 | Eps 0.375 | MeanReward(10ep):  36.900 | MeanLoss(100it):  0.02533 | MeanQ:  19.269 | GradNorm:  0.2125\n",
      "[DEBUG] Step   3300 | Ep  194 | Eps 0.347 | MeanReward(10ep):  49.300 | MeanLoss(100it):  0.02368 | MeanQ:  19.289 | GradNorm:  0.4454\n",
      "[DEBUG] Step   3317 | Ep  195 | Eps 0.343 | MeanReward(10ep):  49.600 | MeanLoss(100it):  0.02353 | MeanQ:  19.214 | GradNorm:  0.5125\n",
      "[DEBUG] Step   3333 | Ep  196 | Eps 0.340 | MeanReward(10ep):  47.700 | MeanLoss(100it):  0.02348 | MeanQ:  19.255 | GradNorm:  0.0487\n",
      "[DEBUG] Step   3346 | Ep  197 | Eps 0.338 | MeanReward(10ep):  46.800 | MeanLoss(100it):  0.02338 | MeanQ:  19.366 | GradNorm:  0.2764\n",
      "[DEBUG] Step   3411 | Ep  198 | Eps 0.325 | MeanReward(10ep):  45.100 | MeanLoss(100it):  0.02310 | MeanQ:  19.243 | GradNorm:  0.9901\n",
      "[DEBUG] Step   3455 | Ep  199 | Eps 0.316 | MeanReward(10ep):  43.200 | MeanLoss(100it):  0.02156 | MeanQ:  19.279 | GradNorm:  0.3747\n",
      "[DEBUG] Step   3484 | Ep  200 | Eps 0.310 | MeanReward(10ep):  44.100 | MeanLoss(100it):  0.02130 | MeanQ:  19.293 | GradNorm:  0.5611\n",
      "[cartpole] Ep 200 Steps 3484 RecentMean 44.10 Eps 0.310\n",
      "Eval @ Ep 200 and 3484 steps => mean return 98.00\n",
      "[DEBUG] Step   3559 | Ep  201 | Eps 0.296 | MeanReward(10ep):  49.600 | MeanLoss(100it):  0.01996 | MeanQ:  19.274 | GradNorm:  0.0748\n",
      "[DEBUG] Step   3572 | Ep  202 | Eps 0.293 | MeanReward(10ep):  47.500 | MeanLoss(100it):  0.01959 | MeanQ:  19.393 | GradNorm:  0.0950\n",
      "[DEBUG] Step   3663 | Ep  203 | Eps 0.275 | MeanReward(10ep):  49.700 | MeanLoss(100it):  0.01801 | MeanQ:  19.495 | GradNorm:  0.1510\n",
      "[DEBUG] Step   3780 | Ep  204 | Eps 0.252 | MeanReward(10ep):  47.000 | MeanLoss(100it):  0.01650 | MeanQ:  19.422 | GradNorm:  0.1786\n",
      "[DEBUG] Step   3821 | Ep  205 | Eps 0.244 | MeanReward(10ep):  49.400 | MeanLoss(100it):  0.01600 | MeanQ:  19.569 | GradNorm:  0.5137\n",
      "[DEBUG] Step   4011 | Ep  206 | Eps 0.206 | MeanReward(10ep):  66.800 | MeanLoss(100it):  0.01661 | MeanQ:  19.575 | GradNorm:  1.0820\n",
      "[DEBUG] Step   4165 | Ep  207 | Eps 0.176 | MeanReward(10ep):  80.900 | MeanLoss(100it):  0.01552 | MeanQ:  19.640 | GradNorm:  0.3371\n",
      "[DEBUG] Step   4342 | Ep  208 | Eps 0.140 | MeanReward(10ep):  92.100 | MeanLoss(100it):  0.01248 | MeanQ:  19.716 | GradNorm:  0.4035\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import dataclasses\n",
    "\n",
    "def save_checkpoint(path, q_net, optimizer, cfg, stage_name: str):\n",
    "    torch.save({\n",
    "        'model': q_net.state_dict(),\n",
    "        'optimizer': optimizer.state_dict(),\n",
    "        'config': dataclasses.asdict(cfg),\n",
    "        'stage': stage_name,\n",
    "    }, path)\n",
    "    print('Saved checkpoint:', path)\n",
    "\n",
    "\n",
    "# === Training ===\n",
    "all_stage_logs = {}\n",
    "\n",
    "q_net = None\n",
    "target_net = None\n",
    "optimizer = None\n",
    "stage_index = 0\n",
    "\n",
    "# Make directories for outputs\n",
    "os.makedirs(\"logs\", exist_ok=True)\n",
    "os.makedirs(\"checkpoints\", exist_ok=True)\n",
    "\n",
    "for stage_env_name in cfg.curriculum:\n",
    "    print(f\"\\n=== Stage {stage_index+1}: {stage_env_name} ===\")\n",
    "    env = make_env(stage_env_name, seed=cfg.seed + stage_index)()\n",
    "    eval_env = make_env(stage_env_name, seed=cfg.seed + 100 + stage_index)()\n",
    "\n",
    "    q_net = build_q_network(env, cfg).to(DEVICE)\n",
    "    # if cfg.use_noisy_layer:\n",
    "    #     q_net.reset_noise()\n",
    "    #     target_net.reset_noise()\n",
    "    target_net = build_q_network(env, cfg).to(DEVICE)\n",
    "    hard_update(target_net, q_net)\n",
    "    optimizer = optim.Adam(q_net.parameters(), lr=cfg.lr)\n",
    "\n",
    "    replay = ReplayBuffer(cfg.buffer_capacity, env.single_observation_space.shape, DEVICE)\n",
    "\n",
    "    logs = {\n",
    "        'episode_rewards': [],\n",
    "        'losses': [],\n",
    "        'eps_history': [],\n",
    "        'eval': [],\n",
    "    }\n",
    "\n",
    "    obs, _ = env.reset()\n",
    "    ep_reward = 0\n",
    "    ep_steps = 0\n",
    "    episode_counter = 0\n",
    "    recent_ep_returns = []\n",
    "    total_steps = 0\n",
    "\n",
    "    trajectory_buffer = deque(maxlen=cfg.n_steps)\n",
    "\n",
    "    print('Starting training...')\n",
    "    while total_steps < cfg.max_steps_per_env:\n",
    "        if cfg.use_noisy_layer:\n",
    "            epsilon = 0.0  # No epsilon-greedy when using noisy networks\n",
    "        else:\n",
    "            epsilon = epsilon_by_step(total_steps, cfg)\n",
    "        action = select_action(q_net, obs, epsilon, env)\n",
    "        next_obs, reward, terminated, truncated, info = env.step(action)\n",
    "        reward = process_reward(reward, cfg)\n",
    "        reward = float(np.asarray(reward).reshape(-1)[0])\n",
    "        done = bool(np.asarray(terminated).reshape(-1)[0] or np.asarray(truncated).reshape(-1)[0])\n",
    "\n",
    "        trajectory_buffer.append((obs.copy(), int(action), reward, next_obs.copy(), done))\n",
    "\n",
    "        if len(trajectory_buffer) == cfg.n_steps or done:\n",
    "            n_step_reward = 0\n",
    "\n",
    "            # N-step return calculation\n",
    "            for i, (_, _, reward, _, _) in enumerate(trajectory_buffer):\n",
    "                n_step_reward += (cfg.gamma ** i) * reward\n",
    "            first_obs, first_action, _, _, first_done = trajectory_buffer[0]\n",
    "            _, _, _, last_next_obs, _ = trajectory_buffer[-1]  # Get the last next_obs (n steps ahead)\n",
    "\n",
    "            # Push n-step transition to replay buffer\n",
    "            replay.push(first_obs, first_action, n_step_reward, last_next_obs, first_done)\n",
    "\n",
    "            # Reset buffer if done before filling all n steps\n",
    "            if done:\n",
    "                trajectory_buffer.clear()\n",
    "\n",
    "        obs = next_obs.copy()\n",
    "        ep_reward += reward\n",
    "        ep_steps += 1\n",
    "        total_steps += 1\n",
    "\n",
    "        # Training\n",
    "        if total_steps % cfg.train_freq == 0:\n",
    "            loss = train_step(q_net, target_net, optimizer, replay, cfg)\n",
    "            if loss is not None:\n",
    "                logs['losses'].append(loss)\n",
    "\n",
    "        # Target update\n",
    "        if total_steps % cfg.target_update_interval == 0:\n",
    "            hard_update(target_net, q_net)\n",
    "\n",
    "        logs['eps_history'].append(epsilon)\n",
    "\n",
    "        if done or ep_steps >= cfg.max_episode_len:\n",
    "            logs['episode_rewards'].append(ep_reward)\n",
    "            recent_ep_returns.append(ep_reward)\n",
    "            episode_counter += 1\n",
    "\n",
    "            # --- DEBUG PRINT ---\n",
    "            mean_loss = np.mean(logs['losses'][-100:]) if logs['losses'] else 0.0\n",
    "            mean_reward = np.mean(recent_ep_returns[-cfg.log_interval_episodes:]) if recent_ep_returns else 0.0\n",
    "            with torch.no_grad():\n",
    "                q_sample = q_net(torch.FloatTensor(obs).unsqueeze(0).to(DEVICE))\n",
    "                mean_q = q_sample.mean().item()\n",
    "            grads = [p.grad.norm().item() for p in q_net.parameters() if p.grad is not None]\n",
    "            grad_norm = np.mean(grads) if grads else 0.0\n",
    "            print(f\"[DEBUG] Step {total_steps:6d} | Ep {episode_counter:4d} | \"\n",
    "                  f\"Eps {epsilon:.3f} | MeanReward(10ep): {mean_reward:7.3f} | \"\n",
    "                  f\"MeanLoss(100it): {mean_loss:8.5f} | \"\n",
    "                  f\"MeanQ: {mean_q:7.3f} | GradNorm: {grad_norm:7.4f}\")\n",
    "\n",
    "            if episode_counter % cfg.log_interval_episodes == 0:\n",
    "                mean_recent = np.mean(recent_ep_returns[-cfg.log_interval_episodes:])\n",
    "                print(f\"[{stage_env_name}] Ep {episode_counter} Steps {total_steps} \"\n",
    "                      f\"RecentMean {mean_recent:.2f} Eps {epsilon:.3f}\")\n",
    "\n",
    "            obs, _ = env.reset()\n",
    "            ep_reward = 0\n",
    "            ep_steps = 0\n",
    "\n",
    "            # Periodic evaluation (every 2*log_interval episodes if we have progress)\n",
    "            if episode_counter % (cfg.log_interval_episodes * 2) == 0:\n",
    "                eval_ret = evaluate(eval_env, q_net, cfg, cfg.eval_episodes)\n",
    "                logs['eval'].append((total_steps, eval_ret))\n",
    "                print(f\"Eval @ Ep {episode_counter} and {total_steps} steps => mean return {eval_ret:.2f}\")\n",
    "\n",
    "    # --- Save logs and model after each stage ---\n",
    "    all_stage_logs[stage_env_name] = logs\n",
    "\n",
    "    # Save training logs\n",
    "    df = pd.DataFrame({\n",
    "        'episode_rewards': logs['episode_rewards'],\n",
    "        'losses': logs['losses'][:len(logs['episode_rewards'])],\n",
    "        'eps_history': logs['eps_history'][:len(logs['episode_rewards'])],\n",
    "    })\n",
    "    train_csv = f\"logs/{stage_env_name}_training_log.csv\"\n",
    "    df.to_csv(train_csv, index=False)\n",
    "    print(f\"Saved training data → {train_csv}\")\n",
    "\n",
    "    # Save evaluation results\n",
    "    eval_df = pd.DataFrame(logs['eval'], columns=['total_steps', 'eval_return'])\n",
    "    eval_csv = f\"logs/{stage_env_name}_eval_log.csv\"\n",
    "    eval_df.to_csv(eval_csv, index=False)\n",
    "    print(f\"Saved eval data → {eval_csv}\")\n",
    "\n",
    "    # Save model checkpoint\n",
    "    checkpoint_path = f\"checkpoints/{stage_env_name}_final.pt\"\n",
    "    save_checkpoint(checkpoint_path, q_net, optimizer, cfg, stage_env_name)\n",
    "\n",
    "    stage_index += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Visualization Per Stage\n",
    "helper code for plots, feel free to modify or use different tools, e.g. WandB.\n",
    "Plots: Episode rewards, Epsilon schedule, Loss curve, and Evaluation returns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def moving_average(x, w):\n",
    "    # Ensure x is a flattened 1D numpy array to handle lists like [[1],[2],[3]]\n",
    "    x = np.array(x).flatten()\n",
    "    if len(x) < w:\n",
    "        return x\n",
    "    return np.convolve(x, np.ones(w)/w, mode='valid')\n",
    "\n",
    "for stage, logs in all_stage_logs.items():\n",
    "    rewards = logs['episode_rewards']\n",
    "    losses = logs['losses']\n",
    "    eps_hist = logs['eps_history']\n",
    "    evals = logs['eval']\n",
    "    print(f\"\\n=== {stage} ===\")\n",
    "    plt.figure(figsize=(14,4))\n",
    "    # Rewards\n",
    "    plt.subplot(1,3,1)\n",
    "    plt.title(f'{stage} Rewards')\n",
    "    plt.plot(rewards, alpha=0.5, label='Return')\n",
    "    ma = moving_average(rewards, 100)\n",
    "    if len(ma) != len(rewards):\n",
    "        plt.plot(range(len(rewards)-len(ma), len(rewards)), ma, label='MA(20)')\n",
    "    plt.legend()\n",
    "    plt.xlabel('Episode')\n",
    "\n",
    "    # Epsilon\n",
    "    plt.subplot(1,3,2)\n",
    "    plt.title('Epsilon')\n",
    "    plt.plot(eps_hist)\n",
    "    plt.xlabel('Step')\n",
    "\n",
    "    # Losses\n",
    "    plt.subplot(1,3,3)\n",
    "    plt.title('Loss (smoothed)')\n",
    "    if len(losses) > 0:\n",
    "        lma = moving_average(losses, 200)\n",
    "        plt.plot(losses, alpha=0.3)\n",
    "        plt.plot(range(len(losses)-len(lma), len(losses)), lma, label='MA(200)')\n",
    "        plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    if len(evals) > 0:\n",
    "        xs = [x for x,_ in evals]\n",
    "        ys = [y for _,y in evals]\n",
    "        plt.figure()\n",
    "        plt.title(f'{stage} Evaluation Returns')\n",
    "        plt.plot(xs, ys, marker='o')\n",
    "        plt.xlabel('Env Steps')\n",
    "        plt.ylabel('Mean Return (greedy)')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Group Project: Extensions and Analysis\n",
    "\n",
    "This assignment is a group project. After familiarizing yourselves with the baseline DQN code, your group will divide the following implementation tasks. Once each part is complete, you will merge your code and collaborate on a final report that analyzes and compares the performance of each extension.\n",
    "\n",
    "---\n",
    "### **Part 1: Individual Implementation Tasks**\n",
    "\n",
    "Each group member must implement one of the following sets of extensions. You will need to modify the core components (`MLPQ`, `ReplayBuffer`, `train_step`) to support these new algorithms, using the `cfg` object to toggle them on and off.\n",
    "\n",
    "---\n",
    "#### **Extension A: Double DQN & Dueling DQN**\n",
    "\n",
    "**Assigned to:** *[Student A Name Here]*\n",
    "\n",
    "This set of tasks focuses on improving the Q-value estimation and network architecture.\n",
    "\n",
    "**TODO (A1): Implement Double DQN**\n",
    "In the `train_step` function, implement the logic inside the `if cfg.use_double_dqn:` block. This involves decoupling action selection from evaluation to mitigate overestimation bias by using the **online network** to select the best next action and the **target network** to evaluate its value.\n",
    "\n",
    "**TODO (A2): Implement Dueling DQN**\n",
    "In the `MLPQ` class, modify the `__init__` and `forward` methods to support a dueling architecture. You will need to create separate `value_head` and `advantage_head` layers and combine their outputs using the formula: `Q(s,a) = V(s) + (A(s,a) - mean(A(s,:)))`.\n",
    "\n",
    "---\n",
    "#### **Extension B: Prioritized Experience Replay (PER)**\n",
    "\n",
    "**Assigned to:** *[Student B Name Here]*\n",
    "\n",
    "This task replaces the uniform `ReplayBuffer` with a more intelligent sampling strategy that prioritizes \"surprising\" transitions.\n",
    "\n",
    "**TODO (B1): Implement `PrioritizedReplayBuffer`**\n",
    "Create a new `PrioritizedReplayBuffer` class. This class must manage transition priorities (based on TD-error), sample transitions according to these priorities, and compute importance sampling (IS) weights to correct for the biased sampling. Implementing a **SumTree** is the standard approach for this.\n",
    "\n",
    "**TODO (B2): Integrate PER into `train_step`**\n",
    "Modify the training loop to use your new buffer. In `train_step`, you must use the IS weights to scale the loss for each transition and then call a method on your buffer to update the priorities of the sampled transitions with their new TD-errors.\n",
    "\n",
    "---\n",
    "#### **Extension C: N-Step Returns & Noisy Networks**\n",
    "\n",
    "**Assigned to:** *[Student C Name Here]*\n",
    "\n",
    "This set of tasks focuses on improving the TD target and the agent's exploration strategy.\n",
    "\n",
    "**TODO (C1): Implement N-Step Returns**\n",
    "Modify the training loop and `train_step` to use N-step returns. This involves temporarily storing the last `N` transitions to calculate the discounted N-step reward (`R_n`) and updating the target formula in `train_step` to use `γ^N` for bootstrapping.\n",
    "\n",
    "**TODO (C2): Implement Noisy Networks**\n",
    "Replace epsilon-greedy exploration with learned exploration. Create a custom `NoisyLinear` PyTorch layer that adds parametric noise to its weights. Replace the final linear layers of your `MLPQ` with this new layer and disable epsilon-greedy exploration in the main loop when this feature is active.\n",
    "\n",
    "---\n",
    "### **Optional Extension**\n",
    "\n",
    "#### **Vectorized Environments**\n",
    "For a deeper challenge, modify the entire pipeline to support vectorized environments. This involves changing `cfg.vector_envs` to > 1 and refactoring the training loop to handle batched operations for action selection, environment stepping, and episode tracking. This is a highly effective method for speeding up training but requires careful management of parallel data streams.\n",
    "\n",
    "---\n",
    "### **Part 2: Final Report (Group Task)**\n",
    "\n",
    "Your group's final submission should be a report that includes the following:\n",
    "\n",
    "**1. Experimental Analysis:**\n",
    "*   Run experiments on the Comparing environment comparing your baseline DQN against each of the implemented extensions (A, B, and C) and all extension combined on the challenge environment.\n",
    "*   Generate and include plots for each experiment (Episode Rewards, Loss, etc.).\n",
    "*  For the replay buffer size and N-Step Returns, test out 2-4 different values and reflect on the difference in performance. You may change other hyperparameters as needed to get good performance, also put the highlights in the report.\n",
    "*   Analyze the plots: Reflect how you can see the improvements for each extension. Which extension provided the biggest performance boost or the most stable training? Justify your claims with evidence.\n",
    "\n",
    "**2. Conceptual Questions:**\n",
    "\n",
    "*   **Q1:** Explain \"maximization bias\" in Q-learning. How does your **Double DQN** implementation address it?\n",
    "*   **Q2:** What is the theoretical motivation for the **Dueling DQN** architecture? Why is the special averaging mechanism important?\n",
    "*   **Q3:** Why is uniform sampling from the replay buffer inefficient? How do the **importance sampling weights** in **PER** correct for the biased sampling you introduced?\n",
    "*   **Q4:** Explain the difference between epsilon-greedy exploration and the exploration provided by **Noisy Networks**. What is an advantage of the latter?\n",
    "*   **Q5:** How does changing 'N' in **N-Step Returns** affect the bias-variance trade-off in your Q-learning updates?\n",
    "\n",
    "**3. Reflection:**\n",
    "*   Briefly discuss the biggest challenge your group faced during implementation and how you solved it.\n",
    "*   Were some of the results unexpected, if so in what way?\n",
    "*   If you implemented the vectorized environments, describe the performance improvement you observed in terms of wall-clock time.\n",
    "\n",
    "**4. Code Submission:**\n",
    "*   Submit your complete code with all extensions implemented. Ensure it is well-commented and organized.\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize agent\n",
    "helper code to visualize your trained agent in the notebook. You may modify this code as needed or visualize it in a different way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyvirtualdisplay import Display\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display, clear_output\n",
    "import time\n",
    "import os\n",
    "import pyscreenshot as ImageGrab\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# --- Visualization Setup (from our debugging) ---\n",
    "seed = 0\n",
    "steps_to_show = 1000 # Your original value\n",
    "game_width = 576\n",
    "game_height = 330\n",
    "bbox = (0, 0, game_width, game_height)\n",
    "\n",
    "# Initialize variables to None before the try block for safe cleanup\n",
    "vdisplay = None\n",
    "env = None\n",
    "\n",
    "original_dir = os.getcwd()\n",
    "pufferlib_dir = '/opt/pufferlib'\n",
    "# -------------------------------------------------\n",
    "def first_obs(x):\n",
    "    x = np.asarray(x)\n",
    "    return x[0] if x.ndim > 1 else x\n",
    "\n",
    "# This is your agent's decision-making function, it's perfect as is.\n",
    "def policy_net(st):\n",
    "    return q_net(st)\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def greedy_action(state_np):\n",
    "    st = torch.as_tensor(state_np, dtype=torch.float32, device=DEVICE).unsqueeze(0)\n",
    "    q = policy_net(st)\n",
    "    print(q)\n",
    "    return int(q.argmax(dim=1).item())\n",
    "\n",
    "# The robust 'try...finally' block for safe execution and cleanup\n",
    "try:\n",
    "    vdisplay = Display(visible=False, size=(game_width, game_height))\n",
    "    vdisplay.start()\n",
    "\n",
    "    # os.chdir(pufferlib_dir)\n",
    "    # env = breakout.Breakout(num_envs=1, render_mode='human', seed=seed)\n",
    "    # env = cartpole.Cartpole(num_envs=1, render_mode='human', seed=seed)\n",
    "    env = cartpole.Cartpole(num_envs=1, render_mode='human', seed=seed)\n",
    "    \n",
    "    fig, ax = plt.subplots(1)\n",
    "\n",
    "    current_directory = os.getcwd()\n",
    "    print(f\"Current directory: {current_directory}\")\n",
    "    \n",
    "    # List all files and directories in the current directory\n",
    "    contents = os.listdir(current_directory)\n",
    "    print(\"Contents of the directory:\")\n",
    "    for item in contents:\n",
    "        print(item)\n",
    "\n",
    "    q_net = build_q_network(env, cfg).to(DEVICE)\n",
    "    optimizer = torch.optim.Adam(q_net.parameters(), lr=cfg.lr)  # Placeholder optimizer for load_checkpoint\n",
    "    checkpoint_path = 'checkpoint_cartpole_stage0.pth'  # Adjust path to your saved checkpoint\n",
    "    if os.path.exists(checkpoint_path):\n",
    "        print(f\"File found at: {checkpoint_path}\")\n",
    "    \n",
    "    load_checkpoint(checkpoint_path, q_net, optimizer)  # Load the trained model\n",
    "\n",
    "    raise Error\n",
    "    \n",
    "    q_net.eval()  # Set to evaluation mode\n",
    "\n",
    "    # --- Your Agent's Logic Starts Here ---\n",
    "    obs, _ = env.reset(seed)\n",
    "    state = first_obs(obs).astype(np.float32)\n",
    "    episode_return = 0.0\n",
    "\n",
    "    print(\"Starting agent visualization... Press 'Stop' to interrupt cleanly.\")\n",
    "    print(\"IMPORTANT: After interrupting, you MUST restart the kernel before running this cell again.\")\n",
    "\n",
    "    for t in range(steps_to_show):\n",
    "        # The inner 'try...except' for clean interruption\n",
    "        try:\n",
    "            # Get action from your policy instead of random\n",
    "            # q_net = build_q_network(env, cfg).to(DEVICE)\n",
    "            a = greedy_action(state)\n",
    "\n",
    "            # Step the environment\n",
    "            obs, reward, term, trunc, _info = env.step(a)\n",
    "\n",
    "            # --- Apply the Visualization Pipeline ---\n",
    "            # 1. Render the game to the invisible virtual display\n",
    "            env.render()\n",
    "\n",
    "            # 2. Grab a cropped screenshot of the virtual display\n",
    "            frame = ImageGrab.grab(bbox=bbox)\n",
    "\n",
    "            # 3. Display the screenshot in the notebook\n",
    "            clear_output(wait=True)\n",
    "            ax.imshow(frame)\n",
    "            ax.axis('off')\n",
    "            display(fig)\n",
    "\n",
    "            time.sleep(0.01)\n",
    "            # --- End of Visualization Pipeline ---\n",
    "\n",
    "            # Continue with your agent's logic\n",
    "            state = first_obs(obs).astype(np.float32)\n",
    "            episode_return += float(first_obs(reward))\n",
    "            done = bool(first_obs(term)) or bool(first_obs(trunc))\n",
    "            if done:\n",
    "                print(\"Episode finished.\")\n",
    "                break\n",
    "\n",
    "        except KeyboardInterrupt:\n",
    "            print(\"\\n\\nInterrupted by user. The kernel is now in an unstable state and must be restarted.\")\n",
    "            break\n",
    "\n",
    "    print(f'Greedy policy return: {episode_return:.2f}')\n",
    "\n",
    "finally:\n",
    "    # This block is ALWAYS executed, ensuring a safe shutdown.\n",
    "    print(\"Cleaning up resources...\")\n",
    "    os.chdir(original_dir)\n",
    "\n",
    "    if env is not None:\n",
    "        env.close()\n",
    "\n",
    "    time.sleep(0.5)\n",
    "\n",
    "    if vdisplay is not None:\n",
    "        vdisplay.stop()\n",
    "    print(\"Cleanup complete. If you interrupted the cell, please restart the kernel now.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(path, q_net, optimizer, cfg: Config, stage_name: str):\n",
    "    torch.save({\n",
    "        'model': q_net.state_dict(),\n",
    "        'optimizer': optimizer.state_dict(),\n",
    "        'config': dataclasses.asdict(cfg),\n",
    "        'stage': stage_name,\n",
    "    }, path)\n",
    "    print('Saved checkpoint:', path)\n",
    "\n",
    "def load_checkpoint(path, q_net, optimizer):\n",
    "    data = torch.load(path, map_location=DEVICE)\n",
    "    q_net.load_state_dict(data['model'])\n",
    "    optimizer.load_state_dict(data['optimizer'])\n",
    "    print('Loaded checkpoint from', path)\n",
    "    print(data['config'])\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "deepnote_notebook_id": "43ce95ddb5754e06b99d62b9ae367324",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
