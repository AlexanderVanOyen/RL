{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Assignment 1: Deep Q-Networks (DQN) with [PufferLib Ocean Environments](https://puffer.ai/ocean.html)"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "*This assignment must be completed solely by the members of your group. Sharing of code between groups is strictly prohibited. However, you are allowed to discuss general solution approaches or share publicly available resources with members of other groups. Therefore, clearly indicate which public resources you consulted and/or copied code from. Any plagiarism between groups will result in the initiation of a fraud procedure with the director of education.*"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Overview\n",
    "\n",
    "In this notebook, your group will implement a fully vectorized Deep Q-Network (DQN) agent and several of its key improvements, inspired by the [\"Rainbow\"](https://arxiv.org/abs/1710.02298) paper. The project is divided into a foundational group task followed by individual implementation of specific DQN extensions. Your goal is to compare the performance of these components and then combine them into the ultimate rainbow-lite agent.\n",
    "\n",
    "### Project Structure:\n",
    "1.  **Foundational (Group Task):** The entire group will first collaborate to right the necessary code to  make simple DQN work.\n",
    "2.  **Individual Extensions:** Once the baseline is complete, each group member will be assigned one of the following algorithmic extensions to implement:\n",
    "    *   **Extension A:** Double DQN & Dueling DQN (Architectural Improvements)\n",
    "    *   **Extension B:** Prioritized Experience Replay (PER) (Advanced Sampling)\n",
    "    *   **Extension C:** N-Step Returns & Noisy Networks (Target & Exploration Improvements)\n",
    "3.  **Analysis (Group Task):** The group will integrate all components, run a comparative analysis on the `breakout` environment, and collaboratively answer the conceptual and reflection questions in a final report.\n",
    "\n",
    "Cells with `TODO` indicate where you must add or adjust code. However feel free to modify any part of the code to improve clarity, efficiency, or performance. You are encouraged to experiment with hyperparameters and other design choices to optimize your agent's learning.\n",
    "\n",
    "---\n",
    "\n",
    "### Environment Tiers:\n",
    "You will use three types of environments from `pufferlib.ocean`:\n",
    "1.  **Debug:** `squared` (fast iterations; verify your code  is correct, your agent should learn within seconds and become optimal around 2 minutes, max episode score is 1).\n",
    "2.  **Comparing:** `cartpole` (fast iterations; verify your extensions and evaluate the agent, learning signs should be clear within 2 minutes but optimal agent could take 30+ minutes, max episode score is 199).\n",
    "3. **Challenge:** `breakout` (focus on performance, and diagnostics, an optimal agent can take multiple hours, you will get bonus points if you find an optimal agent in this environment, max episode score is 864).\n",
    "\n",
    "You are allowed to experiment with other environments from `pufferlib.ocean` if you wish, but the above three are mandatory.\n",
    "\n",
    "---\n",
    "\n",
    "### Report\n",
    "We expect you to write the report in a separate document and submit both the notebook and the report on Ufora. Remember that a plot often tells more than a thousand words. When you explain somethings or you show your results, try to add figures to accompany the text. Go beyond just souly describing what you did or how the techniques work but additionally, share your observations, reflect on why things turned out the way they did, and help us understand the story behind your findings.\n",
    "\n",
    "The report has a soft limit of 8 pages. You are welcome to go over this limit if you keep your writing concise and make sure any extra content is relevant. If you are highly motivated and want to test many different things, feel free to share your findings as long as you follow the requirements mentioned earlier.\n",
    "\n",
    "If your group is smaller than 3 people, you can choose to implement more than one extension per person. In that case, please clearly indicate who did what in the report.\n",
    "\n",
    "**Deadline:** October 26, 2025, 23:59\n",
    "\n",
    "\n",
    "### Office hours\n",
    "We will hold weekly office hours to help you with questions about the assignment. Your are more then welcome between 13:30 and 16:00. We reserved IDLab9 (IGent) for you. If we are not there, you can find us in our offices on the 10th floor (IGent): 200.026 (Elias and Thibault), 200.031 (Ciem).\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### Docker\n",
    "To ensure a consistent and hassle-free setup, we have prepared a Docker image with all necessary dependencies pre-installed. This is the recommended way to run the notebook to avoid issues with package versions.\n",
    "\n",
    "You can also use there container more information at [puffertank](https://github.com/PufferAI/PufferTank).\n",
    "\n",
    "The Docker image is available on Docker Hub:\n",
    "*   **Image:** `ciemcornelissen/puffer-notebook:latest`\n",
    "*   **URL:** [https://hub.docker.com/r/ciemcornelissen/puffer-notebook](https://hub.docker.com/r/ciemcornelissen/puffer-notebook)\n",
    "\n",
    "You can use this image in several ways. Below are instructions for three common setups: VS Code (recommended for local use), Deepnote for a cloud-based environment, and locally with classic Jupyter.\n",
    "\n",
    "\n",
    "### Option 1: Local Development with VS Code\n",
    "\n",
    "This is the most seamless way to work locally. VS Code's \"Dev Containers\" extension allows you to open your project folder directly inside the running container, giving you access to a fully integrated terminal, file editor, and Jupyter renderer.\n",
    "\n",
    "**Prerequisites:**\n",
    "*   [Docker Desktop](https://www.docker.com/products/docker-desktop/) installed and running.\n",
    "*   [Visual Studio Code](https://code.visualstudio.com/) installed.\n",
    "*   The [Dev Containers extension](https://marketplace.visualstudio.com/items?itemName=ms-vscode-remote.remote-containers) installed in VS Code.\n",
    "\n",
    "**Instructions:**\n",
    "\n",
    "1.  **Pull and Run the Container:** Open a terminal in your project directory (where this notebook is located) and run the appropriate command below. **Keep this terminal window open.**\n",
    "\n",
    "    *   **For Linux, Windows (WSL), and Intel-based Macs:**\n",
    "        ```bash\n",
    "        docker run -it --rm -p 8888:8888 -v \"$(pwd)\":/app --name puffer-dev ciemcornelissen/puffer-notebook:latest\n",
    "\n",
    "    *   **For Apple Silicon (M1/M2/M3) Macs:** You must add the `--platform` flag to emulate the correct architecture.\n",
    "        ```bash\n",
    "        docker run -it --rm --platform linux/amd64 -p 8888:8888 -v \"$(pwd)\":/app --name puffer-dev ciemcornelissen/puffer-notebook:latest\n",
    "\n",
    "    *   *(Note: We use `--name puffer-dev` to give the container an easy-to-find name).*\n",
    "\n",
    "2.  **Attach VS Code to the Container:**\n",
    "    *   Open VS Code.\n",
    "    *   Attach vsual studio code to the running container:\n",
    "        1.  Click on the container extension in VS Code.\n",
    "        2.  Select **\"Attach to Running Container...\"**.\n",
    "        3.  Choose the container named `puffer-dev`.\n",
    "    *   Or open the Command Palette (`Ctrl+Shift+P` on Windows/Linux, `Cmd+Shift+P` on Mac).\n",
    "    *   Type and select **\"Dev Containers: Attach to Running Container...\"**.\n",
    "    *   Choose the right container from the list.\n",
    "\n",
    "\n",
    "3.  **Start Working:** A new VS Code window will open, connected to the container. Click **\"Open Folder\"** to open your project files. You can now edit code, run the notebook, and use the terminal as if you were running natively inside the correct environment.\n",
    "\n",
    "**If you can not select a kernel when trying to run code in the notebook then you need to update the jupyter extension of vsc. This can be done by clicking on the extensions tab on the left and searching for jupyter. Then click on the little gear icon and select install a specific version and choose the newest version.**\n",
    "\n",
    "### Option 2: Cloud Development with [Deepnote](https://deepnote.com/)\n",
    "\n",
    "If you prefer not to install Docker locally, you can use Deepnote to run the environment in the cloud.\n",
    "\n",
    "**Instructions:**\n",
    "\n",
    "1.  In your Deepnote project, navigate to the **Environment** tab in the left sidebar at the bottem under machine.\n",
    "2.  Click on the **\"Set up a new Docker image\"**.\n",
    "3.  In the \"Docker image\" field, paste the image name:\n",
    "    ```\n",
    "    ciemcornelissen/puffer-notebook:latest\n",
    "\n",
    "4.  Deepnote will pull the image. Once it's ready, you'll be ready to work.\n",
    "\n",
    "\n",
    "\n",
    "### Option 3: Local Development with Classic Jupyter\n",
    "\n",
    "This method uses the command line to start a Jupyter server, which you access through your web browser.\n",
    "\n",
    "**Instructions:**\n",
    "\n",
    "1.  **Pull the Image:**\n",
    "    ```bash\n",
    "    docker pull ciemcornelissen/puffer-notebook:latest\n",
    "\n",
    "2.  **Run the Container:** Run the command below from your project directory.\n",
    "\n",
    "    *   **For Linux, Windows (WSL), and Intel-based Macs:**\n",
    "        ```bash\n",
    "        docker run -it --rm -p 8888:8888 -v \"$(pwd)\":/app ciemcornelissen/puffer-notebook:latest\n",
    "\n",
    "    *   **For Apple Silicon (M1/M2/M3) Macs:**\n",
    "        ```bash\n",
    "        docker run -it --rm --platform linux/amd64 -p 8888:8888 -v \"$(pwd)\":/app ciemcornelissen/puffer-notebook:latest\n",
    "\n",
    "3.  **Access Jupyter:** Your terminal will display a URL (e.g., `http://127.0.0.1:8888/lab?token=...`). Copy and paste this full URL into your web browser to start the notebook. Your files will be in the `/app` directory.\n",
    "\n",
    "\n",
    "\n",
    "### **Important Note for Apple Silicon (M1/M2/M3/M4) Users**\n",
    "\n",
    "The Docker image is built for the `amd64` (Intel/AMD) architecture. If you are using a Mac with Apple Silicon, you must tell Docker to emulate this architecture by adding the `--platform linux/amd64` flag.\n",
    "\n",
    "---"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 1.1 Setup"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "#when using deepnote and you get the error no module pufferlib run this cell\n",
    "\n",
    "# try:\n",
    "#     import __editable___pufferlib_3_0_0_finder as _pf\n",
    "#     _pf.install()          # registers the module loader\n",
    "#     import pufferlib\n",
    "#     print(\"Loaded:\", pufferlib.__file__)\n",
    "# except Exception as e:\n",
    "#     print(\"Failed to manually activate editable hook:\", e)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import random, dataclasses\n",
    "import math\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Optional, Dict, Any\n",
    "from collections import deque\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Import ocean environments, you can import more if you want to experiment with other envs\n",
    "from pufferlib.ocean.squared import squared\n",
    "from pufferlib.ocean.pong import pong\n",
    "from pufferlib.ocean.pacman import pacman\n",
    "from pufferlib.ocean.enduro import enduro\n",
    "from pufferlib.ocean.tetris import tetris\n",
    "from pufferlib.ocean.breakout import breakout\n",
    "from pufferlib.ocean.cartpole import cartpole\n",
    "\n",
    "import gymnasium as gym\n",
    "\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', DEVICE)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Understanding PufferLib and Ocean Environments\n",
    "\n",
    "### What is [PufferLib](https://puffer.ai/ocean.html)?\n",
    "\n",
    "**PufferLib** is a high-performance reinforcement learning framework designed to bridge the gap between research and production RL. It was created to address common pain points in RL development:\n",
    "\n",
    "- **Speed:** PufferLib environments are highly optimized, often running 10-100x faster than traditional implementations\n",
    "- **Scalability:** Built-in support for massive parallelization across thousands of environments\n",
    "- **Simplicity:** Clean, minimal API that follows Gymnasium standards\n",
    "- **GPU-Native:** Environments can run directly on GPU, eliminating CPU-GPU transfer bottlenecks\n",
    "\n",
    "\n",
    "### PufferLib Ocean: Educational RL Environments\n",
    "\n",
    "**Ocean** is PufferLib's collection of lightweight, educational environments. The name reflects its purpose: a \"sea\" of diverse environments for learning and experimentation. Ocean environments are specifically designed for:\n",
    "\n",
    "1. **Fast Debugging:** Quickly verify your algorithm works before scaling up\n",
    "2. **Rapid Prototyping:** Test new ideas without waiting hours for results\n",
    "3. **Educational Clarity:** Simpler codebases that are easier to understand and modify\n",
    "4. **Vectorization by Default:** Learn modern RL practices from the start\n",
    "\n",
    "### Available Ocean Environments\n",
    "\n",
    "Ocean includes a variety of environments with different characteristics (more can be found in the [docs](https://github.com/PufferAI/PufferLib/tree/3.0/pufferlib/ocean)):\n",
    "\n",
    "- **Classic Control:** `cartpole`\n",
    "  - Simple physics simulations\n",
    "  - Low-dimensional observations\n",
    "  - Great for debugging and initial testing\n",
    "\n",
    "- **Grid Worlds:** `squared`, `minigrid_variants`\n",
    "  - Discrete state/action spaces\n",
    "  - Fast iteration times\n",
    "  - Perfect for verifying algorithm correctness\n",
    "\n",
    "- **Atari-Style:** `breakout`, `pong`, `pacman`, `enduro`\n",
    "  - More complex visual observations\n",
    "  - Longer training times\n",
    "  - Closer to real-world RL challenges\n",
    "\n",
    "- **Puzzle Games:** `tetris`\n",
    "  - Strategic planning required\n",
    "  - Sparse rewards\n",
    "  - Advanced challenge tasks\n",
    "\n",
    "### Why PufferLib for This Assignment?\n",
    "\n",
    "We chose PufferLib Ocean for several pedagogical reasons:\n",
    "\n",
    "1. **Immediate Feedback:** Fast environments mean you can iterate quickly on your code\n",
    "2. **Realistic Scale:** The environment enables easy vectorized environments like in real RL research\n",
    "3. **Low Hardware Requirements:** Efficient implementation means you don't need expensive GPUs because of vectorised observations\n",
    "4. **Clear Progression:** From simple (Squared) to complex (Breakout) in the same framework\n",
    "5. **Industry Relevance:** PufferLib is used in actual RL research and applications\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 1.2 Environment Factories"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class TimeLimitVec:\n",
    "    \"\"\"\n",
    "    Generic vector env time-limit wrapper for PufferLib/Gymnasium-like envs.docker run -it --rm -p 8888:8888 -v \"$(pwd)\":/app ciemcornelissen/puffer-notebook:latest\n",
    "    - Marks truncations True when per-env step count reaches max_episode_steps.\n",
    "    - Preserves existing terminals/truncations from the underlying env.\n",
    "    - Resets per-env counters on reset or when an env ends.\n",
    "    \"\"\"\n",
    "    def __init__(self, env: Any, max_episode_steps: int, info_key: str = \"time_limit\"):\n",
    "        assert max_episode_steps and max_episode_steps > 0\n",
    "        self.env = env\n",
    "        self.max_episode_steps = int(max_episode_steps)\n",
    "        self.info_key = info_key\n",
    "\n",
    "        # Mirror common attributes for compatibility\n",
    "        self.single_observation_space = getattr(env, \"single_observation_space\", None)\n",
    "        self.single_action_space = getattr(env, \"single_action_space\", None)\n",
    "        self.observation_space = getattr(env, \"observation_space\", None)\n",
    "        self.action_space = getattr(env, \"action_space\", None)\n",
    "        self.num_agents = getattr(env, \"num_agents\", 1)\n",
    "\n",
    "        self._steps = np.zeros(self.num_agents, dtype=np.int64)\n",
    "\n",
    "    def reset(self, seed: Optional[int] = 0):\n",
    "        obs, infos = self.env.reset(seed)\n",
    "        self._steps[...] = 0\n",
    "        return obs, infos\n",
    "\n",
    "    # def reset(self, seed: Optional[int] = None): \n",
    "    #     if seed is None:\n",
    "    #         seed = random.randint(1, 2**32 - 1)\n",
    "    #     obs, infos = self.env.reset(seed)\n",
    "    #     self._steps[...] = 0\n",
    "    #     return obs, infos\n",
    "\n",
    "\n",
    "    def step(self, actions):\n",
    "        obs, rewards, terminals, truncations, infos = self.env.step(actions)\n",
    "\n",
    "        t = np.asarray(terminals, dtype=bool)\n",
    "        tr = np.asarray(truncations, dtype=bool)\n",
    "        if t.ndim == 0:\n",
    "            t = t.reshape(1)\n",
    "        if tr.ndim == 0:\n",
    "            tr = tr.reshape(1)\n",
    "\n",
    "        active = ~(t | tr)\n",
    "        # Increment only for envs still active before this step's end flags\n",
    "        self._steps[active] += 1\n",
    "\n",
    "        # Apply time limit where not already ended this step\n",
    "        timeouts = (self._steps >= self.max_episode_steps) & active\n",
    "        if np.any(timeouts):\n",
    "            tr = np.logical_or(tr, timeouts)\n",
    "            self._steps[timeouts] = 0\n",
    "            if infos is None:\n",
    "                infos = []\n",
    "            if not isinstance(infos, list):\n",
    "                infos = [infos]\n",
    "            infos.append({self.info_key: {\"timeouts\": timeouts.copy()}})\n",
    "\n",
    "        # Also reset counters for any envs that ended naturally\n",
    "        ended = t | tr\n",
    "        if np.any(ended):\n",
    "            self._steps[ended] = 0\n",
    "\n",
    "        return obs, rewards, t, tr, infos\n",
    "\n",
    "    def render(self, *args, **kwargs):\n",
    "        return getattr(self.env, \"render\")(*args, **kwargs)\n",
    "\n",
    "    def close(self):\n",
    "        return getattr(self.env, \"close\")()\n",
    "def make_env(name: str, seed: int = 0):\n",
    "    env_map = {\n",
    "        'squared': squared,\n",
    "        'pong': pong,\n",
    "        'pacman': pacman,\n",
    "        'enduro': enduro,\n",
    "        'tetris': tetris,\n",
    "        'breakout': breakout,\n",
    "        'cartpole': cartpole\n",
    "    }\n",
    "    if name not in env_map:\n",
    "        raise ValueError(f'Unknown environment {name}')\n",
    "\n",
    "    def thunk():\n",
    "        # Get the module from the map\n",
    "        env_module = env_map[name]\n",
    "\n",
    "        # For example, in the 'squared' module, there is a 'Squared' class.\n",
    "        env_class_name = name.capitalize()\n",
    "        env_class = getattr(env_module, env_class_name)\n",
    "\n",
    "        raw_env = env_class(num_envs=1, render_mode=None, seed=seed)  # Instantiate the class\n",
    "        env = TimeLimitVec(raw_env, max_episode_steps=10_000)\n",
    "        env.reset(seed=seed)\n",
    "        return env\n",
    "    return thunk\n",
    "\n",
    "debug_env_name = 'squared'\n",
    "intermediate_env_name = 'cartpole'  # Change to 'pacman', 'enduro', or 'tetris' if desired\n",
    "challenge_env_name = 'breakout'\n",
    "\n",
    "test_env = make_env(debug_env_name)()\n",
    "obs, info = test_env.reset()\n",
    "print('Debug env observation shape:', np.array(obs).shape)\n",
    "print('Action space:', test_env.action_space)\n",
    "print('Observation space:', test_env.observation_space)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 1.3 Configuration\n",
    "Adjust hyperparameters and add hyperparameters as needed."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "@dataclass\n",
    "class Config:\n",
    "    seed: int = 7\n",
    "    gamma: float = 0.95                # prioritize short-term rewards\n",
    "    lr: float = 1e-3                   # can reduce to 1e-4 if unstable\n",
    "    batch_size: int = 128\n",
    "    buffer_capacity: int = 1000      # larger buffer for diverse data\n",
    "    min_buffer_size: int = 200       # start training with more data\n",
    "    max_steps_per_env: int = 60_000\n",
    "    max_episode_len: int = 500\n",
    "    epsilon_start: float = 1.0\n",
    "    epsilon_end: float = 0.01\n",
    "    epsilon_decay_steps: int = 5000\n",
    "    target_update_interval: int = 100\n",
    "    train_freq: int = 4\n",
    "    gradient_clip: float = 10.0\n",
    "    reward_clip: Optional[float] = None\n",
    "    use_double_dqn: bool = False\n",
    "    use_dueling: bool = False           # enabled for better value estimation\n",
    "    vector_envs: int = 1\n",
    "    log_interval_episodes: int = 10\n",
    "    eval_episodes: int = 3\n",
    "    curriculum: List[str] = None\n",
    "    hidden_size: int = 256             # larger network for complex patterns\n",
    "\n",
    "    n_steps: int = 3\n",
    "    use_noisy_layer: bool = False\n",
    "\n",
    "cfg = Config()\n",
    "cfg.curriculum = [intermediate_env_name]\n",
    "random.seed(cfg.seed)\n",
    "np.random.seed(cfg.seed)\n",
    "torch.manual_seed(cfg.seed)\n",
    "cfg"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 1.4 Replay Buffer\n"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity: int, obs_shape: tuple, device):\n",
    "        self.capacity = capacity\n",
    "        self.device = device\n",
    "        self.obs_shape = obs_shape\n",
    "        self.position = 0\n",
    "        self.size = 0\n",
    "        \n",
    "        # Pre-allocate numpy arrays for storage\n",
    "        self.obs = np.zeros((capacity, *obs_shape), dtype=np.float32)\n",
    "        self.actions = np.zeros((capacity, 1), dtype=np.int64)  # Shape: (capacity, 1)\n",
    "        self.rewards = np.zeros((capacity, 1), dtype=np.float32)\n",
    "        self.next_obs = np.zeros((capacity, *obs_shape), dtype=np.float32)\n",
    "        self.dones = np.zeros((capacity, 1), dtype=np.float32)\n",
    "    \n",
    "    def push(self, obs, action, reward, next_obs, done):\n",
    "        \"\"\"Store a transition\"\"\"\n",
    "        self.obs[self.position] = obs\n",
    "        self.actions[self.position, 0] = action  # Store as scalar in (1,) shaped array\n",
    "        self.rewards[self.position, 0] = reward\n",
    "        self.next_obs[self.position] = next_obs\n",
    "        self.dones[self.position, 0] = float(done)\n",
    "        \n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "        self.size = min(self.size + 1, self.capacity)\n",
    "    \n",
    "    def sample(self, batch_size: int):\n",
    "        \"\"\"Sample a batch of transitions and convert to PyTorch tensors\"\"\"\n",
    "        indices = np.random.choice(self.size, batch_size, replace=False)\n",
    "        \n",
    "        # Convert to tensors - actions will have shape (batch_size, 1)\n",
    "        obs = torch.FloatTensor(self.obs[indices]).to(self.device)\n",
    "        actions = torch.LongTensor(self.actions[indices]).to(self.device)  # (batch_size, 1)\n",
    "        rewards = torch.FloatTensor(self.rewards[indices]).to(self.device)\n",
    "        next_obs = torch.FloatTensor(self.next_obs[indices]).to(self.device)\n",
    "        dones = torch.FloatTensor(self.dones[indices]).to(self.device)\n",
    "        \n",
    "        return obs, actions, rewards, next_obs, dones\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.size\n",
    "\n",
    "# Test ReplayBuffer\n",
    "print(\"Testing ReplayBuffer...\")\n",
    "test_buffer = ReplayBuffer(100, (4,), DEVICE)\n",
    "test_buffer.push(np.array([1,2,3,4]), 0, 1.0, np.array([2,3,4,5]), False)\n",
    "assert len(test_buffer) == 1, \"Buffer should have 1 element\"\n",
    "obs, actions, rewards, next_obs, dones = test_buffer.sample(1)\n",
    "assert obs.shape == (1, 4), f\"Expected shape (1, 4), got {obs.shape}\"\n",
    "print(\"✓ ReplayBuffer tests passed!\")"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 1.5 Epsilon Schedule\n",
    "Linear decay from start to end over `epsilon_decay_steps`."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def epsilon_by_step(step: int, cfg: Config) -> float:\n",
    "    per_step = (cfg.epsilon_start-cfg.epsilon_end)/cfg.epsilon_decay_steps\n",
    "    if step < cfg.epsilon_decay_steps:\n",
    "        eps = cfg.epsilon_start - (step*per_step)\n",
    "    else:\n",
    "        eps = cfg.epsilon_end\n",
    "    return eps\n",
    "\n",
    "print('Epsilon(0)=', epsilon_by_step(0, cfg), 'Epsilon(mid)=', epsilon_by_step(cfg.epsilon_decay_steps//2, cfg), 'Epsilon(end)=', epsilon_by_step(cfg.epsilon_decay_steps*2, cfg))"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 1.6 Q-Network Architectures"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "\n",
    "class NoisyLinear(nn.Module):\n",
    "    \"\"\"\n",
    "    Factorized NoisyNet linear layer (Fortunato et al., 2018).\n",
    "    Samples noise during training; uses deterministic weights in eval mode.\n",
    "\n",
    "    Args:\n",
    "        in_features (int): input feature size\n",
    "        out_features (int): output feature size\n",
    "        sigma0 (float): initial scale for noise parameters\n",
    "        bias (bool): include bias term\n",
    "        auto_reset (bool): resample noise every forward() in training mode\n",
    "    \"\"\"\n",
    "    def __init__(self, in_features: int, out_features: int, sigma0: float = 0.5, bias: bool = True, auto_reset: bool = True):\n",
    "        super().__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.sigma0 = sigma0\n",
    "        self.auto_reset = auto_reset\n",
    "\n",
    "        # Learnable parameters (mu and sigma)\n",
    "        self.weight_mu = nn.Parameter(torch.empty(out_features, in_features))\n",
    "        self.weight_sigma = nn.Parameter(torch.empty(out_features, in_features))\n",
    "        if bias:\n",
    "            self.bias_mu = nn.Parameter(torch.empty(out_features))\n",
    "            self.bias_sigma = nn.Parameter(torch.empty(out_features))\n",
    "        else:\n",
    "            self.register_parameter(\"bias_mu\", None)\n",
    "            self.register_parameter(\"bias_sigma\", None)\n",
    "\n",
    "        # Buffers for factorized noise\n",
    "        self.register_buffer(\"eps_in\", torch.empty(1, in_features))\n",
    "        self.register_buffer(\"eps_out\", torch.empty(out_features, 1))\n",
    "\n",
    "        self.reset_parameters()\n",
    "        self.reset_noise()\n",
    "\n",
    "    def reset_parameters(self) -> None:\n",
    "        mu_range = 1.0 / math.sqrt(self.in_features)\n",
    "        self.weight_mu.data.uniform_(-mu_range, mu_range)\n",
    "        self.weight_sigma.data.fill_(self.sigma0 / math.sqrt(self.in_features))\n",
    "        if self.bias_mu is not None:\n",
    "            self.bias_mu.data.uniform_(-mu_range, mu_range)\n",
    "            self.bias_sigma.data.fill_(self.sigma0 / math.sqrt(self.out_features))\n",
    "\n",
    "    @staticmethod\n",
    "    def _scale_noise(size, device):\n",
    "        # f(epsilon) = sign(epsilon) * sqrt(|epsilon|)\n",
    "        x = torch.randn(size, device=device)\n",
    "        return x.sign().mul_(x.abs().sqrt_())\n",
    "\n",
    "    def reset_noise(self) -> None:\n",
    "        device = self.weight_mu.device\n",
    "        self.eps_in.copy_(self._scale_noise((1, self.in_features), device))\n",
    "        self.eps_out.copy_(self._scale_noise((self.out_features, 1), device))\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        if self.training:\n",
    "            if self.auto_reset:\n",
    "                self.reset_noise()\n",
    "            weight = self.weight_mu + self.weight_sigma * (self.eps_out @ self.eps_in)\n",
    "            bias = None\n",
    "            if self.bias_mu is not None:\n",
    "                bias = self.bias_mu + self.bias_sigma * self.eps_out.squeeze(1)\n",
    "        else:\n",
    "            weight = self.weight_mu\n",
    "            bias = self.bias_mu if self.bias_mu is not None else None\n",
    "\n",
    "        return F.linear(x, weight, bias)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "class MLPQ(nn.Module):\n",
    "    def __init__(self, obs_dim: int, action_dim: int, dueling=False):\n",
    "        super().__init__()\n",
    "        self.hidden_size = cfg.hidden_size  # Typically 128, 256, or 512\n",
    "        self.dueling = dueling\n",
    "        self.action_dim = action_dim\n",
    "\n",
    "        final_layer = NoisyLinear(self.hidden_size, action_dim) if cfg.use_noisy_layer else nn.Linear(self.hidden_size, action_dim)\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(obs_dim, self.hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(self.hidden_size, self.hidden_size),\n",
    "            nn.ReLU(),\n",
    "            final_layer\n",
    "        )\n",
    "\n",
    "\n",
    "        if not dueling:\n",
    "            # Standard single stream for Q-values\n",
    "            self.q_head = nn.Linear(self.hidden_size, action_dim)\n",
    "        else:\n",
    "            # Separate value and advantage streams\n",
    "            self.value_stream = nn.Sequential(\n",
    "                nn.Linear(self.hidden_size, self.hidden_size),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(self.hidden_size, 1)\n",
    "            )\n",
    "            self.advantage_stream = nn.Sequential(\n",
    "                nn.Linear(self.hidden_size, self.hidden_size),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(self.hidden_size, action_dim)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = self.feature(x)\n",
    "        if not self.dueling:\n",
    "            return self.q_head(features)\n",
    "        else:\n",
    "            value = self.value_stream(features)\n",
    "            advantages = self.advantage_stream(features)\n",
    "            q_values = value + (advantages - advantages.mean(dim=1, keepdim=True))\n",
    "            return q_values\n",
    "\n",
    "def build_q_network(env, cfg: Config):\n",
    "    obs_space = env.single_observation_space\n",
    "    print('Obs space:', obs_space.shape[0])\n",
    "    act_space = env.single_action_space\n",
    "    action_dim = act_space.n\n",
    "    assert isinstance(act_space, gym.spaces.Discrete), 'DQN requires discrete actions'\n",
    "    if len(obs_space.shape) == 1:\n",
    "        return MLPQ(obs_space.shape[0], action_dim, dueling=cfg.use_dueling)\n",
    "\n",
    "\n",
    "# Quick test on debug env\n",
    "net_test_env = test_env\n",
    "test_net = build_q_network(net_test_env, cfg).to(DEVICE)\n",
    "print(test_net)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 1.7 Action Selection\n",
    "Standard epsilon-greedy policy. (Later you might incorporate noisy networks or exploration bonuses.)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "@torch.no_grad()\n",
    "def select_action(q_net, obs, epsilon: float, env):\n",
    "    if random.random() < epsilon:\n",
    "        return env.action_space.sample()\n",
    "    if isinstance(obs, torch.Tensor):\n",
    "        obs_t = obs.to(DEVICE, dtype=torch.float32, non_blocking=True)\n",
    "    else:\n",
    "        obs_t = torch.as_tensor(obs, dtype=torch.float32, device=DEVICE)\n",
    "    q_vals = q_net(obs_t)\n",
    "    return int(q_vals.argmax(dim=1).item())\n",
    "\n",
    "# Test call\n",
    "dummy_action = select_action(test_net, obs, 1.0, test_env)\n",
    "print('Random/greedy test action:', dummy_action)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 1.8 Training Step\n",
    "Compute standard DQN loss or Double DQN target if `cfg.use_double_dqn=True`.\n",
    "\n",
    "Target for vanilla DQN:\n",
    "$$ y = r + (1-d) \\gamma \\max_{a'} Q_{target}(s', a') $$\n",
    "\n",
    "Double DQN variant:\n",
    "$$ a^* = \\arg\\max_{a'} Q_{online}(s', a') \\quad; \\quad y = r + (1-d) \\gamma Q_{target}(s', a^*) $$"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "loss_fn = nn.SmoothL1Loss()\n",
    "\n",
    "def train_step(q_net, target_net, optimizer, replay: ReplayBuffer, cfg: Config):\n",
    "    \"\"\"\n",
    "    Performs one training step of DQN.\n",
    "    \n",
    "    Args:\n",
    "        q_net: Online Q-network (being trained)\n",
    "        target_net: Target Q-network (frozen, updated periodically)\n",
    "        optimizer: Optimizer for q_net\n",
    "        replay: Replay buffer containing transitions\n",
    "        cfg: Configuration object\n",
    "    \n",
    "    Returns:\n",
    "        Loss value or None if buffer not ready\n",
    "    \"\"\"\n",
    "    # Don't train until we have enough experiences\n",
    "    if len(replay) < cfg.min_buffer_size:\n",
    "        return None\n",
    "    \n",
    "    obs, actions, rewards, next_obs, dones = replay.sample(cfg.batch_size)\n",
    "\n",
    "    q_values = q_net(obs)  # Shape: (batch_size, action_dim)\n",
    "\n",
    "    q_sa = q_values.gather(1, actions)  # Shape: (batch_size, 1)\n",
    "    \n",
    "    # ============= Compute Target Q-values =============\n",
    "    with torch.no_grad():  # Don't compute gradients for target\n",
    "        if cfg.use_double_dqn:\n",
    "            \n",
    "            # Step 1: Use online network to find best action in next state\n",
    "            next_q_values_online = q_net(next_obs)  # Shape: (batch_size, action_dim)\n",
    "            best_actions = next_q_values_online.argmax(dim=1, keepdim=True)  # Shape: (batch_size, 1)\n",
    "            \n",
    "            # Step 2: Use target network to evaluate that action\n",
    "            next_q_values_target = target_net(next_obs)  # Shape: (batch_size, action_dim)\n",
    "            next_q_value = next_q_values_target.gather(1, best_actions)  # Shape: (batch_size, 1)\n",
    "            \n",
    "        else:\n",
    "            # Vanilla DQN: Use target network for both selection and evaluation\n",
    "            next_q_values = target_net(next_obs)  # Shape: (batch_size, action_dim)\n",
    "            next_q_value = next_q_values.max(dim=1, keepdim=True)[0]  # Shape: (batch_size, 1)\n",
    "            # Note: max returns (values, indices), we take [0] for values\n",
    "        \n",
    "        # Compute TD target: y = r + γ * max Q(s', a') * (1 - done)\n",
    "        # The (1 - dones) term zeros out the next_q_value if episode ended\n",
    "        target = rewards + (cfg.gamma ** cfg.n_steps) * next_q_value * (1 - dones)  # Shape: (batch_size, 1)\n",
    "\n",
    "    loss = loss_fn(q_sa, target)\n",
    "    \n",
    "    # Standard PyTorch training loop\n",
    "    optimizer.zero_grad()  # Clear previous gradients\n",
    "    loss.backward()  # Compute gradients\n",
    "    nn.utils.clip_grad_norm_(q_net.parameters(), cfg.gradient_clip)  # Prevent exploding gradients\n",
    "    optimizer.step()  # Update weights\n",
    "    \n",
    "    return float(loss.item())"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 1.9 Reward Processing\n",
    "Add optional reward clipping to stabilize training on environments with high or varied reward magnitudes.\n",
    "\n",
    "If `cfg.reward_clip` is set, clip reward to `[-cfg.reward_clip, cfg.reward_clip]`."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def process_reward(r: float, cfg: Config):\n",
    "    if cfg.reward_clip is not None:\n",
    "        return max(-cfg.reward_clip, min(cfg.reward_clip, r))\n",
    "    return r\n",
    "\n",
    "# Quick test\n",
    "cfg.reward_clip = None  # Set to 1.0 to try clipping later\n",
    "print('Reward test (no clip):', process_reward(5.0, cfg))\n",
    "cfg.reward_clip = 1.0\n",
    "print('Reward test (clip=1.0):', process_reward(5.0, cfg))\n",
    "cfg.reward_clip = None  # Reset for training; modify if desired"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 1.10 Evaluation Utilities\n",
    "Evaluation runs with greedy policy (`epsilon=0`).\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def evaluate(env_fn, q_net, cfg: Config, episodes: int):\n",
    "    env = env_fn\n",
    "    returns = []\n",
    "    for _ in range(episodes):\n",
    "        obs, _ = env.reset()\n",
    "        done = False\n",
    "        ep_ret = 0\n",
    "        steps = 0\n",
    "        while not done and steps < cfg.max_episode_len:\n",
    "            a = select_action(q_net, obs, 0.0, env)\n",
    "            obs2, r, terminated, truncated, _ = env.step(a)\n",
    "            done = terminated or truncated\n",
    "            ep_ret += r\n",
    "            obs = obs2\n",
    "            steps += 1\n",
    "        returns.append(ep_ret)\n",
    "    return float(np.mean(returns))\n",
    "\n",
    "def hard_update(target, source):\n",
    "    target.load_state_dict(source.state_dict())\n",
    "\n",
    "print('Eval utility ready.')"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 1.11 Training Loop\n",
    "We iterate over the environments in.\n",
    "Logging per stage:\n",
    "- `episode_rewards`\n",
    "- `losses`\n",
    "- `eps_history`\n",
    "- `eval` (periodic greedy evaluation)\n",
    "\n",
    "Increase `max_steps_per_env` for stronger performance. For quick debugging, you may temporarily reduce it.\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import dataclasses\n",
    "\n",
    "def save_checkpoint(path, q_net, optimizer, cfg, stage_name: str):\n",
    "    torch.save({\n",
    "        'model': q_net.state_dict(),\n",
    "        'optimizer': optimizer.state_dict(),\n",
    "        'config': dataclasses.asdict(cfg),\n",
    "        'stage': stage_name,\n",
    "    }, path)\n",
    "    print('Saved checkpoint:', path)\n",
    "\n",
    "\n",
    "# === Training ===\n",
    "all_stage_logs = {}\n",
    "\n",
    "q_net = None\n",
    "target_net = None\n",
    "optimizer = None\n",
    "stage_index = 0\n",
    "\n",
    "# Make directories for outputs\n",
    "os.makedirs(\"logs\", exist_ok=True)\n",
    "os.makedirs(\"checkpoints\", exist_ok=True)\n",
    "\n",
    "for stage_env_name in cfg.curriculum:\n",
    "    print(f\"\\n=== Stage {stage_index+1}: {stage_env_name} ===\")\n",
    "    env = make_env(stage_env_name, seed=cfg.seed + stage_index)()\n",
    "    eval_env = make_env(stage_env_name, seed=cfg.seed + 100 + stage_index)()\n",
    "\n",
    "    q_net = build_q_network(env, cfg).to(DEVICE)\n",
    "    # if cfg.use_noisy_layer:\n",
    "    #     q_net.reset_noise()\n",
    "    #     target_net.reset_noise()\n",
    "    target_net = build_q_network(env, cfg).to(DEVICE)\n",
    "    hard_update(target_net, q_net)\n",
    "    optimizer = optim.Adam(q_net.parameters(), lr=cfg.lr)\n",
    "\n",
    "    replay = ReplayBuffer(cfg.buffer_capacity, env.single_observation_space.shape, DEVICE)\n",
    "\n",
    "    logs = {\n",
    "        'episode_rewards': [],\n",
    "        'losses': [],\n",
    "        'eps_history': [],\n",
    "        'eval': [],\n",
    "    }\n",
    "\n",
    "    obs, _ = env.reset()\n",
    "    ep_reward = 0\n",
    "    ep_steps = 0\n",
    "    episode_counter = 0\n",
    "    recent_ep_returns = []\n",
    "    total_steps = 0\n",
    "\n",
    "    trajectory_buffer = deque(maxlen=cfg.n_steps)\n",
    "\n",
    "    print('Starting training...')\n",
    "    while total_steps < cfg.max_steps_per_env:\n",
    "        if cfg.use_noisy_layer:\n",
    "            epsilon = 0.0  # No epsilon-greedy when using noisy networks\n",
    "        else:\n",
    "            epsilon = epsilon_by_step(total_steps, cfg)\n",
    "        action = select_action(q_net, obs, epsilon, env)\n",
    "        next_obs, reward, terminated, truncated, info = env.step(action)\n",
    "        reward = process_reward(reward, cfg)\n",
    "        reward = float(np.asarray(reward).reshape(-1)[0])\n",
    "        done = bool(np.asarray(terminated).reshape(-1)[0] or np.asarray(truncated).reshape(-1)[0])\n",
    "\n",
    "        trajectory_buffer.append((obs.copy(), int(action), reward, next_obs.copy(), done))\n",
    "\n",
    "        if len(trajectory_buffer) == cfg.n_steps or done:\n",
    "            n_step_reward = 0\n",
    "\n",
    "            # N-step return calculation\n",
    "            for i, (_, _, reward, _, _) in enumerate(trajectory_buffer):\n",
    "                n_step_reward += (cfg.gamma ** i) * reward\n",
    "            first_obs, first_action, _, _, first_done = trajectory_buffer[0]\n",
    "            _, _, _, last_next_obs, _ = trajectory_buffer[-1]  # Get the last next_obs (n steps ahead)\n",
    "\n",
    "            # Push n-step transition to replay buffer\n",
    "            replay.push(first_obs, first_action, n_step_reward, last_next_obs, first_done)\n",
    "\n",
    "            # Reset buffer if done before filling all n steps\n",
    "            if done:\n",
    "                trajectory_buffer.clear()\n",
    "\n",
    "        obs = next_obs.copy()\n",
    "        ep_reward += reward\n",
    "        ep_steps += 1\n",
    "        total_steps += 1\n",
    "\n",
    "        # Training\n",
    "        if total_steps % cfg.train_freq == 0:\n",
    "            loss = train_step(q_net, target_net, optimizer, replay, cfg)\n",
    "            if loss is not None:\n",
    "                logs['losses'].append(loss)\n",
    "\n",
    "        # Target update\n",
    "        if total_steps % cfg.target_update_interval == 0:\n",
    "            hard_update(target_net, q_net)\n",
    "\n",
    "        logs['eps_history'].append(epsilon)\n",
    "\n",
    "        if done or ep_steps >= cfg.max_episode_len:\n",
    "            logs['episode_rewards'].append(ep_reward)\n",
    "            recent_ep_returns.append(ep_reward)\n",
    "            episode_counter += 1\n",
    "\n",
    "            # --- DEBUG PRINT ---\n",
    "            mean_loss = np.mean(logs['losses'][-100:]) if logs['losses'] else 0.0\n",
    "            mean_reward = np.mean(recent_ep_returns[-cfg.log_interval_episodes:]) if recent_ep_returns else 0.0\n",
    "            with torch.no_grad():\n",
    "                q_sample = q_net(torch.FloatTensor(obs).unsqueeze(0).to(DEVICE))\n",
    "                mean_q = q_sample.mean().item()\n",
    "            grads = [p.grad.norm().item() for p in q_net.parameters() if p.grad is not None]\n",
    "            grad_norm = np.mean(grads) if grads else 0.0\n",
    "            print(f\"[DEBUG] Step {total_steps:6d} | Ep {episode_counter:4d} | \"\n",
    "                  f\"Eps {epsilon:.3f} | MeanReward(10ep): {mean_reward:7.3f} | \"\n",
    "                  f\"MeanLoss(100it): {mean_loss:8.5f} | \"\n",
    "                  f\"MeanQ: {mean_q:7.3f} | GradNorm: {grad_norm:7.4f}\")\n",
    "\n",
    "            if episode_counter % cfg.log_interval_episodes == 0:\n",
    "                mean_recent = np.mean(recent_ep_returns[-cfg.log_interval_episodes:])\n",
    "                print(f\"[{stage_env_name}] Ep {episode_counter} Steps {total_steps} \"\n",
    "                      f\"RecentMean {mean_recent:.2f} Eps {epsilon:.3f}\")\n",
    "\n",
    "            obs, _ = env.reset()\n",
    "            ep_reward = 0\n",
    "            ep_steps = 0\n",
    "\n",
    "            # Periodic evaluation (every 2*log_interval episodes if we have progress)\n",
    "            if episode_counter % (cfg.log_interval_episodes * 2) == 0:\n",
    "                eval_ret = evaluate(eval_env, q_net, cfg, cfg.eval_episodes)\n",
    "                logs['eval'].append((total_steps, eval_ret))\n",
    "                print(f\"Eval @ Ep {episode_counter} and {total_steps} steps => mean return {eval_ret:.2f}\")\n",
    "\n",
    "    # --- Save logs and model after each stage ---\n",
    "    all_stage_logs[stage_env_name] = logs\n",
    "\n",
    "    # Save training logs\n",
    "    df = pd.DataFrame({\n",
    "        'episode_rewards': logs['episode_rewards'],\n",
    "        'losses': logs['losses'][:len(logs['episode_rewards'])],\n",
    "        'eps_history': logs['eps_history'][:len(logs['episode_rewards'])],\n",
    "    })\n",
    "    train_csv = f\"logs/{stage_env_name}_training_log.csv\"\n",
    "    df.to_csv(train_csv, index=False)\n",
    "    print(f\"Saved training data → {train_csv}\")\n",
    "\n",
    "    # Save evaluation results\n",
    "    eval_df = pd.DataFrame(logs['eval'], columns=['total_steps', 'eval_return'])\n",
    "    eval_csv = f\"logs/{stage_env_name}_eval_log.csv\"\n",
    "    eval_df.to_csv(eval_csv, index=False)\n",
    "    print(f\"Saved eval data → {eval_csv}\")\n",
    "\n",
    "    # Save model checkpoint\n",
    "    checkpoint_path = f\"checkpoints/{stage_env_name}_final.pt\"\n",
    "    save_checkpoint(checkpoint_path, q_net, optimizer, cfg, stage_env_name)\n",
    "\n",
    "    stage_index += 1"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "##  Visualization Per Stage\n",
    "helper code for plots, feel free to modify or use different tools, e.g. WandB.\n",
    "Plots: Episode rewards, Epsilon schedule, Loss curve, and Evaluation returns."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def moving_average(x, w):\n",
    "    # Ensure x is a flattened 1D numpy array to handle lists like [[1],[2],[3]]\n",
    "    x = np.array(x).flatten()\n",
    "    if len(x) < w:\n",
    "        return x\n",
    "    return np.convolve(x, np.ones(w)/w, mode='valid')\n",
    "\n",
    "for stage, logs in all_stage_logs.items():\n",
    "    rewards = logs['episode_rewards']\n",
    "    losses = logs['losses']\n",
    "    eps_hist = logs['eps_history']\n",
    "    evals = logs['eval']\n",
    "    print(f\"\\n=== {stage} ===\")\n",
    "    plt.figure(figsize=(14,4))\n",
    "    # Rewards\n",
    "    plt.subplot(1,3,1)\n",
    "    plt.title(f'{stage} Rewards')\n",
    "    plt.plot(rewards, alpha=0.5, label='Return')\n",
    "    ma = moving_average(rewards, 100)\n",
    "    if len(ma) != len(rewards):\n",
    "        plt.plot(range(len(rewards)-len(ma), len(rewards)), ma, label='MA(20)')\n",
    "    plt.legend()\n",
    "    plt.xlabel('Episode')\n",
    "\n",
    "    # Epsilon\n",
    "    plt.subplot(1,3,2)\n",
    "    plt.title('Epsilon')\n",
    "    plt.plot(eps_hist)\n",
    "    plt.xlabel('Step')\n",
    "\n",
    "    # Losses\n",
    "    plt.subplot(1,3,3)\n",
    "    plt.title('Loss (smoothed)')\n",
    "    if len(losses) > 0:\n",
    "        lma = moving_average(losses, 200)\n",
    "        plt.plot(losses, alpha=0.3)\n",
    "        plt.plot(range(len(losses)-len(lma), len(losses)), lma, label='MA(200)')\n",
    "        plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    if len(evals) > 0:\n",
    "        xs = [x for x,_ in evals]\n",
    "        ys = [y for _,y in evals]\n",
    "        plt.figure()\n",
    "        plt.title(f'{stage} Evaluation Returns')\n",
    "        plt.plot(xs, ys, marker='o')\n",
    "        plt.xlabel('Env Steps')\n",
    "        plt.ylabel('Mean Return (greedy)')\n",
    "        plt.show()"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 2.1 Group Project: Extensions and Analysis\n",
    "\n",
    "This assignment is a group project. After familiarizing yourselves with the baseline DQN code, your group will divide the following implementation tasks. Once each part is complete, you will merge your code and collaborate on a final report that analyzes and compares the performance of each extension.\n",
    "\n",
    "---\n",
    "### **Part 1: Individual Implementation Tasks**\n",
    "\n",
    "Each group member must implement one of the following sets of extensions. You will need to modify the core components (`MLPQ`, `ReplayBuffer`, `train_step`) to support these new algorithms, using the `cfg` object to toggle them on and off.\n",
    "\n",
    "---\n",
    "#### **Extension A: Double DQN & Dueling DQN**\n",
    "\n",
    "**Assigned to:** *[Student A Name Here]*\n",
    "\n",
    "This set of tasks focuses on improving the Q-value estimation and network architecture.\n",
    "\n",
    "**TODO (A1): Implement Double DQN**\n",
    "In the `train_step` function, implement the logic inside the `if cfg.use_double_dqn:` block. This involves decoupling action selection from evaluation to mitigate overestimation bias by using the **online network** to select the best next action and the **target network** to evaluate its value.\n",
    "\n",
    "**TODO (A2): Implement Dueling DQN**\n",
    "In the `MLPQ` class, modify the `__init__` and `forward` methods to support a dueling architecture. You will need to create separate `value_head` and `advantage_head` layers and combine their outputs using the formula: `Q(s,a) = V(s) + (A(s,a) - mean(A(s,:)))`.\n",
    "\n",
    "---\n",
    "#### **Extension B: Prioritized Experience Replay (PER)**\n",
    "\n",
    "**Assigned to:** *[Student B Name Here]*\n",
    "\n",
    "This task replaces the uniform `ReplayBuffer` with a more intelligent sampling strategy that prioritizes \"surprising\" transitions.\n",
    "\n",
    "**TODO (B1): Implement `PrioritizedReplayBuffer`**\n",
    "Create a new `PrioritizedReplayBuffer` class. This class must manage transition priorities (based on TD-error), sample transitions according to these priorities, and compute importance sampling (IS) weights to correct for the biased sampling. Implementing a **SumTree** is the standard approach for this.\n",
    "\n",
    "**TODO (B2): Integrate PER into `train_step`**\n",
    "Modify the training loop to use your new buffer. In `train_step`, you must use the IS weights to scale the loss for each transition and then call a method on your buffer to update the priorities of the sampled transitions with their new TD-errors.\n",
    "\n",
    "---\n",
    "#### **Extension C: N-Step Returns & Noisy Networks**\n",
    "\n",
    "**Assigned to:** *[Student C Name Here]*\n",
    "\n",
    "This set of tasks focuses on improving the TD target and the agent's exploration strategy.\n",
    "\n",
    "**TODO (C1): Implement N-Step Returns**\n",
    "Modify the training loop and `train_step` to use N-step returns. This involves temporarily storing the last `N` transitions to calculate the discounted N-step reward (`R_n`) and updating the target formula in `train_step` to use `γ^N` for bootstrapping.\n",
    "\n",
    "**TODO (C2): Implement Noisy Networks**\n",
    "Replace epsilon-greedy exploration with learned exploration. Create a custom `NoisyLinear` PyTorch layer that adds parametric noise to its weights. Replace the final linear layers of your `MLPQ` with this new layer and disable epsilon-greedy exploration in the main loop when this feature is active.\n",
    "\n",
    "---\n",
    "### **Optional Extension**\n",
    "\n",
    "#### **Vectorized Environments**\n",
    "For a deeper challenge, modify the entire pipeline to support vectorized environments. This involves changing `cfg.vector_envs` to > 1 and refactoring the training loop to handle batched operations for action selection, environment stepping, and episode tracking. This is a highly effective method for speeding up training but requires careful management of parallel data streams.\n",
    "\n",
    "---\n",
    "### **Part 2: Final Report (Group Task)**\n",
    "\n",
    "Your group's final submission should be a report that includes the following:\n",
    "\n",
    "**1. Experimental Analysis:**\n",
    "*   Run experiments on the Comparing environment comparing your baseline DQN against each of the implemented extensions (A, B, and C) and all extension combined on the challenge environment.\n",
    "*   Generate and include plots for each experiment (Episode Rewards, Loss, etc.).\n",
    "*  For the replay buffer size and N-Step Returns, test out 2-4 different values and reflect on the difference in performance. You may change other hyperparameters as needed to get good performance, also put the highlights in the report.\n",
    "*   Analyze the plots: Reflect how you can see the improvements for each extension. Which extension provided the biggest performance boost or the most stable training? Justify your claims with evidence.\n",
    "\n",
    "**2. Conceptual Questions:**\n",
    "\n",
    "*   **Q1:** Explain \"maximization bias\" in Q-learning. How does your **Double DQN** implementation address it?\n",
    "*   **Q2:** What is the theoretical motivation for the **Dueling DQN** architecture? Why is the special averaging mechanism important?\n",
    "*   **Q3:** Why is uniform sampling from the replay buffer inefficient? How do the **importance sampling weights** in **PER** correct for the biased sampling you introduced?\n",
    "*   **Q4:** Explain the difference between epsilon-greedy exploration and the exploration provided by **Noisy Networks**. What is an advantage of the latter?\n",
    "*   **Q5:** How does changing 'N' in **N-Step Returns** affect the bias-variance trade-off in your Q-learning updates?\n",
    "\n",
    "**3. Reflection:**\n",
    "*   Briefly discuss the biggest challenge your group faced during implementation and how you solved it.\n",
    "*   Were some of the results unexpected, if so in what way?\n",
    "*   If you implemented the vectorized environments, describe the performance improvement you observed in terms of wall-clock time.\n",
    "\n",
    "**4. Code Submission:**\n",
    "*   Submit your complete code with all extensions implemented. Ensure it is well-commented and organized.\n",
    "---"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Visualize agent\n",
    "helper code to visualize your trained agent in the notebook. You may modify this code as needed or visualize it in a different way."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from pyvirtualdisplay import Display\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display, clear_output\n",
    "import time\n",
    "import os\n",
    "import pyscreenshot as ImageGrab\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# --- Visualization Setup (from our debugging) ---\n",
    "seed = 0\n",
    "steps_to_show = 1000 # Your original value\n",
    "game_width = 576\n",
    "game_height = 330\n",
    "bbox = (0, 0, game_width, game_height)\n",
    "\n",
    "# Initialize variables to None before the try block for safe cleanup\n",
    "vdisplay = None\n",
    "env = None\n",
    "\n",
    "original_dir = os.getcwd()\n",
    "pufferlib_dir = '/opt/pufferlib'\n",
    "# -------------------------------------------------\n",
    "def first_obs(x):\n",
    "    x = np.asarray(x)\n",
    "    return x[0] if x.ndim > 1 else x\n",
    "\n",
    "# This is your agent's decision-making function, it's perfect as is.\n",
    "def policy_net(st):\n",
    "    return q_net(st)\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def greedy_action(state_np):\n",
    "    st = torch.as_tensor(state_np, dtype=torch.float32, device=DEVICE).unsqueeze(0)\n",
    "    q = policy_net(st)\n",
    "    print(q)\n",
    "    return int(q.argmax(dim=1).item())\n",
    "\n",
    "# The robust 'try...finally' block for safe execution and cleanup\n",
    "try:\n",
    "    vdisplay = Display(visible=False, size=(game_width, game_height))\n",
    "    vdisplay.start()\n",
    "\n",
    "    # os.chdir(pufferlib_dir)\n",
    "    # env = breakout.Breakout(num_envs=1, render_mode='human', seed=seed)\n",
    "    # env = cartpole.Cartpole(num_envs=1, render_mode='human', seed=seed)\n",
    "    env = cartpole.Cartpole(num_envs=1, render_mode='human', seed=seed)\n",
    "    \n",
    "    fig, ax = plt.subplots(1)\n",
    "\n",
    "    current_directory = os.getcwd()\n",
    "    print(f\"Current directory: {current_directory}\")\n",
    "    \n",
    "    # List all files and directories in the current directory\n",
    "    contents = os.listdir(current_directory)\n",
    "    print(\"Contents of the directory:\")\n",
    "    for item in contents:\n",
    "        print(item)\n",
    "\n",
    "    q_net = build_q_network(env, cfg).to(DEVICE)\n",
    "    optimizer = torch.optim.Adam(q_net.parameters(), lr=cfg.lr)  # Placeholder optimizer for load_checkpoint\n",
    "    checkpoint_path = 'checkpoint_cartpole_stage0.pth'  # Adjust path to your saved checkpoint\n",
    "    if os.path.exists(checkpoint_path):\n",
    "        print(f\"File found at: {checkpoint_path}\")\n",
    "    \n",
    "    load_checkpoint(checkpoint_path, q_net, optimizer)  # Load the trained model\n",
    "\n",
    "    raise Error\n",
    "    \n",
    "    q_net.eval()  # Set to evaluation mode\n",
    "\n",
    "    # --- Your Agent's Logic Starts Here ---\n",
    "    obs, _ = env.reset(seed)\n",
    "    state = first_obs(obs).astype(np.float32)\n",
    "    episode_return = 0.0\n",
    "\n",
    "    print(\"Starting agent visualization... Press 'Stop' to interrupt cleanly.\")\n",
    "    print(\"IMPORTANT: After interrupting, you MUST restart the kernel before running this cell again.\")\n",
    "\n",
    "    for t in range(steps_to_show):\n",
    "        # The inner 'try...except' for clean interruption\n",
    "        try:\n",
    "            # Get action from your policy instead of random\n",
    "            # q_net = build_q_network(env, cfg).to(DEVICE)\n",
    "            a = greedy_action(state)\n",
    "\n",
    "            # Step the environment\n",
    "            obs, reward, term, trunc, _info = env.step(a)\n",
    "\n",
    "            # --- Apply the Visualization Pipeline ---\n",
    "            # 1. Render the game to the invisible virtual display\n",
    "            env.render()\n",
    "\n",
    "            # 2. Grab a cropped screenshot of the virtual display\n",
    "            frame = ImageGrab.grab(bbox=bbox)\n",
    "\n",
    "            # 3. Display the screenshot in the notebook\n",
    "            clear_output(wait=True)\n",
    "            ax.imshow(frame)\n",
    "            ax.axis('off')\n",
    "            display(fig)\n",
    "\n",
    "            time.sleep(0.01)\n",
    "            # --- End of Visualization Pipeline ---\n",
    "\n",
    "            # Continue with your agent's logic\n",
    "            state = first_obs(obs).astype(np.float32)\n",
    "            episode_return += float(first_obs(reward))\n",
    "            done = bool(first_obs(term)) or bool(first_obs(trunc))\n",
    "            if done:\n",
    "                print(\"Episode finished.\")\n",
    "                break\n",
    "\n",
    "        except KeyboardInterrupt:\n",
    "            print(\"\\n\\nInterrupted by user. The kernel is now in an unstable state and must be restarted.\")\n",
    "            break\n",
    "\n",
    "    print(f'Greedy policy return: {episode_return:.2f}')\n",
    "\n",
    "finally:\n",
    "    # This block is ALWAYS executed, ensuring a safe shutdown.\n",
    "    print(\"Cleaning up resources...\")\n",
    "    os.chdir(original_dir)\n",
    "\n",
    "    if env is not None:\n",
    "        env.close()\n",
    "\n",
    "    time.sleep(0.5)\n",
    "\n",
    "    if vdisplay is not None:\n",
    "        vdisplay.stop()\n",
    "    print(\"Cleanup complete. If you interrupted the cell, please restart the kernel now.\")"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def save_checkpoint(path, q_net, optimizer, cfg: Config, stage_name: str):\n",
    "    torch.save({\n",
    "        'model': q_net.state_dict(),\n",
    "        'optimizer': optimizer.state_dict(),\n",
    "        'config': dataclasses.asdict(cfg),\n",
    "        'stage': stage_name,\n",
    "    }, path)\n",
    "    print('Saved checkpoint:', path)\n",
    "\n",
    "def load_checkpoint(path, q_net, optimizer):\n",
    "    data = torch.load(path, map_location=DEVICE)\n",
    "    q_net.load_state_dict(data['model'])\n",
    "    optimizer.load_state_dict(data['optimizer'])\n",
    "    print('Loaded checkpoint from', path)\n",
    "    print(data['config'])\n",
    " "
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": ""
  }
 ],
 "metadata": {
  "deepnote_notebook_id": "43ce95ddb5754e06b99d62b9ae367324",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
