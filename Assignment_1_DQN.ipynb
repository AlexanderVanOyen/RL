{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "a5db11000fcd4b82b6c8550e661dfafc",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "# Assignment 1: Deep Q-Networks (DQN) with [PufferLib Ocean Environments](https://puffer.ai/ocean.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "876862545d4b4acbac8ca57af4d6d5f1",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "*This assignment must be completed solely by the members of your group. Sharing of code between groups is strictly prohibited. However, you are allowed to discuss general solution approaches or share publicly available resources with members of other groups. Therefore, clearly indicate which public resources you consulted and/or copied code from. Any plagiarism between groups will result in the initiation of a fraud procedure with the director of education.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "In this notebook, your group will implement a fully vectorized Deep Q-Network (DQN) agent and several of its key improvements, inspired by the [\"Rainbow\"](https://arxiv.org/abs/1710.02298) paper. The project is divided into a foundational group task followed by individual implementation of specific DQN extensions. Your goal is to compare the performance of these components and then combine them into the ultimate rainbow-lite agent.\n",
    "\n",
    "### Project Structure:\n",
    "1.  **Foundational (Group Task):** The entire group will first collaborate to right the necessary code to  make simple DQN work.\n",
    "2.  **Individual Extensions:** Once the baseline is complete, each group member will be assigned one of the following algorithmic extensions to implement:\n",
    "    *   **Extension A:** Double DQN & Dueling DQN (Architectural Improvements)\n",
    "    *   **Extension B:** Prioritized Experience Replay (PER) (Advanced Sampling)\n",
    "    *   **Extension C:** N-Step Returns & Noisy Networks (Target & Exploration Improvements)\n",
    "3.  **Analysis (Group Task):** The group will integrate all components, run a comparative analysis on the `breakout` environment, and collaboratively answer the conceptual and reflection questions in a final report.\n",
    "\n",
    "Cells with `TODO` indicate where you must add or adjust code. However feel free to modify any part of the code to improve clarity, efficiency, or performance. You are encouraged to experiment with hyperparameters and other design choices to optimize your agent's learning.\n",
    "\n",
    "---\n",
    "\n",
    "### Environment Tiers:\n",
    "You will use three types of environments from `pufferlib.ocean`:\n",
    "1.  **Debug:** `squared` (fast iterations; verify your code  is correct, your agent should learn within seconds and become optimal around 2 minutes, max episode score is 1).\n",
    "2.  **Comparing:** `cartpole` (fast iterations; verify your extensions and evaluate the agent, learning signs should be clear within 2 minutes but optimal agent could take 30+ minutes, max episode score is 199).\n",
    "3. **Challenge:** `breakout` (focus on performance, and diagnostics, an optimal agent can take multiple hours, you will get bonus points if you find an optimal agent in this environment, max episode score is 864).\n",
    "\n",
    "You are allowed to experiment with other environments from `pufferlib.ocean` if you wish, but the above three are mandatory.\n",
    "\n",
    "---\n",
    "\n",
    "### Report\n",
    "We expect you to write the report in a separate document and submit both the notebook and the report on Ufora. Remember that a plot often tells more than a thousand words. When you explain somethings or you show your results, try to add figures to accompany the text. Go beyond just souly describing what you did or how the techniques work but additionally, share your observations, reflect on why things turned out the way they did, and help us understand the story behind your findings.\n",
    "\n",
    "The report has a soft limit of 8 pages. You are welcome to go over this limit if you keep your writing concise and make sure any extra content is relevant. If you are highly motivated and want to test many different things, feel free to share your findings as long as you follow the requirements mentioned earlier.\n",
    "\n",
    "If your group is smaller than 3 people, you can choose to implement more than one extension per person. In that case, please clearly indicate who did what in the report.\n",
    "\n",
    "**Deadline:** October 26, 2025, 23:59\n",
    "\n",
    "\n",
    "### Office hours\n",
    "We will hold weekly office hours to help you with questions about the assignment. Your are more then welcome between 13:30 and 16:00. We reserved IDLab9 (IGent) for you. If we are not there, you can find us in our offices on the 10th floor (IGent): 200.026 (Elias and Thibault), 200.031 (Ciem).\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### Docker\n",
    "To ensure a consistent and hassle-free setup, we have prepared a Docker image with all necessary dependencies pre-installed. This is the recommended way to run the notebook to avoid issues with package versions.\n",
    "\n",
    "You can also use there container more information at [puffertank](https://github.com/PufferAI/PufferTank).\n",
    "\n",
    "The Docker image is available on Docker Hub:\n",
    "*   **Image:** `ciemcornelissen/puffer-notebook:latest`\n",
    "*   **URL:** [https://hub.docker.com/r/ciemcornelissen/puffer-notebook](https://hub.docker.com/r/ciemcornelissen/puffer-notebook)\n",
    "\n",
    "You can use this image in several ways. Below are instructions for three common setups: VS Code (recommended for local use), Deepnote for a cloud-based environment, and locally with classic Jupyter.\n",
    "\n",
    "\n",
    "### Option 1: Local Development with VS Code\n",
    "\n",
    "This is the most seamless way to work locally. VS Code's \"Dev Containers\" extension allows you to open your project folder directly inside the running container, giving you access to a fully integrated terminal, file editor, and Jupyter renderer.\n",
    "\n",
    "**Prerequisites:**\n",
    "*   [Docker Desktop](https://www.docker.com/products/docker-desktop/) installed and running.\n",
    "*   [Visual Studio Code](https://code.visualstudio.com/) installed.\n",
    "*   The [Dev Containers extension](https://marketplace.visualstudio.com/items?itemName=ms-vscode-remote.remote-containers) installed in VS Code.\n",
    "\n",
    "**Instructions:**\n",
    "\n",
    "1.  **Pull and Run the Container:** Open a terminal in your project directory (where this notebook is located) and run the appropriate command below. **Keep this terminal window open.**\n",
    "\n",
    "    *   **For Linux, Windows (WSL), and Intel-based Macs:**\n",
    "        ```bash\n",
    "        docker run -it --rm -p 8888:8888 -v \"$(pwd)\":/app --name puffer-dev ciemcornelissen/puffer-notebook:latest\n",
    "\n",
    "    *   **For Apple Silicon (M1/M2/M3) Macs:** You must add the `--platform` flag to emulate the correct architecture.\n",
    "        ```bash\n",
    "        docker run -it --rm --platform linux/amd64 -p 8888:8888 -v \"$(pwd)\":/app --name puffer-dev ciemcornelissen/puffer-notebook:latest\n",
    "\n",
    "    *   *(Note: We use `--name puffer-dev` to give the container an easy-to-find name).*\n",
    "\n",
    "2.  **Attach VS Code to the Container:**\n",
    "    *   Open VS Code.\n",
    "    *   Attach vsual studio code to the running container:\n",
    "        1.  Click on the container extension in VS Code.\n",
    "        2.  Select **\"Attach to Running Container...\"**.\n",
    "        3.  Choose the container named `puffer-dev`.\n",
    "    *   Or open the Command Palette (`Ctrl+Shift+P` on Windows/Linux, `Cmd+Shift+P` on Mac).\n",
    "    *   Type and select **\"Dev Containers: Attach to Running Container...\"**.\n",
    "    *   Choose the right container from the list.\n",
    "\n",
    "\n",
    "3.  **Start Working:** A new VS Code window will open, connected to the container. Click **\"Open Folder\"** to open your project files. You can now edit code, run the notebook, and use the terminal as if you were running natively inside the correct environment.\n",
    "\n",
    "**If you can not select a kernel when trying to run code in the notebook then you need to update the jupyter extension of vsc. This can be done by clicking on the extensions tab on the left and searching for jupyter. Then click on the little gear icon and select install a specific version and choose the newest version.**\n",
    "\n",
    "### Option 2: Cloud Development with [Deepnote](https://deepnote.com/)\n",
    "\n",
    "If you prefer not to install Docker locally, you can use Deepnote to run the environment in the cloud.\n",
    "\n",
    "**Instructions:**\n",
    "\n",
    "1.  In your Deepnote project, navigate to the **Environment** tab in the left sidebar at the bottem under machine.\n",
    "2.  Click on the **\"Set up a new Docker image\"**.\n",
    "3.  In the \"Docker image\" field, paste the image name:\n",
    "    ```\n",
    "    ciemcornelissen/puffer-notebook:latest\n",
    "\n",
    "4.  Deepnote will pull the image. Once it's ready, you'll be ready to work.\n",
    "\n",
    "\n",
    "\n",
    "### Option 3: Local Development with Classic Jupyter\n",
    "\n",
    "This method uses the command line to start a Jupyter server, which you access through your web browser.\n",
    "\n",
    "**Instructions:**\n",
    "\n",
    "1.  **Pull the Image:**\n",
    "    ```bash\n",
    "    docker pull ciemcornelissen/puffer-notebook:latest\n",
    "\n",
    "2.  **Run the Container:** Run the command below from your project directory.\n",
    "\n",
    "    *   **For Linux, Windows (WSL), and Intel-based Macs:**\n",
    "        ```bash\n",
    "        docker run -it --rm -p 8888:8888 -v \"$(pwd)\":/app ciemcornelissen/puffer-notebook:latest\n",
    "\n",
    "    *   **For Apple Silicon (M1/M2/M3) Macs:**\n",
    "        ```bash\n",
    "        docker run -it --rm --platform linux/amd64 -p 8888:8888 -v \"$(pwd)\":/app ciemcornelissen/puffer-notebook:latest\n",
    "\n",
    "3.  **Access Jupyter:** Your terminal will display a URL (e.g., `http://127.0.0.1:8888/lab?token=...`). Copy and paste this full URL into your web browser to start the notebook. Your files will be in the `/app` directory.\n",
    "\n",
    "\n",
    "\n",
    "### **Important Note for Apple Silicon (M1/M2/M3/M4) Users**\n",
    "\n",
    "The Docker image is built for the `amd64` (Intel/AMD) architecture. If you are using a Mac with Apple Silicon, you must tell Docker to emulate this architecture by adding the `--platform linux/amd64` flag.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "afa901fd106d47f199d2756d2676af93",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "## 1.1 Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#when using deepnote and you get the error no module pufferlib run this cell\n",
    "\n",
    "# try:\n",
    "#     import __editable___pufferlib_3_0_0_finder as _pf\n",
    "#     _pf.install()          # registers the module loader\n",
    "#     import pufferlib\n",
    "#     print(\"Loaded:\", pufferlib.__file__)\n",
    "# except Exception as e:\n",
    "#     print(\"Failed to manually activate editable hook:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "cell_id": "50d16c26487a4221aa1e54ef4dba5644",
    "deepnote_cell_type": "code"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "import random, dataclasses\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Optional, Dict, Any\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Import ocean environments, you can import more if you want to experiment with other envs\n",
    "from pufferlib.ocean.squared import squared\n",
    "from pufferlib.ocean.pong import pong\n",
    "from pufferlib.ocean.pacman import pacman\n",
    "from pufferlib.ocean.enduro import enduro\n",
    "from pufferlib.ocean.tetris import tetris\n",
    "from pufferlib.ocean.breakout import breakout\n",
    "from pufferlib.ocean.cartpole import cartpole\n",
    "\n",
    "import gymnasium as gym\n",
    "\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding PufferLib and Ocean Environments\n",
    "\n",
    "### What is [PufferLib](https://puffer.ai/ocean.html)?\n",
    "\n",
    "**PufferLib** is a high-performance reinforcement learning framework designed to bridge the gap between research and production RL. It was created to address common pain points in RL development:\n",
    "\n",
    "- **Speed:** PufferLib environments are highly optimized, often running 10-100x faster than traditional implementations\n",
    "- **Scalability:** Built-in support for massive parallelization across thousands of environments\n",
    "- **Simplicity:** Clean, minimal API that follows Gymnasium standards\n",
    "- **GPU-Native:** Environments can run directly on GPU, eliminating CPU-GPU transfer bottlenecks\n",
    "\n",
    "\n",
    "### PufferLib Ocean: Educational RL Environments\n",
    "\n",
    "**Ocean** is PufferLib's collection of lightweight, educational environments. The name reflects its purpose: a \"sea\" of diverse environments for learning and experimentation. Ocean environments are specifically designed for:\n",
    "\n",
    "1. **Fast Debugging:** Quickly verify your algorithm works before scaling up\n",
    "2. **Rapid Prototyping:** Test new ideas without waiting hours for results\n",
    "3. **Educational Clarity:** Simpler codebases that are easier to understand and modify\n",
    "4. **Vectorization by Default:** Learn modern RL practices from the start\n",
    "\n",
    "### Available Ocean Environments\n",
    "\n",
    "Ocean includes a variety of environments with different characteristics (more can be found in the [docs](https://github.com/PufferAI/PufferLib/tree/3.0/pufferlib/ocean)):\n",
    "\n",
    "- **Classic Control:** `cartpole`\n",
    "  - Simple physics simulations\n",
    "  - Low-dimensional observations\n",
    "  - Great for debugging and initial testing\n",
    "\n",
    "- **Grid Worlds:** `squared`, `minigrid_variants`\n",
    "  - Discrete state/action spaces\n",
    "  - Fast iteration times\n",
    "  - Perfect for verifying algorithm correctness\n",
    "\n",
    "- **Atari-Style:** `breakout`, `pong`, `pacman`, `enduro`\n",
    "  - More complex visual observations\n",
    "  - Longer training times\n",
    "  - Closer to real-world RL challenges\n",
    "\n",
    "- **Puzzle Games:** `tetris`\n",
    "  - Strategic planning required\n",
    "  - Sparse rewards\n",
    "  - Advanced challenge tasks\n",
    "\n",
    "### Why PufferLib for This Assignment?\n",
    "\n",
    "We chose PufferLib Ocean for several pedagogical reasons:\n",
    "\n",
    "1. **Immediate Feedback:** Fast environments mean you can iterate quickly on your code\n",
    "2. **Realistic Scale:** The environment enables easy vectorized environments like in real RL research\n",
    "3. **Low Hardware Requirements:** Efficient implementation means you don't need expensive GPUs because of vectorised observations\n",
    "4. **Clear Progression:** From simple (Squared) to complex (Breakout) in the same framework\n",
    "5. **Industry Relevance:** PufferLib is used in actual RL research and applications\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "d293b91f1fd3434292141c0ac7f10498",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "## 1.2 Environment Factories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "cell_id": "49e63a1849a74541812980fd1b1418f5",
    "deepnote_cell_type": "code"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Debug env observation shape: (1, 121)\n",
      "Action space: MultiDiscrete([5])\n",
      "Observation space: Box(0, 1, (1, 121), uint8)\n"
     ]
    }
   ],
   "source": [
    "class TimeLimitVec:\n",
    "    \"\"\"\n",
    "    Generic vector env time-limit wrapper for PufferLib/Gymnasium-like envs.docker run -it --rm -p 8888:8888 -v \"$(pwd)\":/app ciemcornelissen/puffer-notebook:latest\n",
    "    - Marks truncations True when per-env step count reaches max_episode_steps.\n",
    "    - Preserves existing terminals/truncations from the underlying env.\n",
    "    - Resets per-env counters on reset or when an env ends.\n",
    "    \"\"\"\n",
    "    def __init__(self, env: Any, max_episode_steps: int, info_key: str = \"time_limit\"):\n",
    "        assert max_episode_steps and max_episode_steps > 0\n",
    "        self.env = env\n",
    "        self.max_episode_steps = int(max_episode_steps)\n",
    "        self.info_key = info_key\n",
    "\n",
    "        # Mirror common attributes for compatibility\n",
    "        self.single_observation_space = getattr(env, \"single_observation_space\", None)\n",
    "        self.single_action_space = getattr(env, \"single_action_space\", None)\n",
    "        self.observation_space = getattr(env, \"observation_space\", None)\n",
    "        self.action_space = getattr(env, \"action_space\", None)\n",
    "        self.num_agents = getattr(env, \"num_agents\", 1)\n",
    "\n",
    "        self._steps = np.zeros(self.num_agents, dtype=np.int64)\n",
    "\n",
    "    def reset(self, seed: Optional[int] = 0):\n",
    "        obs, infos = self.env.reset(seed)\n",
    "        self._steps[...] = 0\n",
    "        return obs, infos\n",
    "\n",
    "    # def reset(self, seed: Optional[int] = None): \n",
    "    #     if seed is None:\n",
    "    #         seed = random.randint(1, 2**32 - 1)\n",
    "    #     obs, infos = self.env.reset(seed)\n",
    "    #     self._steps[...] = 0\n",
    "    #     return obs, infos\n",
    "\n",
    "\n",
    "    def step(self, actions):\n",
    "        obs, rewards, terminals, truncations, infos = self.env.step(actions)\n",
    "\n",
    "        t = np.asarray(terminals, dtype=bool)\n",
    "        tr = np.asarray(truncations, dtype=bool)\n",
    "        if t.ndim == 0:\n",
    "            t = t.reshape(1)\n",
    "        if tr.ndim == 0:\n",
    "            tr = tr.reshape(1)\n",
    "\n",
    "        active = ~(t | tr)\n",
    "        # Increment only for envs still active before this step's end flags\n",
    "        self._steps[active] += 1\n",
    "\n",
    "        # Apply time limit where not already ended this step\n",
    "        timeouts = (self._steps >= self.max_episode_steps) & active\n",
    "        if np.any(timeouts):\n",
    "            tr = np.logical_or(tr, timeouts)\n",
    "            self._steps[timeouts] = 0\n",
    "            if infos is None:\n",
    "                infos = []\n",
    "            if not isinstance(infos, list):\n",
    "                infos = [infos]\n",
    "            infos.append({self.info_key: {\"timeouts\": timeouts.copy()}})\n",
    "\n",
    "        # Also reset counters for any envs that ended naturally\n",
    "        ended = t | tr\n",
    "        if np.any(ended):\n",
    "            self._steps[ended] = 0\n",
    "\n",
    "        return obs, rewards, t, tr, infos\n",
    "\n",
    "    def render(self, *args, **kwargs):\n",
    "        return getattr(self.env, \"render\")(*args, **kwargs)\n",
    "\n",
    "    def close(self):\n",
    "        return getattr(self.env, \"close\")()\n",
    "def make_env(name: str, seed: int = 0):\n",
    "    env_map = {\n",
    "        'squared': squared,\n",
    "        'pong': pong,\n",
    "        'pacman': pacman,\n",
    "        'enduro': enduro,\n",
    "        'tetris': tetris,\n",
    "        'breakout': breakout,\n",
    "        'cartpole': cartpole\n",
    "    }\n",
    "    if name not in env_map:\n",
    "        raise ValueError(f'Unknown environment {name}')\n",
    "\n",
    "    def thunk():\n",
    "        # Get the module from the map\n",
    "        env_module = env_map[name]\n",
    "\n",
    "        # For example, in the 'squared' module, there is a 'Squared' class.\n",
    "        env_class_name = name.capitalize()\n",
    "        env_class = getattr(env_module, env_class_name)\n",
    "\n",
    "        raw_env = env_class(num_envs=1, render_mode=None, seed=seed)  # Instantiate the class\n",
    "        env = TimeLimitVec(raw_env, max_episode_steps=10_000)\n",
    "        env.reset(seed=seed)\n",
    "        return env\n",
    "    return thunk\n",
    "\n",
    "debug_env_name = 'squared'\n",
    "intermediate_env_name = 'cartpole'  # Change to 'pacman', 'enduro', or 'tetris' if desired\n",
    "challenge_env_name = 'breakout'\n",
    "\n",
    "test_env = make_env(debug_env_name)()\n",
    "obs, info = test_env.reset()\n",
    "print('Debug env observation shape:', np.array(obs).shape)\n",
    "print('Action space:', test_env.action_space)\n",
    "print('Observation space:', test_env.observation_space)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "860f7b4e661a4aaa84ef33d95edf1d1e",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "## 1.3 Configuration\n",
    "Adjust hyperparameters and add hyperparameters as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "cell_id": "7ca2c9f04cc141b9a8e5cfd8d2bede7f",
    "deepnote_cell_type": "code"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Config(seed=7, gamma=0.99, lr=0.0001, batch_size=64, buffer_capacity=200000, min_buffer_size=5000, max_steps_per_env=100000, max_episode_len=2000, epsilon_start=1.0, epsilon_end=0.05, epsilon_decay_steps=20000, target_update_interval=500, train_freq=4, gradient_clip=10.0, reward_clip=None, use_double_dqn=True, use_dueling=False, vector_envs=1, log_interval_episodes=100, eval_episodes=5, curriculum=['squared', 'cartpole', 'breakout'], hidden_size=256)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "@dataclass\n",
    "class Config:\n",
    "    seed: int = 7\n",
    "    gamma: float = 0.99\n",
    "    lr: float = 1e-4\n",
    "    batch_size: int = 64\n",
    "    buffer_capacity: int = 200_000\n",
    "    min_buffer_size: int = 5_000\n",
    "    max_steps_per_env: int = 100_000  # Increase for more serious training\n",
    "    max_episode_len: int = 2000\n",
    "    epsilon_start: float = 1.0\n",
    "    epsilon_end: float = 0.05\n",
    "    epsilon_decay_steps: int = 20_000\n",
    "    target_update_interval: int = 500\n",
    "    train_freq: int = 4\n",
    "    gradient_clip: float = 10.0\n",
    "    reward_clip: Optional[float] = None  # Set to e.g. 1.0 for clipping\n",
    "    use_double_dqn: bool = True         # TODO: Try True in an extension\n",
    "    use_dueling: bool = False            # TODO: Try True in an extension\n",
    "    vector_envs: int = 1                 # Extension: use >1 for parallel data can increase learning speed but might be harder to implement\n",
    "    log_interval_episodes: int = 100\n",
    "    eval_episodes: int = 5\n",
    "    curriculum: List[str] = None\n",
    "    hidden_size: int = 256               # You can adjust hidden size\n",
    "\n",
    "cfg = Config()\n",
    "cfg.curriculum = [debug_env_name, intermediate_env_name, challenge_env_name]\n",
    "\n",
    "random.seed(cfg.seed)\n",
    "np.random.seed(cfg.seed)\n",
    "torch.manual_seed(cfg.seed)\n",
    "cfg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "015590713e13436db31f422dfafe9efa",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "## 1.4 Replay Buffer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "cell_id": "ecbe377120604089a88f3c4ce07a2665",
    "deepnote_cell_type": "code"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing ReplayBuffer...\n",
      "✓ ReplayBuffer tests passed!\n"
     ]
    }
   ],
   "source": [
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity: int, obs_shape: tuple, device):\n",
    "        self.capacity = capacity\n",
    "        self.device = device\n",
    "        self.obs_shape = obs_shape\n",
    "        self.position = 0\n",
    "        self.size = 0\n",
    "        \n",
    "        # Pre-allocate numpy arrays for storage\n",
    "        self.obs = np.zeros((capacity, *obs_shape), dtype=np.float32)\n",
    "        self.actions = np.zeros((capacity, 1), dtype=np.int64)  # Shape: (capacity, 1)\n",
    "        self.rewards = np.zeros((capacity, 1), dtype=np.float32)\n",
    "        self.next_obs = np.zeros((capacity, *obs_shape), dtype=np.float32)\n",
    "        self.dones = np.zeros((capacity, 1), dtype=np.float32)\n",
    "    \n",
    "    def push(self, obs, action, reward, next_obs, done):\n",
    "        \"\"\"Store a transition\"\"\"\n",
    "        self.obs[self.position] = obs\n",
    "        self.actions[self.position, 0] = action  # Store as scalar in (1,) shaped array\n",
    "        self.rewards[self.position, 0] = reward\n",
    "        self.next_obs[self.position] = next_obs\n",
    "        self.dones[self.position, 0] = float(done)\n",
    "        \n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "        self.size = min(self.size + 1, self.capacity)\n",
    "    \n",
    "    def sample(self, batch_size: int):\n",
    "        \"\"\"Sample a batch of transitions and convert to PyTorch tensors\"\"\"\n",
    "        indices = np.random.choice(self.size, batch_size, replace=False)\n",
    "        \n",
    "        # Convert to tensors - actions will have shape (batch_size, 1)\n",
    "        obs = torch.FloatTensor(self.obs[indices]).to(self.device)\n",
    "        actions = torch.LongTensor(self.actions[indices]).to(self.device)  # (batch_size, 1)\n",
    "        rewards = torch.FloatTensor(self.rewards[indices]).to(self.device)\n",
    "        next_obs = torch.FloatTensor(self.next_obs[indices]).to(self.device)\n",
    "        dones = torch.FloatTensor(self.dones[indices]).to(self.device)\n",
    "        \n",
    "        return obs, actions, rewards, next_obs, dones\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.size\n",
    "\n",
    "# Test ReplayBuffer\n",
    "print(\"Testing ReplayBuffer...\")\n",
    "test_buffer = ReplayBuffer(100, (4,), DEVICE)\n",
    "test_buffer.push(np.array([1,2,3,4]), 0, 1.0, np.array([2,3,4,5]), False)\n",
    "assert len(test_buffer) == 1, \"Buffer should have 1 element\"\n",
    "obs, actions, rewards, next_obs, dones = test_buffer.sample(1)\n",
    "assert obs.shape == (1, 4), f\"Expected shape (1, 4), got {obs.shape}\"\n",
    "print(\"✓ ReplayBuffer tests passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "da1b64981ced4db5951c9c14d54bb70f",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "## 1.5 Epsilon Schedule\n",
    "Linear decay from start to end over `epsilon_decay_steps`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "cell_id": "b6ce250528e240b4aa946a31144ff2cc",
    "deepnote_cell_type": "code"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epsilon(0)= 1.0 Epsilon(mid)= 0.525 Epsilon(end)= 0.05\n"
     ]
    }
   ],
   "source": [
    "def epsilon_by_step(step: int, cfg: Config) -> float:\n",
    "    per_step = (cfg.epsilon_start-cfg.epsilon_end)/cfg.epsilon_decay_steps\n",
    "    if step < cfg.epsilon_decay_steps:\n",
    "        eps = cfg.epsilon_start - (step*per_step)\n",
    "    else:\n",
    "        eps = cfg.epsilon_end\n",
    "    return eps\n",
    "\n",
    "print('Epsilon(0)=', epsilon_by_step(0, cfg), 'Epsilon(mid)=', epsilon_by_step(cfg.epsilon_decay_steps//2, cfg), 'Epsilon(end)=', epsilon_by_step(cfg.epsilon_decay_steps*2, cfg))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "0af0c9e0a6a24a11a9c3114be6ce2e77",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "## 1.6 Q-Network Architectures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "cell_id": "c4ba582e9935425882af600d137664b4",
    "deepnote_cell_type": "code"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obs space: 121\n",
      "MLPQ(\n",
      "  (network): Sequential(\n",
      "    (0): Linear(in_features=121, out_features=256, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=256, out_features=256, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=256, out_features=5, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class MLPQ(nn.Module):\n",
    "    def __init__(self, obs_dim: int, action_dim: int, dueling=False):\n",
    "        super().__init__()\n",
    "        self.hidden_size = cfg.hidden_size  # Typically 128, 256, or 512\n",
    "        self.dueling = dueling\n",
    "        self.action_dim = action_dim\n",
    "        \n",
    "        if not dueling:\n",
    "            # Standard DQN: obs -> hidden -> hidden -> Q-values\n",
    "            self.network = nn.Sequential(\n",
    "                nn.Linear(obs_dim, self.hidden_size),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(self.hidden_size, self.hidden_size),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(self.hidden_size, action_dim)\n",
    "            )\n",
    "        else:\n",
    "            # Dueling DQN: separate value and advantage streams\n",
    "            # Shared feature extractor\n",
    "            self.feature = nn.Sequential(\n",
    "                nn.Linear(obs_dim, self.hidden_size),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(self.hidden_size, self.hidden_size),\n",
    "                nn.ReLU()\n",
    "            )\n",
    "            \n",
    "            # Value stream: V(s) - scalar value of state\n",
    "            self.value_stream = nn.Sequential(\n",
    "                nn.Linear(self.hidden_size, self.hidden_size),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(self.hidden_size, 1)\n",
    "            )\n",
    "            \n",
    "            # Advantage stream: A(s,a) - advantage of each action\n",
    "            self.advantage_stream = nn.Sequential(\n",
    "                nn.Linear(self.hidden_size, self.hidden_size),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(self.hidden_size, action_dim)\n",
    "            )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        if not self.dueling:\n",
    "            # Standard DQN: directly output Q-values\n",
    "            return self.network(x)\n",
    "        else:\n",
    "            # Dueling DQN: Q(s,a) = V(s) + (A(s,a) - mean(A(s,a)))\n",
    "            features = self.feature(x)\n",
    "            \n",
    "            value = self.value_stream(features)\n",
    "            advantages = self.advantage_stream(features)\n",
    "            \n",
    "            q_values = value + (advantages - advantages.mean(dim=1, keepdim=True))\n",
    "            \n",
    "            return q_values\n",
    "\n",
    "def build_q_network(env, cfg: Config):\n",
    "    obs_space = env.single_observation_space\n",
    "    print('Obs space:', obs_space.shape[0])\n",
    "    act_space = env.single_action_space\n",
    "    action_dim = act_space.n\n",
    "    assert isinstance(act_space, gym.spaces.Discrete), 'DQN requires discrete actions'\n",
    "    if len(obs_space.shape) == 1:\n",
    "        return MLPQ(obs_space.shape[0], action_dim, dueling=cfg.use_dueling)\n",
    "\n",
    "\n",
    "# Quick test on debug env\n",
    "net_test_env = test_env\n",
    "test_net = build_q_network(net_test_env, cfg).to(DEVICE)\n",
    "print(test_net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "53904af525cf4b91995182c37c0bd733",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "## 1.7 Action Selection\n",
    "Standard epsilon-greedy policy. (Later you might incorporate noisy networks or exploration bonuses.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "cell_id": "af4deae398504b41ace66db77ffef56c",
    "deepnote_cell_type": "code"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random/greedy test action: [1]\n"
     ]
    }
   ],
   "source": [
    "@torch.no_grad()\n",
    "def select_action(q_net, obs, epsilon: float, env):\n",
    "    if random.random() < epsilon:\n",
    "        return env.action_space.sample()\n",
    "    if isinstance(obs, torch.Tensor):\n",
    "        obs_t = obs.to(DEVICE, dtype=torch.float32, non_blocking=True)\n",
    "    else:\n",
    "        obs_t = torch.as_tensor(obs, dtype=torch.float32, device=DEVICE)\n",
    "    q_vals = q_net(obs_t)\n",
    "    return int(q_vals.argmax(dim=1).item())\n",
    "\n",
    "# Test call\n",
    "dummy_action = select_action(test_net, obs, 1.0, test_env)\n",
    "print('Random/greedy test action:', dummy_action)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "0f71e78fee7341fdb9365eba355d2b3f",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "## 1.8 Training Step\n",
    "Compute standard DQN loss or Double DQN target if `cfg.use_double_dqn=True`.\n",
    "\n",
    "Target for vanilla DQN:\n",
    "$$ y = r + (1-d) \\gamma \\max_{a'} Q_{target}(s', a') $$\n",
    "\n",
    "Double DQN variant:\n",
    "$$ a^* = \\arg\\max_{a'} Q_{online}(s', a') \\quad; \\quad y = r + (1-d) \\gamma Q_{target}(s', a^*) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "cell_id": "5625b0a7417c451c826e4d5de225a77b",
    "deepnote_cell_type": "code"
   },
   "outputs": [],
   "source": [
    "loss_fn = nn.SmoothL1Loss()\n",
    "\n",
    "def train_step(q_net, target_net, optimizer, replay: ReplayBuffer, cfg: Config):\n",
    "    \"\"\"\n",
    "    Performs one training step of DQN.\n",
    "    \n",
    "    Args:\n",
    "        q_net: Online Q-network (being trained)\n",
    "        target_net: Target Q-network (frozen, updated periodically)\n",
    "        optimizer: Optimizer for q_net\n",
    "        replay: Replay buffer containing transitions\n",
    "        cfg: Configuration object\n",
    "    \n",
    "    Returns:\n",
    "        Loss value or None if buffer not ready\n",
    "    \"\"\"\n",
    "    # Don't train until we have enough experiences\n",
    "    if len(replay) < cfg.min_buffer_size:\n",
    "        return None\n",
    "    \n",
    "    obs, actions, rewards, next_obs, dones = replay.sample(cfg.batch_size)\n",
    "\n",
    "    q_values = q_net(obs)  # Shape: (batch_size, action_dim)\n",
    "\n",
    "    q_sa = q_values.gather(1, actions)  # Shape: (batch_size, 1)\n",
    "    \n",
    "    # ============= Compute Target Q-values =============\n",
    "    with torch.no_grad():  # Don't compute gradients for target\n",
    "        if cfg.use_double_dqn:\n",
    "            \n",
    "            # Step 1: Use online network to find best action in next state\n",
    "            next_q_values_online = q_net(next_obs)  # Shape: (batch_size, action_dim)\n",
    "            best_actions = next_q_values_online.argmax(dim=1, keepdim=True)  # Shape: (batch_size, 1)\n",
    "            \n",
    "            # Step 2: Use target network to evaluate that action\n",
    "            next_q_values_target = target_net(next_obs)  # Shape: (batch_size, action_dim)\n",
    "            next_q_value = next_q_values_target.gather(1, best_actions)  # Shape: (batch_size, 1)\n",
    "            \n",
    "        else:\n",
    "            # Vanilla DQN: Use target network for both selection and evaluation\n",
    "            next_q_values = target_net(next_obs)  # Shape: (batch_size, action_dim)\n",
    "            next_q_value = next_q_values.max(dim=1, keepdim=True)[0]  # Shape: (batch_size, 1)\n",
    "            # Note: max returns (values, indices), we take [0] for values\n",
    "        \n",
    "        # Compute TD target: y = r + γ * max Q(s', a') * (1 - done)\n",
    "        # The (1 - dones) term zeros out the next_q_value if episode ended\n",
    "        target = rewards + cfg.gamma * next_q_value * (1 - dones)  # Shape: (batch_size, 1)\n",
    "\n",
    "    loss = loss_fn(q_sa, target)\n",
    "    \n",
    "    # Standard PyTorch training loop\n",
    "    optimizer.zero_grad()  # Clear previous gradients\n",
    "    loss.backward()  # Compute gradients\n",
    "    nn.utils.clip_grad_norm_(q_net.parameters(), cfg.gradient_clip)  # Prevent exploding gradients\n",
    "    optimizer.step()  # Update weights\n",
    "    \n",
    "    return float(loss.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "87f36d38e660428ba51b130a1d0806f4",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "## 1.9 Reward Processing\n",
    "Add optional reward clipping to stabilize training on environments with high or varied reward magnitudes.\n",
    "\n",
    "If `cfg.reward_clip` is set, clip reward to `[-cfg.reward_clip, cfg.reward_clip]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "cell_id": "a29b359c88cd4959a25fa0d8d75a5d75",
    "deepnote_cell_type": "code"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reward test (no clip): 5.0\n",
      "Reward test (clip=1.0): 1.0\n"
     ]
    }
   ],
   "source": [
    "def process_reward(r: float, cfg: Config):\n",
    "    if cfg.reward_clip is not None:\n",
    "        return max(-cfg.reward_clip, min(cfg.reward_clip, r))\n",
    "    return r\n",
    "\n",
    "# Quick test\n",
    "cfg.reward_clip = None  # Set to 1.0 to try clipping later\n",
    "print('Reward test (no clip):', process_reward(5.0, cfg))\n",
    "cfg.reward_clip = 1.0\n",
    "print('Reward test (clip=1.0):', process_reward(5.0, cfg))\n",
    "cfg.reward_clip = None  # Reset for training; modify if desired"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "f37526dcaa1b4a52ba6637a2c7f4e2d0",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "## 1.10 Evaluation Utilities\n",
    "Evaluation runs with greedy policy (`epsilon=0`).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "cell_id": "c4a094e26d21456ab1b0708cb51129d3",
    "deepnote_cell_type": "code"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval utility ready.\n"
     ]
    }
   ],
   "source": [
    "def evaluate(env_fn, q_net, cfg: Config, episodes: int):\n",
    "    env = env_fn\n",
    "    returns = []\n",
    "    for _ in range(episodes):\n",
    "        obs, _ = env.reset()\n",
    "        done = False\n",
    "        ep_ret = 0\n",
    "        steps = 0\n",
    "        while not done and steps < cfg.max_episode_len:\n",
    "            a = select_action(q_net, obs, 0.0, env)\n",
    "            obs2, r, terminated, truncated, _ = env.step(a)\n",
    "            done = terminated or truncated\n",
    "            ep_ret += r\n",
    "            obs = obs2\n",
    "            steps += 1\n",
    "        returns.append(ep_ret)\n",
    "    return float(np.mean(returns))\n",
    "\n",
    "def hard_update(target, source):\n",
    "    target.load_state_dict(source.state_dict())\n",
    "\n",
    "print('Eval utility ready.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "6ea520ee581342eab0d7adbb668e3622",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "## 1.11 Training Loop\n",
    "We iterate over the environments in.\n",
    "Logging per stage:\n",
    "- `episode_rewards`\n",
    "- `losses`\n",
    "- `eps_history`\n",
    "- `eval` (periodic greedy evaluation)\n",
    "\n",
    "Increase `max_steps_per_env` for stronger performance. For quick debugging, you may temporarily reduce it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "cell_id": "738a5ba76c9e426d86c6d76fb67b459d",
    "deepnote_cell_type": "code",
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Stage 1: squared ===\n",
      "Obs space: 121\n",
      "Obs space: 121\n",
      "Starting training...\n",
      "[squared] Ep 100 Steps 1563 RecentMean 0.16 Eps 0.926\n",
      "[squared] Ep 200 Steps 2813 RecentMean 0.32 Eps 0.866\n",
      "Eval @ Ep 200 and 2813 steps => mean return 1.00\n",
      "[squared] Ep 300 Steps 4202 RecentMean 0.18 Eps 0.800\n",
      "[squared] Ep 400 Steps 5316 RecentMean 0.24 Eps 0.748\n",
      "Eval @ Ep 400 and 5316 steps => mean return 1.00\n",
      "[squared] Ep 500 Steps 6319 RecentMean 0.46 Eps 0.700\n",
      "[squared] Ep 600 Steps 7115 RecentMean 0.62 Eps 0.662\n",
      "Eval @ Ep 600 and 7115 steps => mean return 1.00\n",
      "[squared] Ep 700 Steps 7456 RecentMean 0.90 Eps 0.646\n",
      "[squared] Ep 800 Steps 8017 RecentMean 0.90 Eps 0.619\n",
      "Eval @ Ep 800 and 8017 steps => mean return -1.00\n",
      "[squared] Ep 900 Steps 9088 RecentMean 0.88 Eps 0.568\n",
      "[squared] Ep 1000 Steps 10887 RecentMean 0.26 Eps 0.483\n",
      "Eval @ Ep 1000 and 10887 steps => mean return -1.00\n",
      "[squared] Ep 1100 Steps 13335 RecentMean -0.14 Eps 0.367\n",
      "[squared] Ep 1200 Steps 16066 RecentMean -0.46 Eps 0.237\n",
      "Eval @ Ep 1200 and 16066 steps => mean return -1.00\n",
      "[squared] Ep 1300 Steps 19026 RecentMean -0.68 Eps 0.096\n",
      "[squared] Ep 1400 Steps 22314 RecentMean -0.92 Eps 0.050\n",
      "Eval @ Ep 1400 and 22314 steps => mean return -1.00\n",
      "[squared] Ep 1500 Steps 25673 RecentMean -0.96 Eps 0.050\n",
      "[squared] Ep 1600 Steps 28896 RecentMean -0.90 Eps 0.050\n",
      "Eval @ Ep 1600 and 28896 steps => mean return -1.00\n",
      "[squared] Ep 1700 Steps 32208 RecentMean -0.94 Eps 0.050\n",
      "[squared] Ep 1800 Steps 35494 RecentMean -0.92 Eps 0.050\n",
      "Eval @ Ep 1800 and 35494 steps => mean return -1.00\n",
      "[squared] Ep 1900 Steps 38863 RecentMean -0.98 Eps 0.050\n",
      "[squared] Ep 2000 Steps 42069 RecentMean -0.88 Eps 0.050\n",
      "Eval @ Ep 2000 and 42069 steps => mean return -1.00\n",
      "[squared] Ep 2100 Steps 45186 RecentMean -0.76 Eps 0.050\n",
      "[squared] Ep 2200 Steps 46123 RecentMean 0.54 Eps 0.050\n",
      "Eval @ Ep 2200 and 46123 steps => mean return 1.00\n",
      "[squared] Ep 2300 Steps 46390 RecentMean 0.90 Eps 0.050\n",
      "[squared] Ep 2400 Steps 46638 RecentMean 0.92 Eps 0.050\n",
      "Eval @ Ep 2400 and 46638 steps => mean return 1.00\n",
      "[squared] Ep 2500 Steps 46771 RecentMean 0.98 Eps 0.050\n",
      "[squared] Ep 2600 Steps 46940 RecentMean 0.96 Eps 0.050\n",
      "Eval @ Ep 2600 and 46940 steps => mean return 1.00\n",
      "[squared] Ep 2700 Steps 47079 RecentMean 1.00 Eps 0.050\n",
      "[squared] Ep 2800 Steps 47223 RecentMean 0.98 Eps 0.050\n",
      "Eval @ Ep 2800 and 47223 steps => mean return 1.00\n",
      "[squared] Ep 2900 Steps 47363 RecentMean 1.00 Eps 0.050\n",
      "[squared] Ep 3000 Steps 47508 RecentMean 1.00 Eps 0.050\n",
      "Eval @ Ep 3000 and 47508 steps => mean return 1.00\n",
      "[squared] Ep 3100 Steps 47616 RecentMean 1.00 Eps 0.050\n",
      "[squared] Ep 3200 Steps 47727 RecentMean 1.00 Eps 0.050\n",
      "Eval @ Ep 3200 and 47727 steps => mean return 1.00\n",
      "[squared] Ep 3300 Steps 47829 RecentMean 1.00 Eps 0.050\n",
      "[squared] Ep 3400 Steps 47963 RecentMean 1.00 Eps 0.050\n",
      "Eval @ Ep 3400 and 47963 steps => mean return 1.00\n",
      "[squared] Ep 3500 Steps 48072 RecentMean 1.00 Eps 0.050\n",
      "[squared] Ep 3600 Steps 48202 RecentMean 1.00 Eps 0.050\n",
      "Eval @ Ep 3600 and 48202 steps => mean return 1.00\n",
      "[squared] Ep 3700 Steps 48304 RecentMean 1.00 Eps 0.050\n",
      "[squared] Ep 3800 Steps 48410 RecentMean 1.00 Eps 0.050\n",
      "Eval @ Ep 3800 and 48410 steps => mean return 1.00\n",
      "[squared] Ep 3900 Steps 48542 RecentMean 1.00 Eps 0.050\n",
      "[squared] Ep 4000 Steps 48704 RecentMean 0.98 Eps 0.050\n",
      "Eval @ Ep 4000 and 48704 steps => mean return 1.00\n",
      "[squared] Ep 4100 Steps 48953 RecentMean 0.94 Eps 0.050\n",
      "[squared] Ep 4200 Steps 49270 RecentMean 0.88 Eps 0.050\n",
      "Eval @ Ep 4200 and 49270 steps => mean return 1.00\n",
      "[squared] Ep 4300 Steps 49384 RecentMean 1.00 Eps 0.050\n",
      "[squared] Ep 4400 Steps 49490 RecentMean 1.00 Eps 0.050\n",
      "Eval @ Ep 4400 and 49490 steps => mean return 1.00\n",
      "[squared] Ep 4500 Steps 49600 RecentMean 1.00 Eps 0.050\n",
      "[squared] Ep 4600 Steps 49710 RecentMean 1.00 Eps 0.050\n",
      "Eval @ Ep 4600 and 49710 steps => mean return 1.00\n",
      "[squared] Ep 4700 Steps 49827 RecentMean 1.00 Eps 0.050\n",
      "[squared] Ep 4800 Steps 49944 RecentMean 1.00 Eps 0.050\n",
      "Eval @ Ep 4800 and 49944 steps => mean return 1.00\n",
      "[squared] Ep 4900 Steps 50051 RecentMean 1.00 Eps 0.050\n",
      "[squared] Ep 5000 Steps 50162 RecentMean 1.00 Eps 0.050\n",
      "Eval @ Ep 5000 and 50162 steps => mean return 1.00\n",
      "[squared] Ep 5100 Steps 50272 RecentMean 1.00 Eps 0.050\n",
      "[squared] Ep 5200 Steps 50382 RecentMean 1.00 Eps 0.050\n",
      "Eval @ Ep 5200 and 50382 steps => mean return 1.00\n",
      "[squared] Ep 5300 Steps 50491 RecentMean 1.00 Eps 0.050\n",
      "[squared] Ep 5400 Steps 50593 RecentMean 1.00 Eps 0.050\n",
      "Eval @ Ep 5400 and 50593 steps => mean return 1.00\n",
      "[squared] Ep 5500 Steps 50706 RecentMean 1.00 Eps 0.050\n",
      "[squared] Ep 5600 Steps 50808 RecentMean 1.00 Eps 0.050\n",
      "Eval @ Ep 5600 and 50808 steps => mean return 1.00\n",
      "[squared] Ep 5700 Steps 50917 RecentMean 1.00 Eps 0.050\n",
      "[squared] Ep 5800 Steps 51026 RecentMean 1.00 Eps 0.050\n",
      "Eval @ Ep 5800 and 51026 steps => mean return 1.00\n",
      "[squared] Ep 5900 Steps 51136 RecentMean 1.00 Eps 0.050\n",
      "[squared] Ep 6000 Steps 51238 RecentMean 1.00 Eps 0.050\n",
      "Eval @ Ep 6000 and 51238 steps => mean return 1.00\n",
      "[squared] Ep 6100 Steps 51376 RecentMean 1.00 Eps 0.050\n",
      "[squared] Ep 6200 Steps 51488 RecentMean 1.00 Eps 0.050\n",
      "Eval @ Ep 6200 and 51488 steps => mean return 1.00\n",
      "[squared] Ep 6300 Steps 51591 RecentMean 1.00 Eps 0.050\n",
      "[squared] Ep 6400 Steps 51769 RecentMean 0.98 Eps 0.050\n",
      "Eval @ Ep 6400 and 51769 steps => mean return 1.00\n",
      "[squared] Ep 6500 Steps 51879 RecentMean 1.00 Eps 0.050\n",
      "[squared] Ep 6600 Steps 52016 RecentMean 0.98 Eps 0.050\n",
      "Eval @ Ep 6600 and 52016 steps => mean return 1.00\n",
      "[squared] Ep 6700 Steps 52120 RecentMean 1.00 Eps 0.050\n",
      "[squared] Ep 6800 Steps 52229 RecentMean 1.00 Eps 0.050\n",
      "Eval @ Ep 6800 and 52229 steps => mean return 1.00\n",
      "[squared] Ep 6900 Steps 52365 RecentMean 0.98 Eps 0.050\n",
      "[squared] Ep 7000 Steps 52470 RecentMean 1.00 Eps 0.050\n",
      "Eval @ Ep 7000 and 52470 steps => mean return 1.00\n",
      "[squared] Ep 7100 Steps 52644 RecentMean 0.96 Eps 0.050\n",
      "[squared] Ep 7200 Steps 52753 RecentMean 1.00 Eps 0.050\n",
      "Eval @ Ep 7200 and 52753 steps => mean return 1.00\n",
      "[squared] Ep 7300 Steps 52877 RecentMean 1.00 Eps 0.050\n",
      "[squared] Ep 7400 Steps 52979 RecentMean 1.00 Eps 0.050\n",
      "Eval @ Ep 7400 and 52979 steps => mean return 1.00\n",
      "[squared] Ep 7500 Steps 53082 RecentMean 1.00 Eps 0.050\n",
      "[squared] Ep 7600 Steps 53253 RecentMean 1.00 Eps 0.050\n",
      "Eval @ Ep 7600 and 53253 steps => mean return 1.00\n",
      "[squared] Ep 7700 Steps 53478 RecentMean 0.96 Eps 0.050\n",
      "[squared] Ep 7800 Steps 53648 RecentMean 0.98 Eps 0.050\n",
      "Eval @ Ep 7800 and 53648 steps => mean return 1.00\n",
      "[squared] Ep 7900 Steps 53756 RecentMean 1.00 Eps 0.050\n",
      "[squared] Ep 8000 Steps 53900 RecentMean 1.00 Eps 0.050\n",
      "Eval @ Ep 8000 and 53900 steps => mean return 1.00\n",
      "[squared] Ep 8100 Steps 54005 RecentMean 1.00 Eps 0.050\n",
      "[squared] Ep 8200 Steps 54114 RecentMean 1.00 Eps 0.050\n",
      "Eval @ Ep 8200 and 54114 steps => mean return 1.00\n",
      "[squared] Ep 8300 Steps 54253 RecentMean 0.98 Eps 0.050\n",
      "[squared] Ep 8400 Steps 54359 RecentMean 1.00 Eps 0.050\n",
      "Eval @ Ep 8400 and 54359 steps => mean return 1.00\n",
      "[squared] Ep 8500 Steps 54497 RecentMean 1.00 Eps 0.050\n",
      "[squared] Ep 8600 Steps 54617 RecentMean 1.00 Eps 0.050\n",
      "Eval @ Ep 8600 and 54617 steps => mean return 1.00\n",
      "[squared] Ep 8700 Steps 54796 RecentMean 1.00 Eps 0.050\n",
      "[squared] Ep 8800 Steps 54918 RecentMean 1.00 Eps 0.050\n",
      "Eval @ Ep 8800 and 54918 steps => mean return 1.00\n",
      "[squared] Ep 8900 Steps 55120 RecentMean 0.98 Eps 0.050\n",
      "[squared] Ep 9000 Steps 55462 RecentMean 0.92 Eps 0.050\n",
      "Eval @ Ep 9000 and 55462 steps => mean return 1.00\n",
      "[squared] Ep 9100 Steps 55598 RecentMean 0.98 Eps 0.050\n",
      "[squared] Ep 9200 Steps 55893 RecentMean 0.96 Eps 0.050\n",
      "Eval @ Ep 9200 and 55893 steps => mean return 1.00\n",
      "[squared] Ep 9300 Steps 56010 RecentMean 1.00 Eps 0.050\n",
      "[squared] Ep 9400 Steps 56268 RecentMean 0.92 Eps 0.050\n",
      "Eval @ Ep 9400 and 56268 steps => mean return 1.00\n",
      "[squared] Ep 9500 Steps 56444 RecentMean 0.98 Eps 0.050\n",
      "[squared] Ep 9600 Steps 56606 RecentMean 0.98 Eps 0.050\n",
      "Eval @ Ep 9600 and 56606 steps => mean return 1.00\n",
      "[squared] Ep 9700 Steps 56837 RecentMean 0.96 Eps 0.050\n",
      "[squared] Ep 9800 Steps 57041 RecentMean 0.94 Eps 0.050\n",
      "Eval @ Ep 9800 and 57041 steps => mean return 1.00\n",
      "[squared] Ep 9900 Steps 57178 RecentMean 0.98 Eps 0.050\n",
      "[squared] Ep 10000 Steps 57314 RecentMean 0.98 Eps 0.050\n",
      "Eval @ Ep 10000 and 57314 steps => mean return 1.00\n",
      "[squared] Ep 10100 Steps 57466 RecentMean 0.98 Eps 0.050\n",
      "[squared] Ep 10200 Steps 57574 RecentMean 1.00 Eps 0.050\n",
      "Eval @ Ep 10200 and 57574 steps => mean return 1.00\n",
      "[squared] Ep 10300 Steps 57685 RecentMean 1.00 Eps 0.050\n",
      "[squared] Ep 10400 Steps 57791 RecentMean 1.00 Eps 0.050\n",
      "Eval @ Ep 10400 and 57791 steps => mean return 1.00\n",
      "[squared] Ep 10500 Steps 57946 RecentMean 1.00 Eps 0.050\n",
      "[squared] Ep 10600 Steps 58104 RecentMean 0.98 Eps 0.050\n",
      "Eval @ Ep 10600 and 58104 steps => mean return 1.00\n",
      "[squared] Ep 10700 Steps 58209 RecentMean 1.00 Eps 0.050\n",
      "[squared] Ep 10800 Steps 58350 RecentMean 0.98 Eps 0.050\n",
      "Eval @ Ep 10800 and 58350 steps => mean return 1.00\n",
      "[squared] Ep 10900 Steps 58452 RecentMean 1.00 Eps 0.050\n",
      "[squared] Ep 11000 Steps 58605 RecentMean 0.98 Eps 0.050\n",
      "Eval @ Ep 11000 and 58605 steps => mean return 1.00\n",
      "[squared] Ep 11100 Steps 58714 RecentMean 1.00 Eps 0.050\n",
      "[squared] Ep 11200 Steps 58816 RecentMean 1.00 Eps 0.050\n",
      "Eval @ Ep 11200 and 58816 steps => mean return 1.00\n",
      "[squared] Ep 11300 Steps 58933 RecentMean 1.00 Eps 0.050\n",
      "[squared] Ep 11400 Steps 59038 RecentMean 1.00 Eps 0.050\n",
      "Eval @ Ep 11400 and 59038 steps => mean return 1.00\n",
      "[squared] Ep 11500 Steps 59149 RecentMean 1.00 Eps 0.050\n",
      "[squared] Ep 11600 Steps 59253 RecentMean 1.00 Eps 0.050\n",
      "Eval @ Ep 11600 and 59253 steps => mean return 1.00\n",
      "[squared] Ep 11700 Steps 59360 RecentMean 1.00 Eps 0.050\n",
      "[squared] Ep 11800 Steps 59462 RecentMean 1.00 Eps 0.050\n",
      "Eval @ Ep 11800 and 59462 steps => mean return 1.00\n",
      "[squared] Ep 11900 Steps 59564 RecentMean 1.00 Eps 0.050\n",
      "[squared] Ep 12000 Steps 59706 RecentMean 0.98 Eps 0.050\n",
      "Eval @ Ep 12000 and 59706 steps => mean return 1.00\n",
      "[squared] Ep 12100 Steps 59818 RecentMean 1.00 Eps 0.050\n",
      "[squared] Ep 12200 Steps 59957 RecentMean 1.00 Eps 0.050\n",
      "Eval @ Ep 12200 and 59957 steps => mean return 1.00\n",
      "[squared] Ep 12300 Steps 60121 RecentMean 1.00 Eps 0.050\n",
      "[squared] Ep 12400 Steps 60272 RecentMean 0.98 Eps 0.050\n",
      "Eval @ Ep 12400 and 60272 steps => mean return 1.00\n",
      "[squared] Ep 12500 Steps 60439 RecentMean 0.98 Eps 0.050\n",
      "[squared] Ep 12600 Steps 60627 RecentMean 0.96 Eps 0.050\n",
      "Eval @ Ep 12600 and 60627 steps => mean return 1.00\n",
      "[squared] Ep 12700 Steps 60837 RecentMean 0.98 Eps 0.050\n",
      "[squared] Ep 12800 Steps 60950 RecentMean 1.00 Eps 0.050\n",
      "Eval @ Ep 12800 and 60950 steps => mean return 1.00\n",
      "[squared] Ep 12900 Steps 61098 RecentMean 0.98 Eps 0.050\n",
      "[squared] Ep 13000 Steps 61208 RecentMean 1.00 Eps 0.050\n",
      "Eval @ Ep 13000 and 61208 steps => mean return 1.00\n",
      "[squared] Ep 13100 Steps 61312 RecentMean 1.00 Eps 0.050\n",
      "[squared] Ep 13200 Steps 61425 RecentMean 1.00 Eps 0.050\n",
      "Eval @ Ep 13200 and 61425 steps => mean return 1.00\n",
      "[squared] Ep 13300 Steps 61533 RecentMean 1.00 Eps 0.050\n",
      "[squared] Ep 13400 Steps 61684 RecentMean 1.00 Eps 0.050\n",
      "Eval @ Ep 13400 and 61684 steps => mean return 1.00\n",
      "[squared] Ep 13500 Steps 61816 RecentMean 1.00 Eps 0.050\n",
      "[squared] Ep 13600 Steps 61972 RecentMean 1.00 Eps 0.050\n",
      "Eval @ Ep 13600 and 61972 steps => mean return 1.00\n",
      "[squared] Ep 13700 Steps 62089 RecentMean 1.00 Eps 0.050\n",
      "[squared] Ep 13800 Steps 62433 RecentMean 0.96 Eps 0.050\n",
      "Eval @ Ep 13800 and 62433 steps => mean return 1.00\n",
      "[squared] Ep 13900 Steps 62783 RecentMean 0.88 Eps 0.050\n",
      "[squared] Ep 14000 Steps 63443 RecentMean 0.76 Eps 0.050\n",
      "Eval @ Ep 14000 and 63443 steps => mean return 1.00\n",
      "[squared] Ep 14100 Steps 63828 RecentMean 0.86 Eps 0.050\n",
      "[squared] Ep 14200 Steps 64272 RecentMean 0.82 Eps 0.050\n",
      "Eval @ Ep 14200 and 64272 steps => mean return 1.00\n",
      "[squared] Ep 14300 Steps 64534 RecentMean 0.94 Eps 0.050\n",
      "[squared] Ep 14400 Steps 64723 RecentMean 0.98 Eps 0.050\n",
      "Eval @ Ep 14400 and 64723 steps => mean return 1.00\n",
      "[squared] Ep 14500 Steps 64878 RecentMean 1.00 Eps 0.050\n",
      "[squared] Ep 14600 Steps 64984 RecentMean 1.00 Eps 0.050\n",
      "Eval @ Ep 14600 and 64984 steps => mean return 1.00\n",
      "[squared] Ep 14700 Steps 65090 RecentMean 1.00 Eps 0.050\n",
      "[squared] Ep 14800 Steps 65194 RecentMean 1.00 Eps 0.050\n",
      "Eval @ Ep 14800 and 65194 steps => mean return 1.00\n",
      "[squared] Ep 14900 Steps 65330 RecentMean 1.00 Eps 0.050\n",
      "[squared] Ep 15000 Steps 65465 RecentMean 1.00 Eps 0.050\n",
      "Eval @ Ep 15000 and 65465 steps => mean return 1.00\n",
      "[squared] Ep 15100 Steps 65570 RecentMean 1.00 Eps 0.050\n",
      "[squared] Ep 15200 Steps 65687 RecentMean 1.00 Eps 0.050\n",
      "Eval @ Ep 15200 and 65687 steps => mean return 1.00\n",
      "[squared] Ep 15300 Steps 65791 RecentMean 1.00 Eps 0.050\n",
      "[squared] Ep 15400 Steps 65901 RecentMean 1.00 Eps 0.050\n",
      "Eval @ Ep 15400 and 65901 steps => mean return 1.00\n",
      "[squared] Ep 15500 Steps 66011 RecentMean 1.00 Eps 0.050\n",
      "[squared] Ep 15600 Steps 66115 RecentMean 1.00 Eps 0.050\n",
      "Eval @ Ep 15600 and 66115 steps => mean return 1.00\n",
      "[squared] Ep 15700 Steps 66225 RecentMean 1.00 Eps 0.050\n",
      "[squared] Ep 15800 Steps 66347 RecentMean 1.00 Eps 0.050\n",
      "Eval @ Ep 15800 and 66347 steps => mean return 1.00\n",
      "[squared] Ep 15900 Steps 66451 RecentMean 1.00 Eps 0.050\n",
      "[squared] Ep 16000 Steps 66555 RecentMean 1.00 Eps 0.050\n",
      "Eval @ Ep 16000 and 66555 steps => mean return 1.00\n",
      "[squared] Ep 16100 Steps 66662 RecentMean 1.00 Eps 0.050\n",
      "[squared] Ep 16200 Steps 66850 RecentMean 0.96 Eps 0.050\n",
      "Eval @ Ep 16200 and 66850 steps => mean return 1.00\n",
      "[squared] Ep 16300 Steps 66955 RecentMean 1.00 Eps 0.050\n",
      "[squared] Ep 16400 Steps 67093 RecentMean 1.00 Eps 0.050\n",
      "Eval @ Ep 16400 and 67093 steps => mean return 1.00\n",
      "[squared] Ep 16500 Steps 67271 RecentMean 0.96 Eps 0.050\n",
      "[squared] Ep 16600 Steps 67379 RecentMean 1.00 Eps 0.050\n",
      "Eval @ Ep 16600 and 67379 steps => mean return 1.00\n",
      "[squared] Ep 16700 Steps 67492 RecentMean 1.00 Eps 0.050\n",
      "[squared] Ep 16800 Steps 67595 RecentMean 1.00 Eps 0.050\n",
      "Eval @ Ep 16800 and 67595 steps => mean return 1.00\n",
      "[squared] Ep 16900 Steps 67805 RecentMean 0.94 Eps 0.050\n",
      "[squared] Ep 17000 Steps 67913 RecentMean 1.00 Eps 0.050\n",
      "Eval @ Ep 17000 and 67913 steps => mean return 1.00\n",
      "[squared] Ep 17100 Steps 68046 RecentMean 1.00 Eps 0.050\n",
      "[squared] Ep 17200 Steps 68300 RecentMean 0.94 Eps 0.050\n",
      "Eval @ Ep 17200 and 68300 steps => mean return 1.00\n",
      "[squared] Ep 17300 Steps 68455 RecentMean 0.98 Eps 0.050\n",
      "[squared] Ep 17400 Steps 68624 RecentMean 0.98 Eps 0.050\n",
      "Eval @ Ep 17400 and 68624 steps => mean return 1.00\n",
      "[squared] Ep 17500 Steps 68752 RecentMean 1.00 Eps 0.050\n",
      "[squared] Ep 17600 Steps 68909 RecentMean 1.00 Eps 0.050\n",
      "Eval @ Ep 17600 and 68909 steps => mean return 1.00\n",
      "[squared] Ep 17700 Steps 69162 RecentMean 0.96 Eps 0.050\n",
      "[squared] Ep 17800 Steps 69545 RecentMean 0.94 Eps 0.050\n",
      "Eval @ Ep 17800 and 69545 steps => mean return 1.00\n",
      "[squared] Ep 17900 Steps 69733 RecentMean 1.00 Eps 0.050\n",
      "[squared] Ep 18000 Steps 69893 RecentMean 1.00 Eps 0.050\n",
      "Eval @ Ep 18000 and 69893 steps => mean return -1.00\n",
      "[squared] Ep 18100 Steps 70024 RecentMean 1.00 Eps 0.050\n",
      "[squared] Ep 18200 Steps 70184 RecentMean 0.98 Eps 0.050\n",
      "Eval @ Ep 18200 and 70184 steps => mean return 1.00\n",
      "[squared] Ep 18300 Steps 70288 RecentMean 1.00 Eps 0.050\n",
      "[squared] Ep 18400 Steps 70433 RecentMean 1.00 Eps 0.050\n",
      "Eval @ Ep 18400 and 70433 steps => mean return 1.00\n",
      "[squared] Ep 18500 Steps 70603 RecentMean 0.96 Eps 0.050\n",
      "[squared] Ep 18600 Steps 70712 RecentMean 1.00 Eps 0.050\n",
      "Eval @ Ep 18600 and 70712 steps => mean return 1.00\n",
      "[squared] Ep 18700 Steps 70835 RecentMean 1.00 Eps 0.050\n",
      "[squared] Ep 18800 Steps 70942 RecentMean 1.00 Eps 0.050\n",
      "Eval @ Ep 18800 and 70942 steps => mean return 1.00\n",
      "[squared] Ep 18900 Steps 71056 RecentMean 1.00 Eps 0.050\n",
      "[squared] Ep 19000 Steps 71156 RecentMean 1.00 Eps 0.050\n",
      "Eval @ Ep 19000 and 71156 steps => mean return 1.00\n",
      "[squared] Ep 19100 Steps 71269 RecentMean 1.00 Eps 0.050\n",
      "[squared] Ep 19200 Steps 71379 RecentMean 1.00 Eps 0.050\n",
      "Eval @ Ep 19200 and 71379 steps => mean return 1.00\n",
      "[squared] Ep 19300 Steps 71497 RecentMean 1.00 Eps 0.050\n",
      "[squared] Ep 19400 Steps 71602 RecentMean 1.00 Eps 0.050\n",
      "Eval @ Ep 19400 and 71602 steps => mean return 1.00\n",
      "[squared] Ep 19500 Steps 71711 RecentMean 1.00 Eps 0.050\n",
      "[squared] Ep 19600 Steps 71811 RecentMean 1.00 Eps 0.050\n",
      "Eval @ Ep 19600 and 71811 steps => mean return 1.00\n",
      "[squared] Ep 19700 Steps 71925 RecentMean 1.00 Eps 0.050\n",
      "[squared] Ep 19800 Steps 72031 RecentMean 1.00 Eps 0.050\n",
      "Eval @ Ep 19800 and 72031 steps => mean return 1.00\n",
      "[squared] Ep 19900 Steps 72162 RecentMean 1.00 Eps 0.050\n",
      "[squared] Ep 20000 Steps 72467 RecentMean 0.92 Eps 0.050\n",
      "Eval @ Ep 20000 and 72467 steps => mean return 1.00\n",
      "[squared] Ep 20100 Steps 72575 RecentMean 1.00 Eps 0.050\n",
      "[squared] Ep 20200 Steps 72743 RecentMean 0.98 Eps 0.050\n",
      "Eval @ Ep 20200 and 72743 steps => mean return 1.00\n",
      "[squared] Ep 20300 Steps 72972 RecentMean 0.96 Eps 0.050\n",
      "[squared] Ep 20400 Steps 73080 RecentMean 1.00 Eps 0.050\n",
      "Eval @ Ep 20400 and 73080 steps => mean return 1.00\n",
      "[squared] Ep 20500 Steps 73273 RecentMean 0.96 Eps 0.050\n",
      "[squared] Ep 20600 Steps 73533 RecentMean 0.98 Eps 0.050\n",
      "Eval @ Ep 20600 and 73533 steps => mean return 1.00\n",
      "[squared] Ep 20700 Steps 73687 RecentMean 1.00 Eps 0.050\n",
      "[squared] Ep 20800 Steps 73789 RecentMean 1.00 Eps 0.050\n",
      "Eval @ Ep 20800 and 73789 steps => mean return 1.00\n",
      "[squared] Ep 20900 Steps 73964 RecentMean 0.96 Eps 0.050\n",
      "[squared] Ep 21000 Steps 74107 RecentMean 0.98 Eps 0.050\n",
      "Eval @ Ep 21000 and 74107 steps => mean return 1.00\n",
      "[squared] Ep 21100 Steps 74219 RecentMean 1.00 Eps 0.050\n",
      "[squared] Ep 21200 Steps 74341 RecentMean 1.00 Eps 0.050\n",
      "Eval @ Ep 21200 and 74341 steps => mean return 1.00\n",
      "[squared] Ep 21300 Steps 74449 RecentMean 1.00 Eps 0.050\n",
      "[squared] Ep 21400 Steps 74590 RecentMean 0.98 Eps 0.050\n",
      "Eval @ Ep 21400 and 74590 steps => mean return 1.00\n",
      "[squared] Ep 21500 Steps 74726 RecentMean 1.00 Eps 0.050\n",
      "[squared] Ep 21600 Steps 74862 RecentMean 0.98 Eps 0.050\n",
      "Eval @ Ep 21600 and 74862 steps => mean return 1.00\n",
      "[squared] Ep 21700 Steps 74969 RecentMean 1.00 Eps 0.050\n",
      "[squared] Ep 21800 Steps 75073 RecentMean 1.00 Eps 0.050\n",
      "Eval @ Ep 21800 and 75073 steps => mean return 1.00\n",
      "[squared] Ep 21900 Steps 75183 RecentMean 1.00 Eps 0.050\n",
      "[squared] Ep 22000 Steps 75289 RecentMean 1.00 Eps 0.050\n",
      "Eval @ Ep 22000 and 75289 steps => mean return 1.00\n",
      "[squared] Ep 22100 Steps 75391 RecentMean 1.00 Eps 0.050\n",
      "[squared] Ep 22200 Steps 75496 RecentMean 1.00 Eps 0.050\n",
      "Eval @ Ep 22200 and 75496 steps => mean return 1.00\n",
      "[squared] Ep 22300 Steps 75600 RecentMean 1.00 Eps 0.050\n",
      "[squared] Ep 22400 Steps 75706 RecentMean 1.00 Eps 0.050\n",
      "Eval @ Ep 22400 and 75706 steps => mean return 1.00\n",
      "[squared] Ep 22500 Steps 75875 RecentMean 0.98 Eps 0.050\n",
      "[squared] Ep 22600 Steps 76014 RecentMean 1.00 Eps 0.050\n",
      "Eval @ Ep 22600 and 76014 steps => mean return 1.00\n",
      "[squared] Ep 22700 Steps 76123 RecentMean 1.00 Eps 0.050\n",
      "[squared] Ep 22800 Steps 76252 RecentMean 1.00 Eps 0.050\n",
      "Eval @ Ep 22800 and 76252 steps => mean return 1.00\n",
      "[squared] Ep 22900 Steps 76360 RecentMean 1.00 Eps 0.050\n",
      "[squared] Ep 23000 Steps 76466 RecentMean 1.00 Eps 0.050\n",
      "Eval @ Ep 23000 and 76466 steps => mean return 1.00\n",
      "[squared] Ep 23100 Steps 76569 RecentMean 1.00 Eps 0.050\n",
      "[squared] Ep 23200 Steps 76678 RecentMean 1.00 Eps 0.050\n",
      "Eval @ Ep 23200 and 76678 steps => mean return 1.00\n",
      "[squared] Ep 23300 Steps 76781 RecentMean 1.00 Eps 0.050\n",
      "[squared] Ep 23400 Steps 76887 RecentMean 1.00 Eps 0.050\n",
      "Eval @ Ep 23400 and 76887 steps => mean return 1.00\n",
      "[squared] Ep 23500 Steps 76990 RecentMean 1.00 Eps 0.050\n",
      "[squared] Ep 23600 Steps 77111 RecentMean 1.00 Eps 0.050\n",
      "Eval @ Ep 23600 and 77111 steps => mean return 1.00\n",
      "[squared] Ep 23700 Steps 77222 RecentMean 1.00 Eps 0.050\n",
      "[squared] Ep 23800 Steps 77331 RecentMean 1.00 Eps 0.050\n",
      "Eval @ Ep 23800 and 77331 steps => mean return 1.00\n",
      "[squared] Ep 23900 Steps 77433 RecentMean 1.00 Eps 0.050\n",
      "[squared] Ep 24000 Steps 77549 RecentMean 1.00 Eps 0.050\n",
      "Eval @ Ep 24000 and 77549 steps => mean return 1.00\n",
      "[squared] Ep 24100 Steps 77656 RecentMean 1.00 Eps 0.050\n",
      "[squared] Ep 24200 Steps 77881 RecentMean 0.96 Eps 0.050\n",
      "Eval @ Ep 24200 and 77881 steps => mean return 1.00\n",
      "[squared] Ep 24300 Steps 78091 RecentMean 0.96 Eps 0.050\n",
      "[squared] Ep 24400 Steps 78254 RecentMean 0.98 Eps 0.050\n",
      "Eval @ Ep 24400 and 78254 steps => mean return 1.00\n",
      "[squared] Ep 24500 Steps 78410 RecentMean 0.98 Eps 0.050\n",
      "[squared] Ep 24600 Steps 78552 RecentMean 1.00 Eps 0.050\n",
      "Eval @ Ep 24600 and 78552 steps => mean return 1.00\n",
      "[squared] Ep 24700 Steps 78744 RecentMean 0.96 Eps 0.050\n",
      "[squared] Ep 24800 Steps 78952 RecentMean 0.94 Eps 0.050\n",
      "Eval @ Ep 24800 and 78952 steps => mean return 1.00\n",
      "[squared] Ep 24900 Steps 79097 RecentMean 1.00 Eps 0.050\n",
      "[squared] Ep 25000 Steps 79236 RecentMean 0.98 Eps 0.050\n",
      "Eval @ Ep 25000 and 79236 steps => mean return 1.00\n",
      "[squared] Ep 25100 Steps 79430 RecentMean 1.00 Eps 0.050\n",
      "[squared] Ep 25200 Steps 79629 RecentMean 0.98 Eps 0.050\n",
      "Eval @ Ep 25200 and 79629 steps => mean return 1.00\n",
      "[squared] Ep 25300 Steps 79740 RecentMean 1.00 Eps 0.050\n",
      "[squared] Ep 25400 Steps 79840 RecentMean 1.00 Eps 0.050\n",
      "Eval @ Ep 25400 and 79840 steps => mean return 1.00\n",
      "[squared] Ep 25500 Steps 79970 RecentMean 1.00 Eps 0.050\n",
      "[squared] Ep 25600 Steps 80119 RecentMean 0.98 Eps 0.050\n",
      "Eval @ Ep 25600 and 80119 steps => mean return 1.00\n",
      "[squared] Ep 25700 Steps 80262 RecentMean 0.98 Eps 0.050\n",
      "[squared] Ep 25800 Steps 80405 RecentMean 0.98 Eps 0.050\n",
      "Eval @ Ep 25800 and 80405 steps => mean return 1.00\n",
      "[squared] Ep 25900 Steps 80514 RecentMean 1.00 Eps 0.050\n",
      "[squared] Ep 26000 Steps 80656 RecentMean 1.00 Eps 0.050\n",
      "Eval @ Ep 26000 and 80656 steps => mean return 1.00\n",
      "[squared] Ep 26100 Steps 80762 RecentMean 1.00 Eps 0.050\n",
      "[squared] Ep 26200 Steps 80929 RecentMean 1.00 Eps 0.050\n",
      "Eval @ Ep 26200 and 80929 steps => mean return 1.00\n",
      "[squared] Ep 26300 Steps 81066 RecentMean 0.98 Eps 0.050\n",
      "[squared] Ep 26400 Steps 81205 RecentMean 1.00 Eps 0.050\n",
      "Eval @ Ep 26400 and 81205 steps => mean return 1.00\n",
      "[squared] Ep 26500 Steps 81311 RecentMean 1.00 Eps 0.050\n",
      "[squared] Ep 26600 Steps 81421 RecentMean 1.00 Eps 0.050\n",
      "Eval @ Ep 26600 and 81421 steps => mean return 1.00\n",
      "[squared] Ep 26700 Steps 81552 RecentMean 1.00 Eps 0.050\n",
      "[squared] Ep 26800 Steps 81653 RecentMean 1.00 Eps 0.050\n",
      "Eval @ Ep 26800 and 81653 steps => mean return 1.00\n",
      "[squared] Ep 26900 Steps 81767 RecentMean 1.00 Eps 0.050\n",
      "[squared] Ep 27000 Steps 81879 RecentMean 1.00 Eps 0.050\n",
      "Eval @ Ep 27000 and 81879 steps => mean return 1.00\n",
      "[squared] Ep 27100 Steps 81987 RecentMean 1.00 Eps 0.050\n",
      "[squared] Ep 27200 Steps 82095 RecentMean 1.00 Eps 0.050\n",
      "Eval @ Ep 27200 and 82095 steps => mean return 1.00\n",
      "[squared] Ep 27300 Steps 82220 RecentMean 1.00 Eps 0.050\n",
      "[squared] Ep 27400 Steps 82328 RecentMean 1.00 Eps 0.050\n",
      "Eval @ Ep 27400 and 82328 steps => mean return 1.00\n",
      "[squared] Ep 27500 Steps 82439 RecentMean 1.00 Eps 0.050\n",
      "[squared] Ep 27600 Steps 82722 RecentMean 0.96 Eps 0.050\n",
      "Eval @ Ep 27600 and 82722 steps => mean return 1.00\n",
      "[squared] Ep 27700 Steps 82907 RecentMean 0.98 Eps 0.050\n",
      "[squared] Ep 27800 Steps 83017 RecentMean 1.00 Eps 0.050\n",
      "Eval @ Ep 27800 and 83017 steps => mean return 1.00\n",
      "[squared] Ep 27900 Steps 83329 RecentMean 0.92 Eps 0.050\n",
      "[squared] Ep 28000 Steps 83490 RecentMean 0.98 Eps 0.050\n",
      "Eval @ Ep 28000 and 83490 steps => mean return 1.00\n",
      "[squared] Ep 28100 Steps 83597 RecentMean 1.00 Eps 0.050\n",
      "[squared] Ep 28200 Steps 83763 RecentMean 0.98 Eps 0.050\n",
      "Eval @ Ep 28200 and 83763 steps => mean return 1.00\n",
      "[squared] Ep 28300 Steps 83998 RecentMean 0.94 Eps 0.050\n",
      "[squared] Ep 28400 Steps 84185 RecentMean 0.98 Eps 0.050\n",
      "Eval @ Ep 28400 and 84185 steps => mean return 1.00\n",
      "[squared] Ep 28500 Steps 84289 RecentMean 1.00 Eps 0.050\n",
      "[squared] Ep 28600 Steps 84519 RecentMean 0.96 Eps 0.050\n",
      "Eval @ Ep 28600 and 84519 steps => mean return 1.00\n",
      "[squared] Ep 28700 Steps 84619 RecentMean 1.00 Eps 0.050\n",
      "[squared] Ep 28800 Steps 84811 RecentMean 0.96 Eps 0.050\n",
      "Eval @ Ep 28800 and 84811 steps => mean return 1.00\n",
      "[squared] Ep 28900 Steps 84916 RecentMean 1.00 Eps 0.050\n",
      "[squared] Ep 29000 Steps 85087 RecentMean 0.96 Eps 0.050\n",
      "Eval @ Ep 29000 and 85087 steps => mean return 1.00\n",
      "[squared] Ep 29100 Steps 85218 RecentMean 1.00 Eps 0.050\n",
      "[squared] Ep 29200 Steps 85321 RecentMean 1.00 Eps 0.050\n",
      "Eval @ Ep 29200 and 85321 steps => mean return 1.00\n",
      "[squared] Ep 29300 Steps 85429 RecentMean 1.00 Eps 0.050\n",
      "[squared] Ep 29400 Steps 85532 RecentMean 1.00 Eps 0.050\n",
      "Eval @ Ep 29400 and 85532 steps => mean return 1.00\n",
      "[squared] Ep 29500 Steps 85669 RecentMean 0.98 Eps 0.050\n",
      "[squared] Ep 29600 Steps 85775 RecentMean 1.00 Eps 0.050\n",
      "Eval @ Ep 29600 and 85775 steps => mean return 1.00\n",
      "[squared] Ep 29700 Steps 85879 RecentMean 1.00 Eps 0.050\n",
      "[squared] Ep 29800 Steps 86012 RecentMean 1.00 Eps 0.050\n",
      "Eval @ Ep 29800 and 86012 steps => mean return 1.00\n",
      "[squared] Ep 29900 Steps 86131 RecentMean 1.00 Eps 0.050\n",
      "[squared] Ep 30000 Steps 86290 RecentMean 0.98 Eps 0.050\n",
      "Eval @ Ep 30000 and 86290 steps => mean return 1.00\n",
      "[squared] Ep 30100 Steps 86441 RecentMean 1.00 Eps 0.050\n",
      "[squared] Ep 30200 Steps 86586 RecentMean 1.00 Eps 0.050\n",
      "Eval @ Ep 30200 and 86586 steps => mean return 1.00\n",
      "[squared] Ep 30300 Steps 86787 RecentMean 0.98 Eps 0.050\n",
      "[squared] Ep 30400 Steps 86966 RecentMean 0.98 Eps 0.050\n",
      "Eval @ Ep 30400 and 86966 steps => mean return 1.00\n",
      "[squared] Ep 30500 Steps 87121 RecentMean 1.00 Eps 0.050\n",
      "[squared] Ep 30600 Steps 87227 RecentMean 1.00 Eps 0.050\n",
      "Eval @ Ep 30600 and 87227 steps => mean return 1.00\n",
      "[squared] Ep 30700 Steps 87337 RecentMean 1.00 Eps 0.050\n",
      "[squared] Ep 30800 Steps 87449 RecentMean 1.00 Eps 0.050\n",
      "Eval @ Ep 30800 and 87449 steps => mean return 1.00\n",
      "[squared] Ep 30900 Steps 87556 RecentMean 1.00 Eps 0.050\n",
      "[squared] Ep 31000 Steps 87662 RecentMean 1.00 Eps 0.050\n",
      "Eval @ Ep 31000 and 87662 steps => mean return 1.00\n",
      "[squared] Ep 31100 Steps 87768 RecentMean 1.00 Eps 0.050\n",
      "[squared] Ep 31200 Steps 87920 RecentMean 1.00 Eps 0.050\n",
      "Eval @ Ep 31200 and 87920 steps => mean return 1.00\n",
      "[squared] Ep 31300 Steps 88036 RecentMean 1.00 Eps 0.050\n",
      "[squared] Ep 31400 Steps 88255 RecentMean 0.94 Eps 0.050\n",
      "Eval @ Ep 31400 and 88255 steps => mean return 1.00\n",
      "[squared] Ep 31500 Steps 88394 RecentMean 1.00 Eps 0.050\n",
      "[squared] Ep 31600 Steps 88500 RecentMean 1.00 Eps 0.050\n",
      "Eval @ Ep 31600 and 88500 steps => mean return 1.00\n",
      "[squared] Ep 31700 Steps 88654 RecentMean 0.98 Eps 0.050\n",
      "[squared] Ep 31800 Steps 88754 RecentMean 1.00 Eps 0.050\n",
      "Eval @ Ep 31800 and 88754 steps => mean return 1.00\n",
      "[squared] Ep 31900 Steps 88855 RecentMean 1.00 Eps 0.050\n",
      "[squared] Ep 32000 Steps 88968 RecentMean 1.00 Eps 0.050\n",
      "Eval @ Ep 32000 and 88968 steps => mean return 1.00\n",
      "[squared] Ep 32100 Steps 89080 RecentMean 1.00 Eps 0.050\n",
      "[squared] Ep 32200 Steps 89188 RecentMean 1.00 Eps 0.050\n",
      "Eval @ Ep 32200 and 89188 steps => mean return 1.00\n",
      "[squared] Ep 32300 Steps 89316 RecentMean 1.00 Eps 0.050\n",
      "[squared] Ep 32400 Steps 89420 RecentMean 1.00 Eps 0.050\n",
      "Eval @ Ep 32400 and 89420 steps => mean return 1.00\n",
      "[squared] Ep 32500 Steps 89524 RecentMean 1.00 Eps 0.050\n",
      "[squared] Ep 32600 Steps 89698 RecentMean 0.98 Eps 0.050\n",
      "Eval @ Ep 32600 and 89698 steps => mean return 1.00\n",
      "[squared] Ep 32700 Steps 89801 RecentMean 1.00 Eps 0.050\n",
      "[squared] Ep 32800 Steps 89914 RecentMean 1.00 Eps 0.050\n",
      "Eval @ Ep 32800 and 89914 steps => mean return 1.00\n",
      "[squared] Ep 32900 Steps 90021 RecentMean 1.00 Eps 0.050\n",
      "[squared] Ep 33000 Steps 90127 RecentMean 1.00 Eps 0.050\n",
      "Eval @ Ep 33000 and 90127 steps => mean return 1.00\n",
      "[squared] Ep 33100 Steps 90250 RecentMean 1.00 Eps 0.050\n",
      "[squared] Ep 33200 Steps 90352 RecentMean 1.00 Eps 0.050\n",
      "Eval @ Ep 33200 and 90352 steps => mean return 1.00\n",
      "[squared] Ep 33300 Steps 90461 RecentMean 1.00 Eps 0.050\n",
      "[squared] Ep 33400 Steps 90647 RecentMean 0.96 Eps 0.050\n",
      "Eval @ Ep 33400 and 90647 steps => mean return 1.00\n",
      "[squared] Ep 33500 Steps 90759 RecentMean 1.00 Eps 0.050\n",
      "[squared] Ep 33600 Steps 90865 RecentMean 1.00 Eps 0.050\n",
      "Eval @ Ep 33600 and 90865 steps => mean return 1.00\n",
      "[squared] Ep 33700 Steps 90973 RecentMean 1.00 Eps 0.050\n",
      "[squared] Ep 33800 Steps 91079 RecentMean 1.00 Eps 0.050\n",
      "Eval @ Ep 33800 and 91079 steps => mean return 1.00\n",
      "[squared] Ep 33900 Steps 91189 RecentMean 1.00 Eps 0.050\n",
      "[squared] Ep 34000 Steps 91325 RecentMean 0.98 Eps 0.050\n",
      "Eval @ Ep 34000 and 91325 steps => mean return 1.00\n",
      "[squared] Ep 34100 Steps 91433 RecentMean 1.00 Eps 0.050\n",
      "[squared] Ep 34200 Steps 91538 RecentMean 1.00 Eps 0.050\n",
      "Eval @ Ep 34200 and 91538 steps => mean return 1.00\n",
      "[squared] Ep 34300 Steps 91707 RecentMean 0.96 Eps 0.050\n",
      "[squared] Ep 34400 Steps 91817 RecentMean 1.00 Eps 0.050\n",
      "Eval @ Ep 34400 and 91817 steps => mean return 1.00\n",
      "[squared] Ep 34500 Steps 91919 RecentMean 1.00 Eps 0.050\n",
      "[squared] Ep 34600 Steps 92036 RecentMean 1.00 Eps 0.050\n",
      "Eval @ Ep 34600 and 92036 steps => mean return 1.00\n",
      "[squared] Ep 34700 Steps 92188 RecentMean 1.00 Eps 0.050\n",
      "[squared] Ep 34800 Steps 92297 RecentMean 1.00 Eps 0.050\n",
      "Eval @ Ep 34800 and 92297 steps => mean return 1.00\n",
      "[squared] Ep 34900 Steps 92399 RecentMean 1.00 Eps 0.050\n",
      "[squared] Ep 35000 Steps 92507 RecentMean 1.00 Eps 0.050\n",
      "Eval @ Ep 35000 and 92507 steps => mean return 1.00\n",
      "[squared] Ep 35100 Steps 92645 RecentMean 0.98 Eps 0.050\n",
      "[squared] Ep 35200 Steps 92751 RecentMean 1.00 Eps 0.050\n",
      "Eval @ Ep 35200 and 92751 steps => mean return 1.00\n",
      "[squared] Ep 35300 Steps 92865 RecentMean 1.00 Eps 0.050\n",
      "[squared] Ep 35400 Steps 93026 RecentMean 0.98 Eps 0.050\n",
      "Eval @ Ep 35400 and 93026 steps => mean return 1.00\n",
      "[squared] Ep 35500 Steps 93194 RecentMean 1.00 Eps 0.050\n",
      "[squared] Ep 35600 Steps 93301 RecentMean 1.00 Eps 0.050\n",
      "Eval @ Ep 35600 and 93301 steps => mean return 1.00\n",
      "[squared] Ep 35700 Steps 93477 RecentMean 0.96 Eps 0.050\n",
      "[squared] Ep 35800 Steps 93578 RecentMean 1.00 Eps 0.050\n",
      "Eval @ Ep 35800 and 93578 steps => mean return 1.00\n",
      "[squared] Ep 35900 Steps 93680 RecentMean 1.00 Eps 0.050\n",
      "[squared] Ep 36000 Steps 93792 RecentMean 1.00 Eps 0.050\n",
      "Eval @ Ep 36000 and 93792 steps => mean return 1.00\n",
      "[squared] Ep 36100 Steps 93939 RecentMean 0.98 Eps 0.050\n",
      "[squared] Ep 36200 Steps 94123 RecentMean 0.98 Eps 0.050\n",
      "Eval @ Ep 36200 and 94123 steps => mean return 1.00\n",
      "[squared] Ep 36300 Steps 94223 RecentMean 1.00 Eps 0.050\n",
      "[squared] Ep 36400 Steps 94328 RecentMean 1.00 Eps 0.050\n",
      "Eval @ Ep 36400 and 94328 steps => mean return 1.00\n",
      "[squared] Ep 36500 Steps 94437 RecentMean 1.00 Eps 0.050\n",
      "[squared] Ep 36600 Steps 94540 RecentMean 1.00 Eps 0.050\n",
      "Eval @ Ep 36600 and 94540 steps => mean return 1.00\n",
      "[squared] Ep 36700 Steps 94642 RecentMean 1.00 Eps 0.050\n",
      "[squared] Ep 36800 Steps 94779 RecentMean 0.98 Eps 0.050\n",
      "Eval @ Ep 36800 and 94779 steps => mean return 1.00\n",
      "[squared] Ep 36900 Steps 94923 RecentMean 1.00 Eps 0.050\n",
      "[squared] Ep 37000 Steps 95042 RecentMean 1.00 Eps 0.050\n",
      "Eval @ Ep 37000 and 95042 steps => mean return 1.00\n",
      "[squared] Ep 37100 Steps 95155 RecentMean 1.00 Eps 0.050\n",
      "[squared] Ep 37200 Steps 95266 RecentMean 1.00 Eps 0.050\n",
      "Eval @ Ep 37200 and 95266 steps => mean return 1.00\n",
      "[squared] Ep 37300 Steps 95375 RecentMean 1.00 Eps 0.050\n",
      "[squared] Ep 37400 Steps 95482 RecentMean 1.00 Eps 0.050\n",
      "Eval @ Ep 37400 and 95482 steps => mean return 1.00\n",
      "[squared] Ep 37500 Steps 95586 RecentMean 1.00 Eps 0.050\n",
      "[squared] Ep 37600 Steps 95693 RecentMean 1.00 Eps 0.050\n",
      "Eval @ Ep 37600 and 95693 steps => mean return 1.00\n",
      "[squared] Ep 37700 Steps 95822 RecentMean 1.00 Eps 0.050\n",
      "[squared] Ep 37800 Steps 95934 RecentMean 1.00 Eps 0.050\n",
      "Eval @ Ep 37800 and 95934 steps => mean return 1.00\n",
      "[squared] Ep 37900 Steps 96057 RecentMean 1.00 Eps 0.050\n",
      "[squared] Ep 38000 Steps 96164 RecentMean 1.00 Eps 0.050\n",
      "Eval @ Ep 38000 and 96164 steps => mean return 1.00\n",
      "[squared] Ep 38100 Steps 96268 RecentMean 1.00 Eps 0.050\n",
      "[squared] Ep 38200 Steps 96372 RecentMean 1.00 Eps 0.050\n",
      "Eval @ Ep 38200 and 96372 steps => mean return 1.00\n",
      "[squared] Ep 38300 Steps 96479 RecentMean 1.00 Eps 0.050\n",
      "[squared] Ep 38400 Steps 96582 RecentMean 1.00 Eps 0.050\n",
      "Eval @ Ep 38400 and 96582 steps => mean return 1.00\n",
      "[squared] Ep 38500 Steps 96682 RecentMean 1.00 Eps 0.050\n",
      "[squared] Ep 38600 Steps 96817 RecentMean 1.00 Eps 0.050\n",
      "Eval @ Ep 38600 and 96817 steps => mean return 1.00\n",
      "[squared] Ep 38700 Steps 96933 RecentMean 1.00 Eps 0.050\n",
      "[squared] Ep 38800 Steps 97066 RecentMean 1.00 Eps 0.050\n",
      "Eval @ Ep 38800 and 97066 steps => mean return 1.00\n",
      "[squared] Ep 38900 Steps 97174 RecentMean 1.00 Eps 0.050\n",
      "[squared] Ep 39000 Steps 97349 RecentMean 0.98 Eps 0.050\n",
      "Eval @ Ep 39000 and 97349 steps => mean return 1.00\n",
      "[squared] Ep 39100 Steps 97461 RecentMean 1.00 Eps 0.050\n",
      "[squared] Ep 39200 Steps 97583 RecentMean 1.00 Eps 0.050\n",
      "Eval @ Ep 39200 and 97583 steps => mean return 1.00\n",
      "[squared] Ep 39300 Steps 97813 RecentMean 0.94 Eps 0.050\n",
      "[squared] Ep 39400 Steps 97983 RecentMean 0.96 Eps 0.050\n",
      "Eval @ Ep 39400 and 97983 steps => mean return 1.00\n",
      "[squared] Ep 39500 Steps 98151 RecentMean 1.00 Eps 0.050\n",
      "[squared] Ep 39600 Steps 98293 RecentMean 0.98 Eps 0.050\n",
      "Eval @ Ep 39600 and 98293 steps => mean return 1.00\n",
      "[squared] Ep 39700 Steps 98468 RecentMean 0.98 Eps 0.050\n",
      "[squared] Ep 39800 Steps 98846 RecentMean 0.90 Eps 0.050\n",
      "Eval @ Ep 39800 and 98846 steps => mean return 1.00\n",
      "[squared] Ep 39900 Steps 99170 RecentMean 0.92 Eps 0.050\n",
      "[squared] Ep 40000 Steps 99284 RecentMean 1.00 Eps 0.050\n",
      "Eval @ Ep 40000 and 99284 steps => mean return 1.00\n",
      "[squared] Ep 40100 Steps 99387 RecentMean 1.00 Eps 0.050\n",
      "[squared] Ep 40200 Steps 99544 RecentMean 0.98 Eps 0.050\n",
      "Eval @ Ep 40200 and 99544 steps => mean return 1.00\n",
      "[squared] Ep 40300 Steps 99652 RecentMean 1.00 Eps 0.050\n",
      "[squared] Ep 40400 Steps 99819 RecentMean 1.00 Eps 0.050\n",
      "Eval @ Ep 40400 and 99819 steps => mean return 1.00\n",
      "[squared] Ep 40500 Steps 99961 RecentMean 0.98 Eps 0.050\n",
      "Saved checkpoint: checkpoint_stage_squared.pth\n",
      "\n",
      "=== Stage 2: cartpole ===\n",
      "Obs space: 4\n",
      "Obs space: 4\n",
      "Starting training...\n",
      "[cartpole] Ep 100 Steps 1066 RecentMean 9.66 Eps 0.949\n",
      "[cartpole] Ep 200 Steps 2168 RecentMean 10.02 Eps 0.897\n",
      "Eval @ Ep 200 and 2168 steps => mean return 4.00\n",
      "[cartpole] Ep 300 Steps 3224 RecentMean 9.56 Eps 0.847\n",
      "[cartpole] Ep 400 Steps 4264 RecentMean 9.40 Eps 0.798\n",
      "Eval @ Ep 400 and 4264 steps => mean return 4.00\n",
      "[cartpole] Ep 500 Steps 5201 RecentMean 8.37 Eps 0.753\n",
      "[cartpole] Ep 600 Steps 5992 RecentMean 6.91 Eps 0.715\n",
      "Eval @ Ep 600 and 5992 steps => mean return 5.00\n",
      "[cartpole] Ep 700 Steps 7330 RecentMean 12.38 Eps 0.652\n",
      "[cartpole] Ep 800 Steps 9590 RecentMean 21.60 Eps 0.545\n",
      "Eval @ Ep 800 and 9590 steps => mean return 23.00\n",
      "[cartpole] Ep 900 Steps 12991 RecentMean 33.01 Eps 0.383\n",
      "[cartpole] Ep 1000 Steps 17220 RecentMean 41.29 Eps 0.182\n",
      "Eval @ Ep 1000 and 17220 steps => mean return 94.00\n",
      "[cartpole] Ep 1100 Steps 27082 RecentMean 97.62 Eps 0.050\n",
      "[cartpole] Ep 1200 Steps 36784 RecentMean 96.02 Eps 0.050\n",
      "Eval @ Ep 1200 and 36784 steps => mean return 87.00\n",
      "[cartpole] Ep 1300 Steps 44169 RecentMean 72.85 Eps 0.050\n",
      "[cartpole] Ep 1400 Steps 51167 RecentMean 68.98 Eps 0.050\n",
      "Eval @ Ep 1400 and 51167 steps => mean return 59.00\n",
      "[cartpole] Ep 1500 Steps 57538 RecentMean 62.71 Eps 0.050\n",
      "[cartpole] Ep 1600 Steps 64298 RecentMean 66.60 Eps 0.050\n",
      "Eval @ Ep 1600 and 64298 steps => mean return 71.00\n",
      "[cartpole] Ep 1700 Steps 72236 RecentMean 78.38 Eps 0.050\n",
      "[cartpole] Ep 1800 Steps 81759 RecentMean 94.23 Eps 0.050\n",
      "Eval @ Ep 1800 and 81759 steps => mean return 108.00\n",
      "[cartpole] Ep 1900 Steps 93061 RecentMean 112.02 Eps 0.050\n",
      "Saved checkpoint: checkpoint_stage_cartpole.pth\n",
      "\n",
      "=== Stage 3: breakout ===\n",
      "Obs space: 118\n",
      "Obs space: 118\n",
      "Starting training...\n",
      "[breakout] Ep 100 Steps 27562 RecentMean 4.76 Eps 0.050\n",
      "[breakout] Ep 200 Steps 55713 RecentMean 4.83 Eps 0.050\n",
      "Eval @ Ep 200 and 55713 steps => mean return 3.00\n",
      "[breakout] Ep 300 Steps 85209 RecentMean 5.20 Eps 0.050\n",
      "Saved checkpoint: checkpoint_stage_breakout.pth\n"
     ]
    }
   ],
   "source": [
    "all_stage_logs: Dict[str, Dict[str, List]] = {}\n",
    "\n",
    "q_net = None\n",
    "target_net = None\n",
    "optimizer = None\n",
    "stage_index = 0\n",
    "\n",
    "# Easy way to check agent performance over different envs\n",
    "for stage_env_name in cfg.curriculum:\n",
    "    print(f\"\\n=== Stage {stage_index+1}: {stage_env_name} ===\")\n",
    "    env = make_env(stage_env_name, seed=cfg.seed + stage_index)()\n",
    "    eval_env = make_env(stage_env_name, seed=cfg.seed + 100 + stage_index)()\n",
    "\n",
    "    q_net = build_q_network(env, cfg).to(DEVICE)\n",
    "    target_net = build_q_network(env, cfg).to(DEVICE)\n",
    "    hard_update(target_net, q_net)\n",
    "    optimizer = optim.Adam(q_net.parameters(), lr=cfg.lr)\n",
    "\n",
    "\n",
    "    replay = ReplayBuffer(cfg.buffer_capacity, env.single_observation_space.shape, DEVICE)\n",
    "\n",
    "    logs = {\n",
    "        'episode_rewards': [],\n",
    "        'losses': [],\n",
    "        'eps_history': [],\n",
    "        'eval': [],\n",
    "    }\n",
    "\n",
    "    obs, _ = env.reset()\n",
    "    ep_reward = 0\n",
    "    ep_steps = 0\n",
    "    episode_counter = 0\n",
    "    recent_ep_returns = []\n",
    "    total_steps = 0\n",
    "    print('Starting training...')\n",
    "    while total_steps < cfg.max_steps_per_env:\n",
    "        epsilon = epsilon_by_step(total_steps, cfg)\n",
    "        action = select_action(q_net, obs, epsilon, env) \n",
    "        next_obs, reward, terminated, truncated, info = env.step(action)\n",
    "        reward = process_reward(reward, cfg)\n",
    "        reward = float(np.asarray(reward).reshape(-1)[0])\n",
    "        done = bool(np.asarray(terminated).reshape(-1)[0] or np.asarray(truncated).reshape(-1)[0])\n",
    "        replay.push(obs.copy(), int(action), reward, next_obs.copy(), done)\n",
    "        obs = next_obs.copy()\n",
    "        ep_reward += reward\n",
    "        ep_steps += 1\n",
    "        total_steps += 1\n",
    "\n",
    "        # Training\n",
    "        if total_steps % cfg.train_freq == 0:\n",
    "            loss = train_step(q_net, target_net, optimizer, replay, cfg)\n",
    "            if loss is not None:\n",
    "                logs['losses'].append(loss)\n",
    "\n",
    "        # Target update\n",
    "        if total_steps % cfg.target_update_interval == 0:\n",
    "            hard_update(target_net, q_net)\n",
    "\n",
    "        logs['eps_history'].append(epsilon)\n",
    "\n",
    "        if done or ep_steps >= cfg.max_episode_len:\n",
    "            logs['episode_rewards'].append(ep_reward)\n",
    "            recent_ep_returns.append(ep_reward)\n",
    "            episode_counter += 1\n",
    "            if episode_counter % cfg.log_interval_episodes == 0:\n",
    "                mean_recent = np.mean(recent_ep_returns[-cfg.log_interval_episodes:])\n",
    "                print(f\"[{stage_env_name}] Ep {episode_counter} Steps {total_steps} RecentMean {mean_recent:.2f} Eps {epsilon:.3f}\")\n",
    "            obs, _ = env.reset()\n",
    "            ep_reward = 0\n",
    "            ep_steps = 0\n",
    "\n",
    "            # Periodic evaluation (every 2*log_interval episodes if we have progress)\n",
    "            if episode_counter % (cfg.log_interval_episodes * 2) == 0:\n",
    "                eval_ret = evaluate(eval_env, q_net, cfg, cfg.eval_episodes)\n",
    "                logs['eval'].append((total_steps, eval_ret))\n",
    "                print(f\"Eval @ Ep {episode_counter} and {total_steps} steps => mean return {eval_ret:.2f}\")\n",
    "\n",
    "    save_checkpoint(f\"checkpoint_stage_{stage_env_name}.pth\", q_net, optimizer, cfg, stage_env_name)\n",
    "    all_stage_logs[stage_env_name] = logs\n",
    "    stage_index += 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "5116f30d11e74052a8c1ed8dda096113",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "##  Visualization Per Stage\n",
    "helper code for plots, feel free to modify or use different tools, e.g. WandB.\n",
    "Plots: Episode rewards, Epsilon schedule, Loss curve, and Evaluation returns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "cell_id": "ea17fafbec3e4fbcb411e6d628db8978",
    "deepnote_cell_type": "code",
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'all_stage_logs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m x\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39mconvolve(x, np\u001b[38;5;241m.\u001b[39mones(w)\u001b[38;5;241m/\u001b[39mw, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvalid\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m stage, logs \u001b[38;5;129;01min\u001b[39;00m \u001b[43mall_stage_logs\u001b[49m\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m      9\u001b[0m     rewards \u001b[38;5;241m=\u001b[39m logs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mepisode_rewards\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     10\u001b[0m     losses \u001b[38;5;241m=\u001b[39m logs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlosses\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'all_stage_logs' is not defined"
     ]
    }
   ],
   "source": [
    "def moving_average(x, w):\n",
    "    # Ensure x is a flattened 1D numpy array to handle lists like [[1],[2],[3]]\n",
    "    x = np.array(x).flatten()\n",
    "    if len(x) < w:\n",
    "        return x\n",
    "    return np.convolve(x, np.ones(w)/w, mode='valid')\n",
    "\n",
    "for stage, logs in all_stage_logs.items():\n",
    "    rewards = logs['episode_rewards']\n",
    "    losses = logs['losses']\n",
    "    eps_hist = logs['eps_history']\n",
    "    evals = logs['eval']\n",
    "    print(f\"\\n=== {stage} ===\")\n",
    "    plt.figure(figsize=(14,4))\n",
    "    # Rewards\n",
    "    plt.subplot(1,3,1)\n",
    "    plt.title(f'{stage} Rewards')\n",
    "    plt.plot(rewards, alpha=0.5, label='Return')\n",
    "    ma = moving_average(rewards, 100)\n",
    "    if len(ma) != len(rewards):\n",
    "        plt.plot(range(len(rewards)-len(ma), len(rewards)), ma, label='MA(20)')\n",
    "    plt.legend()\n",
    "    plt.xlabel('Episode')\n",
    "\n",
    "    # Epsilon\n",
    "    plt.subplot(1,3,2)\n",
    "    plt.title('Epsilon')\n",
    "    plt.plot(eps_hist)\n",
    "    plt.xlabel('Step')\n",
    "\n",
    "    # Losses\n",
    "    plt.subplot(1,3,3)\n",
    "    plt.title('Loss (smoothed)')\n",
    "    if len(losses) > 0:\n",
    "        lma = moving_average(losses, 200)\n",
    "        plt.plot(losses, alpha=0.3)\n",
    "        plt.plot(range(len(losses)-len(lma), len(losses)), lma, label='MA(200)')\n",
    "        plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    if len(evals) > 0:\n",
    "        xs = [x for x,_ in evals]\n",
    "        ys = [y for _,y in evals]\n",
    "        plt.figure()\n",
    "        plt.title(f'{stage} Evaluation Returns')\n",
    "        plt.plot(xs, ys, marker='o')\n",
    "        plt.xlabel('Env Steps')\n",
    "        plt.ylabel('Mean Return (greedy)')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "746845cb8d9d4ed895e2549639752df6",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "## 2.1 Group Project: Extensions and Analysis\n",
    "\n",
    "This assignment is a group project. After familiarizing yourselves with the baseline DQN code, your group will divide the following implementation tasks. Once each part is complete, you will merge your code and collaborate on a final report that analyzes and compares the performance of each extension.\n",
    "\n",
    "---\n",
    "### **Part 1: Individual Implementation Tasks**\n",
    "\n",
    "Each group member must implement one of the following sets of extensions. You will need to modify the core components (`MLPQ`, `ReplayBuffer`, `train_step`) to support these new algorithms, using the `cfg` object to toggle them on and off.\n",
    "\n",
    "---\n",
    "#### **Extension A: Double DQN & Dueling DQN**\n",
    "\n",
    "**Assigned to:** *[Student A Name Here]*\n",
    "\n",
    "This set of tasks focuses on improving the Q-value estimation and network architecture.\n",
    "\n",
    "**TODO (A1): Implement Double DQN**\n",
    "In the `train_step` function, implement the logic inside the `if cfg.use_double_dqn:` block. This involves decoupling action selection from evaluation to mitigate overestimation bias by using the **online network** to select the best next action and the **target network** to evaluate its value.\n",
    "\n",
    "**TODO (A2): Implement Dueling DQN**\n",
    "In the `MLPQ` class, modify the `__init__` and `forward` methods to support a dueling architecture. You will need to create separate `value_head` and `advantage_head` layers and combine their outputs using the formula: `Q(s,a) = V(s) + (A(s,a) - mean(A(s,:)))`.\n",
    "\n",
    "---\n",
    "#### **Extension B: Prioritized Experience Replay (PER)**\n",
    "\n",
    "**Assigned to:** *[Student B Name Here]*\n",
    "\n",
    "This task replaces the uniform `ReplayBuffer` with a more intelligent sampling strategy that prioritizes \"surprising\" transitions.\n",
    "\n",
    "**TODO (B1): Implement `PrioritizedReplayBuffer`**\n",
    "Create a new `PrioritizedReplayBuffer` class. This class must manage transition priorities (based on TD-error), sample transitions according to these priorities, and compute importance sampling (IS) weights to correct for the biased sampling. Implementing a **SumTree** is the standard approach for this.\n",
    "\n",
    "**TODO (B2): Integrate PER into `train_step`**\n",
    "Modify the training loop to use your new buffer. In `train_step`, you must use the IS weights to scale the loss for each transition and then call a method on your buffer to update the priorities of the sampled transitions with their new TD-errors.\n",
    "\n",
    "---\n",
    "#### **Extension C: N-Step Returns & Noisy Networks**\n",
    "\n",
    "**Assigned to:** *[Student C Name Here]*\n",
    "\n",
    "This set of tasks focuses on improving the TD target and the agent's exploration strategy.\n",
    "\n",
    "**TODO (C1): Implement N-Step Returns**\n",
    "Modify the training loop and `train_step` to use N-step returns. This involves temporarily storing the last `N` transitions to calculate the discounted N-step reward (`R_n`) and updating the target formula in `train_step` to use `γ^N` for bootstrapping.\n",
    "\n",
    "**TODO (C2): Implement Noisy Networks**\n",
    "Replace epsilon-greedy exploration with learned exploration. Create a custom `NoisyLinear` PyTorch layer that adds parametric noise to its weights. Replace the final linear layers of your `MLPQ` with this new layer and disable epsilon-greedy exploration in the main loop when this feature is active.\n",
    "\n",
    "---\n",
    "### **Optional Extension**\n",
    "\n",
    "#### **Vectorized Environments**\n",
    "For a deeper challenge, modify the entire pipeline to support vectorized environments. This involves changing `cfg.vector_envs` to > 1 and refactoring the training loop to handle batched operations for action selection, environment stepping, and episode tracking. This is a highly effective method for speeding up training but requires careful management of parallel data streams.\n",
    "\n",
    "---\n",
    "### **Part 2: Final Report (Group Task)**\n",
    "\n",
    "Your group's final submission should be a report that includes the following:\n",
    "\n",
    "**1. Experimental Analysis:**\n",
    "*   Run experiments on the Comparing environment comparing your baseline DQN against each of the implemented extensions (A, B, and C) and all extension combined on the challenge environment.\n",
    "*   Generate and include plots for each experiment (Episode Rewards, Loss, etc.).\n",
    "*  For the replay buffer size and N-Step Returns, test out 2-4 different values and reflect on the difference in performance. You may change other hyperparameters as needed to get good performance, also put the highlights in the report.\n",
    "*   Analyze the plots: Reflect how you can see the improvements for each extension. Which extension provided the biggest performance boost or the most stable training? Justify your claims with evidence.\n",
    "\n",
    "**2. Conceptual Questions:**\n",
    "\n",
    "*   **Q1:** Explain \"maximization bias\" in Q-learning. How does your **Double DQN** implementation address it?\n",
    "*   **Q2:** What is the theoretical motivation for the **Dueling DQN** architecture? Why is the special averaging mechanism important?\n",
    "*   **Q3:** Why is uniform sampling from the replay buffer inefficient? How do the **importance sampling weights** in **PER** correct for the biased sampling you introduced?\n",
    "*   **Q4:** Explain the difference between epsilon-greedy exploration and the exploration provided by **Noisy Networks**. What is an advantage of the latter?\n",
    "*   **Q5:** How does changing 'N' in **N-Step Returns** affect the bias-variance trade-off in your Q-learning updates?\n",
    "\n",
    "**3. Reflection:**\n",
    "*   Briefly discuss the biggest challenge your group faced during implementation and how you solved it.\n",
    "*   Were some of the results unexpected, if so in what way?\n",
    "*   If you implemented the vectorized environments, describe the performance improvement you observed in terms of wall-clock time.\n",
    "\n",
    "**4. Code Submission:**\n",
    "*   Submit your complete code with all extensions implemented. Ensure it is well-commented and organized.\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "4c1627b4b47546c786007c46288e7a1a",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "# Visualize agent\n",
    "helper code to visualize your trained agent in the notebook. You may modify this code as needed or visualize it in a different way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "cell_id": "e48d56f26c314eb8855faf060517c9f5",
    "deepnote_cell_type": "code"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode finished.\n",
      "Greedy policy return: 1.00\n",
      "Cleaning up resources...\n",
      "INFO: TEXTURE: [ID 2] Unloaded texture data from VRAM (GPU)\n",
      "INFO: SHADER: [ID 3] Default shader unloaded successfully\n",
      "INFO: TEXTURE: [ID 1] Default texture unloaded successfully\n",
      "INFO: Window closed successfully\n",
      "Cleanup complete. If you interrupted the cell, please restart the kernel now.\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyvirtualdisplay import Display\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display, clear_output\n",
    "import time\n",
    "import os\n",
    "import pyscreenshot as ImageGrab\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# --- Visualization Setup (from our debugging) ---\n",
    "seed = 0\n",
    "steps_to_show = 1000 # Your original value\n",
    "game_width = 576\n",
    "game_height = 330\n",
    "bbox = (0, 0, game_width, game_height)\n",
    "\n",
    "# Initialize variables to None before the try block for safe cleanup\n",
    "vdisplay = None\n",
    "env = None\n",
    "\n",
    "original_dir = os.getcwd()\n",
    "pufferlib_dir = '/opt/pufferlib'\n",
    "# -------------------------------------------------\n",
    "def first_obs(x):\n",
    "    x = np.asarray(x)\n",
    "    return x[0] if x.ndim > 1 else x\n",
    "\n",
    "# This is your agent's decision-making function, it's perfect as is.\n",
    "def policy_net(st):\n",
    "    return q_net(st)\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def greedy_action(state_np):\n",
    "    st = torch.as_tensor(state_np, dtype=torch.float32, device=DEVICE).unsqueeze(0)\n",
    "    q = policy_net(st)\n",
    "    print(q)\n",
    "    return int(q.argmax(dim=1).item())\n",
    "\n",
    "# The robust 'try...finally' block for safe execution and cleanup\n",
    "try:\n",
    "    vdisplay = Display(visible=False, size=(game_width, game_height))\n",
    "    vdisplay.start()\n",
    "\n",
    "    os.chdir(pufferlib_dir)\n",
    "    # env = breakout.Breakout(num_envs=1, render_mode='human', seed=seed)\n",
    "    # env = cartpole.Cartpole(num_envs=1, render_mode='human', seed=seed)\n",
    "    env = squared.Squared(num_envs=1, render_mode='human', seed=seed)\n",
    "    \n",
    "    fig, ax = plt.subplots(1)\n",
    "\n",
    "    # --- Your Agent's Logic Starts Here ---\n",
    "    obs, _ = env.reset(seed)\n",
    "    state = first_obs(obs).astype(np.float32)\n",
    "    episode_return = 0.0\n",
    "\n",
    "    print(\"Starting agent visualization... Press 'Stop' to interrupt cleanly.\")\n",
    "    print(\"IMPORTANT: After interrupting, you MUST restart the kernel before running this cell again.\")\n",
    "\n",
    "    for t in range(steps_to_show):\n",
    "        # The inner 'try...except' for clean interruption\n",
    "        try:\n",
    "            # Get action from your policy instead of random\n",
    "            q_net = build_q_network(env, cfg).to(DEVICE)\n",
    "            a = greedy_action(state)\n",
    "\n",
    "            # Step the environment\n",
    "            obs, reward, term, trunc, _info = env.step(a)\n",
    "\n",
    "            # --- Apply the Visualization Pipeline ---\n",
    "            # 1. Render the game to the invisible virtual display\n",
    "            env.render()\n",
    "\n",
    "            # 2. Grab a cropped screenshot of the virtual display\n",
    "            frame = ImageGrab.grab(bbox=bbox)\n",
    "\n",
    "            # 3. Display the screenshot in the notebook\n",
    "            clear_output(wait=True)\n",
    "            ax.imshow(frame)\n",
    "            ax.axis('off')\n",
    "            display(fig)\n",
    "\n",
    "            time.sleep(0.01)\n",
    "            # --- End of Visualization Pipeline ---\n",
    "\n",
    "            # Continue with your agent's logic\n",
    "            state = first_obs(obs).astype(np.float32)\n",
    "            episode_return += float(first_obs(reward))\n",
    "            done = bool(first_obs(term)) or bool(first_obs(trunc))\n",
    "            if done:\n",
    "                print(\"Episode finished.\")\n",
    "                break\n",
    "\n",
    "        except KeyboardInterrupt:\n",
    "            print(\"\\n\\nInterrupted by user. The kernel is now in an unstable state and must be restarted.\")\n",
    "            break\n",
    "\n",
    "    print(f'Greedy policy return: {episode_return:.2f}')\n",
    "\n",
    "finally:\n",
    "    # This block is ALWAYS executed, ensuring a safe shutdown.\n",
    "    print(\"Cleaning up resources...\")\n",
    "    os.chdir(original_dir)\n",
    "\n",
    "    if env is not None:\n",
    "        env.close()\n",
    "\n",
    "    time.sleep(0.5)\n",
    "\n",
    "    if vdisplay is not None:\n",
    "        vdisplay.stop()\n",
    "    print(\"Cleanup complete. If you interrupted the cell, please restart the kernel now.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "cell_id": "a9c71d46ddeb4799890dbbfc06be8847",
    "deepnote_cell_type": "code"
   },
   "outputs": [],
   "source": [
    "def save_checkpoint(path, q_net, optimizer, cfg: Config, stage_name: str):\n",
    "    torch.save({\n",
    "        'model': q_net.state_dict(),\n",
    "        'optimizer': optimizer.state_dict(),\n",
    "        'config': dataclasses.asdict(cfg),\n",
    "        'stage': stage_name,\n",
    "    }, path)\n",
    "    print('Saved checkpoint:', path)\n",
    "\n",
    "def load_checkpoint(path, q_net, optimizer):\n",
    "    data = torch.load(path, map_location=DEVICE)\n",
    "    q_net.load_state_dict(data['model'])\n",
    "    optimizer.load_state_dict(data['optimizer'])\n",
    "    print('Loaded checkpoint from', path)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "deepnote_notebook_id": "43ce95ddb5754e06b99d62b9ae367324",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
