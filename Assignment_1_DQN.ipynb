{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1: Deep Q-Networks (DQN) with [PufferLib Ocean Environments](https://puffer.ai/ocean.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*This assignment must be completed solely by the members of your group. Sharing of code between groups is strictly prohibited. However, you are allowed to discuss general solution approaches or share publicly available resources with members of other groups. Therefore, clearly indicate which public resources you consulted and/or copied code from. Any plagiarism between groups will result in the initiation of a fraud procedure with the director of education.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "In this notebook, your group will implement a fully vectorized Deep Q-Network (DQN) agent and several of its key improvements, inspired by the [\"Rainbow\"](https://arxiv.org/abs/1710.02298) paper. The project is divided into a foundational group task followed by individual implementation of specific DQN extensions. Your goal is to compare the performance of these components and then combine them into the ultimate rainbow-lite agent.\n",
    "\n",
    "### Project Structure:\n",
    "1.  **Foundational (Group Task):** The entire group will first collaborate to right the necessary code to  make simple DQN work.\n",
    "2.  **Individual Extensions:** Once the baseline is complete, each group member will be assigned one of the following algorithmic extensions to implement:\n",
    "    *   **Extension A:** Double DQN & Dueling DQN (Architectural Improvements)\n",
    "    *   **Extension B:** Prioritized Experience Replay (PER) (Advanced Sampling)\n",
    "    *   **Extension C:** N-Step Returns & Noisy Networks (Target & Exploration Improvements)\n",
    "3.  **Analysis (Group Task):** The group will integrate all components, run a comparative analysis on the `breakout` environment, and collaboratively answer the conceptual and reflection questions in a final report.\n",
    "\n",
    "Cells with `TODO` indicate where you must add or adjust code. However feel free to modify any part of the code to improve clarity, efficiency, or performance. You are encouraged to experiment with hyperparameters and other design choices to optimize your agent's learning.\n",
    "\n",
    "---\n",
    "\n",
    "### Environment Tiers:\n",
    "You will use three types of environments from `pufferlib.ocean`:\n",
    "1.  **Debug:** `squared` (fast iterations; verify your code  is correct, your agent should learn within seconds and become optimal around 2 minutes, max episode score is 1).\n",
    "2.  **Comparing:** `cartpole` (fast iterations; verify your extensions and evaluate the agent, learning signs should be clear within 2 minutes but optimal agent could take 30+ minutes, max episode score is 199).\n",
    "3. **Challenge:** `breakout` (focus on performance, and diagnostics, an optimal agent can take multiple hours, you will get bonus points if you find an optimal agent in this environment, max episode score is 864).\n",
    "\n",
    "You are allowed to experiment with other environments from `pufferlib.ocean` if you wish, but the above three are mandatory.\n",
    "\n",
    "---\n",
    "\n",
    "### Report\n",
    "We expect you to write the report in a separate document and submit both the notebook and the report on Ufora. Remember that a plot often tells more than a thousand words. When you explain somethings or you show your results, try to add figures to accompany the text. Go beyond just souly describing what you did or how the techniques work but additionally, share your observations, reflect on why things turned out the way they did, and help us understand the story behind your findings.\n",
    "\n",
    "The report has a soft limit of 8 pages. You are welcome to go over this limit if you keep your writing concise and make sure any extra content is relevant. If you are highly motivated and want to test many different things, feel free to share your findings as long as you follow the requirements mentioned earlier.\n",
    "\n",
    "If your group is smaller than 3 people, you can choose to implement more than one extension per person. In that case, please clearly indicate who did what in the report.\n",
    "\n",
    "**Deadline:** October 26, 2025, 23:59\n",
    "\n",
    "\n",
    "### Office hours\n",
    "We will hold weekly office hours to help you with questions about the assignment. Your are more then welcome between 13:30 and 16:00. We reserved IDLab9 (IGent) for you. If we are not there, you can find us in our offices on the 10th floor (IGent): 200.026 (Elias and Thibault), 200.031 (Ciem).\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### Docker\n",
    "To ensure a consistent and hassle-free setup, we have prepared a Docker image with all necessary dependencies pre-installed. This is the recommended way to run the notebook to avoid issues with package versions.\n",
    "\n",
    "You can also use there container more information at [puffertank](https://github.com/PufferAI/PufferTank).\n",
    "\n",
    "The Docker image is available on Docker Hub:\n",
    "*   **Image:** `ciemcornelissen/puffer-notebook:latest`\n",
    "*   **URL:** [https://hub.docker.com/r/ciemcornelissen/puffer-notebook](https://hub.docker.com/r/ciemcornelissen/puffer-notebook)\n",
    "\n",
    "You can use this image in several ways. Below are instructions for three common setups: VS Code (recommended for local use), Deepnote for a cloud-based environment, and locally with classic Jupyter.\n",
    "\n",
    "\n",
    "### Option 1: Local Development with VS Code\n",
    "\n",
    "This is the most seamless way to work locally. VS Code's \"Dev Containers\" extension allows you to open your project folder directly inside the running container, giving you access to a fully integrated terminal, file editor, and Jupyter renderer.\n",
    "\n",
    "**Prerequisites:**\n",
    "*   [Docker Desktop](https://www.docker.com/products/docker-desktop/) installed and running.\n",
    "*   [Visual Studio Code](https://code.visualstudio.com/) installed.\n",
    "*   The [Dev Containers extension](https://marketplace.visualstudio.com/items?itemName=ms-vscode-remote.remote-containers) installed in VS Code.\n",
    "\n",
    "**Instructions:**\n",
    "\n",
    "1.  **Pull and Run the Container:** Open a terminal in your project directory (where this notebook is located) and run the appropriate command below. **Keep this terminal window open.**\n",
    "\n",
    "    *   **For Linux, Windows (WSL), and Intel-based Macs:**\n",
    "        ```bash\n",
    "        docker run -it --rm -p 8888:8888 -v \"$(pwd)\":/app --name puffer-dev ciemcornelissen/puffer-notebook:latest\n",
    "\n",
    "    *   **For Apple Silicon (M1/M2/M3) Macs:** You must add the `--platform` flag to emulate the correct architecture.\n",
    "        ```bash\n",
    "        docker run -it --rm --platform linux/amd64 -p 8888:8888 -v \"$(pwd)\":/app --name puffer-dev ciemcornelissen/puffer-notebook:latest\n",
    "\n",
    "    *   *(Note: We use `--name puffer-dev` to give the container an easy-to-find name).*\n",
    "\n",
    "2.  **Attach VS Code to the Container:**\n",
    "    *   Open VS Code.\n",
    "    *   Attach vsual studio code to the running container:\n",
    "        1.  Click on the container extension in VS Code.\n",
    "        2.  Select **\"Attach to Running Container...\"**.\n",
    "        3.  Choose the container named `puffer-dev`.\n",
    "    *   Or open the Command Palette (`Ctrl+Shift+P` on Windows/Linux, `Cmd+Shift+P` on Mac).\n",
    "    *   Type and select **\"Dev Containers: Attach to Running Container...\"**.\n",
    "    *   Choose the right container from the list.\n",
    "\n",
    "\n",
    "3.  **Start Working:** A new VS Code window will open, connected to the container. Click **\"Open Folder\"** to open your project files. You can now edit code, run the notebook, and use the terminal as if you were running natively inside the correct environment.\n",
    "\n",
    "**If you can not select a kernel when trying to run code in the notebook then you need to update the jupyter extension of vsc. This can be done by clicking on the extensions tab on the left and searching for jupyter. Then click on the little gear icon and select install a specific version and choose the newest version.**\n",
    "\n",
    "### Option 2: Cloud Development with [Deepnote](https://deepnote.com/)\n",
    "\n",
    "If you prefer not to install Docker locally, you can use Deepnote to run the environment in the cloud.\n",
    "\n",
    "**Instructions:**\n",
    "\n",
    "1.  In your Deepnote project, navigate to the **Environment** tab in the left sidebar at the bottem under machine.\n",
    "2.  Click on the **\"Set up a new Docker image\"**.\n",
    "3.  In the \"Docker image\" field, paste the image name:\n",
    "    ```\n",
    "    ciemcornelissen/puffer-notebook:latest\n",
    "\n",
    "4.  Deepnote will pull the image. Once it's ready, you'll be ready to work.\n",
    "\n",
    "\n",
    "\n",
    "### Option 3: Local Development with Classic Jupyter\n",
    "\n",
    "This method uses the command line to start a Jupyter server, which you access through your web browser.\n",
    "\n",
    "**Instructions:**\n",
    "\n",
    "1.  **Pull the Image:**\n",
    "    ```bash\n",
    "    docker pull ciemcornelissen/puffer-notebook:latest\n",
    "\n",
    "2.  **Run the Container:** Run the command below from your project directory.\n",
    "\n",
    "    *   **For Linux, Windows (WSL), and Intel-based Macs:**\n",
    "        ```bash\n",
    "        docker run -it --rm -p 8888:8888 -v \"$(pwd)\":/app ciemcornelissen/puffer-notebook:latest\n",
    "\n",
    "    *   **For Apple Silicon (M1/M2/M3) Macs:**\n",
    "        ```bash\n",
    "        docker run -it --rm --platform linux/amd64 -p 8888:8888 -v \"$(pwd)\":/app ciemcornelissen/puffer-notebook:latest\n",
    "\n",
    "3.  **Access Jupyter:** Your terminal will display a URL (e.g., `http://127.0.0.1:8888/lab?token=...`). Copy and paste this full URL into your web browser to start the notebook. Your files will be in the `/app` directory.\n",
    "\n",
    "\n",
    "\n",
    "### **Important Note for Apple Silicon (M1/M2/M3/M4) Users**\n",
    "\n",
    "The Docker image is built for the `amd64` (Intel/AMD) architecture. If you are using a Mac with Apple Silicon, you must tell Docker to emulate this architecture by adding the `--platform linux/amd64` flag.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#when using deepnote and you get the error no module pufferlib run this cell\n",
    "\n",
    "# try:\n",
    "#     import __editable___pufferlib_3_0_0_finder as _pf\n",
    "#     _pf.install()          # registers the module loader\n",
    "#     import pufferlib\n",
    "#     print(\"Loaded:\", pufferlib.__file__)\n",
    "# except Exception as e:\n",
    "#     print(\"Failed to manually activate editable hook:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "import random, dataclasses\n",
    "import math\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Optional, Dict, Any\n",
    "from collections import deque\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Import ocean environments, you can import more if you want to experiment with other envs\n",
    "from pufferlib.ocean.squared import squared\n",
    "from pufferlib.ocean.pong import pong\n",
    "from pufferlib.ocean.pacman import pacman\n",
    "from pufferlib.ocean.enduro import enduro\n",
    "from pufferlib.ocean.tetris import tetris\n",
    "from pufferlib.ocean.breakout import breakout\n",
    "from pufferlib.ocean.cartpole import cartpole\n",
    "\n",
    "import gymnasium as gym\n",
    "\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding PufferLib and Ocean Environments\n",
    "\n",
    "### What is [PufferLib](https://puffer.ai/ocean.html)?\n",
    "\n",
    "**PufferLib** is a high-performance reinforcement learning framework designed to bridge the gap between research and production RL. It was created to address common pain points in RL development:\n",
    "\n",
    "- **Speed:** PufferLib environments are highly optimized, often running 10-100x faster than traditional implementations\n",
    "- **Scalability:** Built-in support for massive parallelization across thousands of environments\n",
    "- **Simplicity:** Clean, minimal API that follows Gymnasium standards\n",
    "- **GPU-Native:** Environments can run directly on GPU, eliminating CPU-GPU transfer bottlenecks\n",
    "\n",
    "\n",
    "### PufferLib Ocean: Educational RL Environments\n",
    "\n",
    "**Ocean** is PufferLib's collection of lightweight, educational environments. The name reflects its purpose: a \"sea\" of diverse environments for learning and experimentation. Ocean environments are specifically designed for:\n",
    "\n",
    "1. **Fast Debugging:** Quickly verify your algorithm works before scaling up\n",
    "2. **Rapid Prototyping:** Test new ideas without waiting hours for results\n",
    "3. **Educational Clarity:** Simpler codebases that are easier to understand and modify\n",
    "4. **Vectorization by Default:** Learn modern RL practices from the start\n",
    "\n",
    "### Available Ocean Environments\n",
    "\n",
    "Ocean includes a variety of environments with different characteristics (more can be found in the [docs](https://github.com/PufferAI/PufferLib/tree/3.0/pufferlib/ocean)):\n",
    "\n",
    "- **Classic Control:** `cartpole`\n",
    "  - Simple physics simulations\n",
    "  - Low-dimensional observations\n",
    "  - Great for debugging and initial testing\n",
    "\n",
    "- **Grid Worlds:** `squared`, `minigrid_variants`\n",
    "  - Discrete state/action spaces\n",
    "  - Fast iteration times\n",
    "  - Perfect for verifying algorithm correctness\n",
    "\n",
    "- **Atari-Style:** `breakout`, `pong`, `pacman`, `enduro`\n",
    "  - More complex visual observations\n",
    "  - Longer training times\n",
    "  - Closer to real-world RL challenges\n",
    "\n",
    "- **Puzzle Games:** `tetris`\n",
    "  - Strategic planning required\n",
    "  - Sparse rewards\n",
    "  - Advanced challenge tasks\n",
    "\n",
    "### Why PufferLib for This Assignment?\n",
    "\n",
    "We chose PufferLib Ocean for several pedagogical reasons:\n",
    "\n",
    "1. **Immediate Feedback:** Fast environments mean you can iterate quickly on your code\n",
    "2. **Realistic Scale:** The environment enables easy vectorized environments like in real RL research\n",
    "3. **Low Hardware Requirements:** Efficient implementation means you don't need expensive GPUs because of vectorised observations\n",
    "4. **Clear Progression:** From simple (Squared) to complex (Breakout) in the same framework\n",
    "5. **Industry Relevance:** PufferLib is used in actual RL research and applications\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Environment Factories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Debug env observation shape: (1, 121)\n",
      "Action space: MultiDiscrete([5])\n",
      "Observation space: Box(0, 1, (1, 121), uint8)\n"
     ]
    }
   ],
   "source": [
    "class TimeLimitVec:\n",
    "    \"\"\"\n",
    "    Generic vector env time-limit wrapper for PufferLib/Gymnasium-like envs.docker run -it --rm -p 8888:8888 -v \"$(pwd)\":/app ciemcornelissen/puffer-notebook:latest\n",
    "    - Marks truncations True when per-env step count reaches max_episode_steps.\n",
    "    - Preserves existing terminals/truncations from the underlying env.\n",
    "    - Resets per-env counters on reset or when an env ends.\n",
    "    \"\"\"\n",
    "    def __init__(self, env: Any, max_episode_steps: int, info_key: str = \"time_limit\"):\n",
    "        assert max_episode_steps and max_episode_steps > 0\n",
    "        self.env = env\n",
    "        self.max_episode_steps = int(max_episode_steps)\n",
    "        self.info_key = info_key\n",
    "\n",
    "        # Mirror common attributes for compatibility\n",
    "        self.single_observation_space = getattr(env, \"single_observation_space\", None)\n",
    "        self.single_action_space = getattr(env, \"single_action_space\", None)\n",
    "        self.observation_space = getattr(env, \"observation_space\", None)\n",
    "        self.action_space = getattr(env, \"action_space\", None)\n",
    "        self.num_agents = getattr(env, \"num_agents\", 1)\n",
    "\n",
    "        self._steps = np.zeros(self.num_agents, dtype=np.int64)\n",
    "\n",
    "    def reset(self, seed: Optional[int] = 0):\n",
    "        obs, infos = self.env.reset(seed)\n",
    "        self._steps[...] = 0\n",
    "        return obs, infos\n",
    "\n",
    "    # def reset(self, seed: Optional[int] = None): \n",
    "    #     if seed is None:\n",
    "    #         seed = random.randint(1, 2**32 - 1)\n",
    "    #     obs, infos = self.env.reset(seed)\n",
    "    #     self._steps[...] = 0\n",
    "    #     return obs, infos\n",
    "\n",
    "\n",
    "    def step(self, actions):\n",
    "        obs, rewards, terminals, truncations, infos = self.env.step(actions)\n",
    "\n",
    "        t = np.asarray(terminals, dtype=bool)\n",
    "        tr = np.asarray(truncations, dtype=bool)\n",
    "        if t.ndim == 0:\n",
    "            t = t.reshape(1)\n",
    "        if tr.ndim == 0:\n",
    "            tr = tr.reshape(1)\n",
    "\n",
    "        active = ~(t | tr)\n",
    "        # Increment only for envs still active before this step's end flags\n",
    "        self._steps[active] += 1\n",
    "\n",
    "        # Apply time limit where not already ended this step\n",
    "        timeouts = (self._steps >= self.max_episode_steps) & active\n",
    "        if np.any(timeouts):\n",
    "            tr = np.logical_or(tr, timeouts)\n",
    "            self._steps[timeouts] = 0\n",
    "            if infos is None:\n",
    "                infos = []\n",
    "            if not isinstance(infos, list):\n",
    "                infos = [infos]\n",
    "            infos.append({self.info_key: {\"timeouts\": timeouts.copy()}})\n",
    "\n",
    "        # Also reset counters for any envs that ended naturally\n",
    "        ended = t | tr\n",
    "        if np.any(ended):\n",
    "            self._steps[ended] = 0\n",
    "\n",
    "        return obs, rewards, t, tr, infos\n",
    "\n",
    "    def render(self, *args, **kwargs):\n",
    "        return getattr(self.env, \"render\")(*args, **kwargs)\n",
    "\n",
    "    def close(self):\n",
    "        return getattr(self.env, \"close\")()\n",
    "def make_env(name: str, seed: int = 0):\n",
    "    env_map = {\n",
    "        'squared': squared,\n",
    "        'pong': pong,\n",
    "        'pacman': pacman,\n",
    "        'enduro': enduro,\n",
    "        'tetris': tetris,\n",
    "        'breakout': breakout,\n",
    "        'cartpole': cartpole\n",
    "    }\n",
    "    if name not in env_map:\n",
    "        raise ValueError(f'Unknown environment {name}')\n",
    "\n",
    "    def thunk():\n",
    "        # Get the module from the map\n",
    "        env_module = env_map[name]\n",
    "\n",
    "        # For example, in the 'squared' module, there is a 'Squared' class.\n",
    "        env_class_name = name.capitalize()\n",
    "        env_class = getattr(env_module, env_class_name)\n",
    "\n",
    "        raw_env = env_class(num_envs=1, render_mode=None, seed=seed)  # Instantiate the class\n",
    "        env = TimeLimitVec(raw_env, max_episode_steps=10_000)\n",
    "        env.reset(seed=seed)\n",
    "        return env\n",
    "    return thunk\n",
    "\n",
    "debug_env_name = 'squared'\n",
    "intermediate_env_name = 'cartpole'  # Change to 'pacman', 'enduro', or 'tetris' if desired\n",
    "challenge_env_name = 'breakout'\n",
    "\n",
    "test_env = make_env(debug_env_name)()\n",
    "obs, info = test_env.reset()\n",
    "print('Debug env observation shape:', np.array(obs).shape)\n",
    "print('Action space:', test_env.action_space)\n",
    "print('Observation space:', test_env.observation_space)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Configuration\n",
    "Adjust hyperparameters and add hyperparameters as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Config(seed=7, gamma=0.98, lr=0.0001, batch_size=128, buffer_capacity=5000, min_buffer_size=1000, max_steps_per_env=200000, max_episode_len=1000, epsilon_start=1.0, epsilon_end=0.01, epsilon_decay_steps=100000, target_update_interval=100, train_freq=4, gradient_clip=10.0, reward_clip=None, use_double_dqn=False, use_dueling=False, vector_envs=1, log_interval_episodes=10, eval_episodes=3, curriculum=['cartpole'], hidden_size=512, per_alpha=0.6, per_eps=0.01, per_beta_start=0.4, per_beta_end=1.0, per_beta_steps=50000, use_per=True, n_steps=3, use_noisy_layer=False)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@dataclass\n",
    "class Config:\n",
    "    seed: int = 7\n",
    "    gamma: float = 0.98                # prioritize short-term rewards\n",
    "    lr: float = 1e-4                   # can reduce to 1e-4 if unstable\n",
    "    batch_size: int = 128\n",
    "    buffer_capacity: int = 5000      # larger buffer for diverse data\n",
    "    min_buffer_size: int = 1000       # start training with more data\n",
    "    max_steps_per_env: int = 200_000\n",
    "    max_episode_len: int = 1000\n",
    "    epsilon_start: float = 1.0\n",
    "    epsilon_end: float = 0.01\n",
    "    epsilon_decay_steps: int = 100_000\n",
    "    target_update_interval: int = 100\n",
    "    train_freq: int = 4\n",
    "    gradient_clip: float = 10.0\n",
    "    reward_clip: Optional[float] = None\n",
    "    use_double_dqn: bool = False\n",
    "    use_dueling: bool = False           # enabled for better value estimation\n",
    "    vector_envs: int = 1\n",
    "    log_interval_episodes: int = 10\n",
    "    eval_episodes: int = 3\n",
    "    curriculum: List[str] = None\n",
    "    hidden_size: int = 512             # larger network for complex patterns\n",
    "\n",
    "    # these 5 hyperparameters are used for the PER (prioritized experience replay)\n",
    "    per_alpha: float = 0.6 \n",
    "    per_eps: float = 0.01\n",
    "    per_beta_start: float = 0.4\n",
    "    per_beta_end: float = 1.0\n",
    "    per_beta_steps: int = 50_000\n",
    "    use_per: bool = True\n",
    "    \n",
    "    n_steps: int = 3\n",
    "    use_noisy_layer: bool = False\n",
    "\n",
    "cfg = Config()\n",
    "cfg.curriculum = [intermediate_env_name]\n",
    "\n",
    "random.seed(cfg.seed)\n",
    "np.random.seed(cfg.seed)\n",
    "torch.manual_seed(cfg.seed)\n",
    "cfg\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Replay Buffer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing ReplayBuffer...\n",
      "✓ ReplayBuffer tests passed!\n",
      "[27. 16. 11.  9.  7. 11.  0.  7.  2.  3.  4.  5.  6.  0.  0.]\n",
      "Total: 27.0\n",
      "v=2.06 -> leaf=7, data_idx=0, priority=7.0\n",
      "v=21.06 -> leaf=12, data_idx=5, priority=6.0\n",
      "v=11.84 -> leaf=9, data_idx=2, priority=3.0\n",
      "v=19.53 -> leaf=11, data_idx=4, priority=5.0\n",
      "v=26.41 -> leaf=12, data_idx=5, priority=6.0\n"
     ]
    }
   ],
   "source": [
    "# SumTree used to store the transitions used for the prioritized replay buffer\n",
    "\n",
    "# round up capacity to nearest power of 2 so you always have a full and balanced tree, and makes the implementation slightly easier\n",
    "def next_power_of_two(x):\n",
    "    return 1 << (x-1).bit_length()\n",
    "\n",
    "class SumTree:\n",
    "    def __init__(self, capacity):\n",
    "        self.real_capacity = capacity\n",
    "        self.capacity = next_power_of_two(capacity)\n",
    "        self.tree = np.zeros(2 * self.capacity - 1, dtype=np.float32)\n",
    "        self.position = 0\n",
    "\n",
    "    def add(self, priority):\n",
    "        tree_idx = self.position + self.capacity - 1\n",
    "        self.update(tree_idx, priority)\n",
    "        self.position = (self.position + 1) % self.real_capacity\n",
    "\n",
    "    def update(self, tree_idx, priority):\n",
    "        change = priority - self.tree[tree_idx]\n",
    "        self.tree[tree_idx] = priority\n",
    "        while tree_idx != 0:\n",
    "            tree_idx = (tree_idx - 1) // 2\n",
    "            self.tree[tree_idx] += change\n",
    "\n",
    "    def get_leaf(self, v):\n",
    "        parent_idx = 0\n",
    "        while True:\n",
    "            left_child = 2 * parent_idx + 1\n",
    "            right_child = left_child + 1\n",
    "            if left_child >= len(self.tree):\n",
    "                leaf_idx = parent_idx\n",
    "                break\n",
    "            if v <= self.tree[left_child]:\n",
    "                parent_idx = left_child\n",
    "            else:\n",
    "                v -= self.tree[left_child]\n",
    "                parent_idx = right_child\n",
    "\n",
    "        data_idx = leaf_idx - self.capacity + 1\n",
    "        return leaf_idx, self.tree[leaf_idx], data_idx\n",
    "\n",
    "    def total_priority(self):\n",
    "        return self.tree[0]\n",
    "\n",
    "    def leaf_from_data_idx(self, i: int) -> int:\n",
    "        return i + self.capacity - 1\n",
    "\n",
    "    def update_leaf(self, leaf_idx: int, priority: float):\n",
    "        self.update(leaf_idx, priority)\n",
    "\n",
    "\n",
    "class PrioritizedReplayBuffer:\n",
    "    def __init__(self, capacity: int, obs_shape: tuple, device, cfg):\n",
    "        self.cfg = cfg\n",
    "        self.capacity = capacity\n",
    "        self.device = device\n",
    "        self.obs_shape = obs_shape\n",
    "        self.position = 0\n",
    "        self.size = 0\n",
    "        self.frame = 0\n",
    "        self.max_priority = 1.0\n",
    "\n",
    "        # PER hyperparameters from cfg\n",
    "        self.alpha = cfg.per_alpha\n",
    "        self.eps   = cfg.per_eps\n",
    "        self.beta_start = cfg.per_beta_start\n",
    "        self.beta_end   = cfg.per_beta_end\n",
    "        self.beta_steps = cfg.per_beta_steps\n",
    "\n",
    "        self.obs = np.zeros((capacity, *obs_shape), dtype=np.float32)\n",
    "        self.actions = np.zeros((capacity, 1), dtype=np.int64)\n",
    "        self.rewards = np.zeros((capacity, 1), dtype=np.float32)\n",
    "        self.next_obs = np.zeros((capacity, *obs_shape), dtype=np.float32)\n",
    "        self.dones = np.zeros((capacity, 1), dtype=np.float32)\n",
    "\n",
    "        self.tree = SumTree(capacity)\n",
    "\n",
    "    def __len__(self): \n",
    "        return self.size\n",
    "\n",
    "    def _beta(self):\n",
    "        t = min(1.0, self.frame / float(self.beta_steps))\n",
    "        return self.beta_start + t * (self.beta_end - self.beta_start)\n",
    "\n",
    "    def _set_priority_by_data_idx(self, data_idx, prio):\n",
    "        leaf = self.tree.leaf_from_data_idx(data_idx)\n",
    "        self.tree.update_leaf(leaf, prio)\n",
    "\n",
    "    def push(self, obs, action, reward, next_obs, done):\n",
    "        i = self.position\n",
    "        self.obs[i] = obs\n",
    "        self.actions[i, 0] = int(action)\n",
    "        self.rewards[i, 0] = float(reward)\n",
    "        self.next_obs[i] = next_obs\n",
    "        self.dones[i, 0] = float(done)\n",
    "\n",
    "        self._set_priority_by_data_idx(i, self.max_priority)\n",
    "\n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "        self.size = min(self.size + 1, self.capacity)\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        assert self.size >= batch_size, \"PER buffer not ready\"\n",
    "        self.frame += 1\n",
    "        beta = self._beta()\n",
    "\n",
    "        total = self.tree.total_priority()\n",
    "        if total <= 1e-12 or not np.isfinite(total):\n",
    "            idxs = np.random.choice(self.size, batch_size, replace=False)\n",
    "            probs = np.full((batch_size,), 1.0 / self.size, dtype=np.float32)\n",
    "            leaf_idx = np.array([self.tree.leaf_from_data_idx(i) for i in idxs], dtype=np.int64)\n",
    "        else:\n",
    "            segment = total / batch_size\n",
    "            leaf_idx, idxs, prios = [], [], []\n",
    "            for k in range(batch_size):\n",
    "                v = np.random.uniform(k * segment, (k + 1) * segment)\n",
    "                leaf, p, di = self.tree.get_leaf(v)\n",
    "                while di >= self.capacity:  # skip ghost leaves\n",
    "                    v = np.random.uniform(0.0, total)\n",
    "                    leaf, p, di = self.tree.get_leaf(v)\n",
    "                leaf_idx.append(leaf); idxs.append(di); prios.append(p)\n",
    "            leaf_idx = np.asarray(leaf_idx, dtype=np.int64)\n",
    "            idxs = np.asarray(idxs, dtype=np.int64)\n",
    "            probs = np.asarray(prios, dtype=np.float32) / (total + 1e-12)\n",
    "\n",
    "        N = self.size\n",
    "        \n",
    "        safe_probs = np.clip(probs, 1e-6, 1.0)\n",
    "        w = (N * safe_probs) ** (-beta)\n",
    "        w = w / (np.max(w) + 1e-6)\n",
    "        w = w.astype(np.float32).reshape(-1, 1)\n",
    "\n",
    "        obs = torch.as_tensor(self.obs[idxs], dtype=torch.float32, device=self.device)\n",
    "        actions = torch.as_tensor(self.actions[idxs], dtype=torch.long, device=self.device)\n",
    "        rewards = torch.as_tensor(self.rewards[idxs], dtype=torch.float32, device=self.device)\n",
    "        next_obs = torch.as_tensor(self.next_obs[idxs], dtype=torch.float32, device=self.device)\n",
    "        dones = torch.as_tensor(self.dones[idxs], dtype=torch.float32, device=self.device)\n",
    "        is_w = torch.as_tensor(w, dtype=torch.float32, device=self.device)\n",
    "\n",
    "        return obs, actions, rewards, next_obs, dones, is_w, leaf_idx\n",
    "\n",
    "    def update_priorities(self, leaf_idx: np.ndarray, td_errors: np.ndarray):\n",
    "        td = np.abs(np.asarray(td_errors).reshape(-1))\n",
    "        new_p = (td + self.eps) ** self.alpha\n",
    "        self.max_priority = max(self.max_priority, float(new_p.max()))\n",
    "        for li, p in zip(leaf_idx.astype(int), new_p.astype(float)):\n",
    "            self.tree.update_leaf(li, p)\n",
    "\n",
    "\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity: int, obs_shape: tuple, device):\n",
    "        self.capacity = capacity\n",
    "        self.device = device\n",
    "        self.obs_shape = obs_shape\n",
    "        self.position = 0\n",
    "        self.size = 0\n",
    "        \n",
    "        # Pre-allocate numpy arrays for storage\n",
    "        self.obs = np.zeros((capacity, *obs_shape), dtype=np.float32)\n",
    "        self.actions = np.zeros((capacity, 1), dtype=np.int64)  # Shape: (capacity, 1)\n",
    "        self.rewards = np.zeros((capacity, 1), dtype=np.float32)\n",
    "        self.next_obs = np.zeros((capacity, *obs_shape), dtype=np.float32)\n",
    "        self.dones = np.zeros((capacity, 1), dtype=np.float32)\n",
    "    \n",
    "    def push(self, obs, action, reward, next_obs, done):\n",
    "        \"\"\"Store a transition\"\"\"\n",
    "        self.obs[self.position] = obs\n",
    "        self.actions[self.position, 0] = action  # Store as scalar in (1,) shaped array\n",
    "        self.rewards[self.position, 0] = reward\n",
    "        self.next_obs[self.position] = next_obs\n",
    "        self.dones[self.position, 0] = float(done)\n",
    "        \n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "        self.size = min(self.size + 1, self.capacity)\n",
    "    \n",
    "    def sample(self, batch_size: int):\n",
    "        \"\"\"Sample a batch of transitions and convert to PyTorch tensors\"\"\"\n",
    "        indices = np.random.choice(self.size, batch_size, replace=False)\n",
    "        \n",
    "        # Convert to tensors - actions will have shape (batch_size, 1)\n",
    "        obs = torch.FloatTensor(self.obs[indices]).to(self.device)\n",
    "        actions = torch.LongTensor(self.actions[indices]).to(self.device)  # (batch_size, 1)\n",
    "        rewards = torch.FloatTensor(self.rewards[indices]).to(self.device)\n",
    "        next_obs = torch.FloatTensor(self.next_obs[indices]).to(self.device)\n",
    "        dones = torch.FloatTensor(self.dones[indices]).to(self.device)\n",
    "        \n",
    "        return obs, actions, rewards, next_obs, dones\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.size\n",
    "\n",
    "# Test ReplayBuffer\n",
    "print(\"Testing ReplayBuffer...\")\n",
    "test_buffer = ReplayBuffer(100, (4,), DEVICE)\n",
    "test_buffer.push(np.array([1,2,3,4]), 0, 1.0, np.array([2,3,4,5]), False)\n",
    "assert len(test_buffer) == 1, \"Buffer should have 1 element\"\n",
    "obs, actions, rewards, next_obs, dones = test_buffer.sample(1)\n",
    "assert obs.shape == (1, 4), f\"Expected shape (1, 4), got {obs.shape}\"\n",
    "print(\"✓ ReplayBuffer tests passed!\")\n",
    "\n",
    "tree = SumTree(6)\n",
    "for p in [1, 2, 3, 4, 5, 6, 7]:\n",
    "    tree.add(p)\n",
    "\n",
    "print(tree.tree)                # should contain cumulative sums\n",
    "print(\"Total:\", tree.total_priority())  # -> 10.0\n",
    "\n",
    "# Try sampling\n",
    "for _ in range(5):\n",
    "    v = np.random.uniform(0, tree.total_priority())\n",
    "    leaf, p, idx = tree.get_leaf(v)\n",
    "    print(f\"v={v:.2f} -> leaf={leaf}, data_idx={idx}, priority={p}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5 Epsilon Schedule\n",
    "Linear decay from start to end over `epsilon_decay_steps`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epsilon(0)= 1.0 Epsilon(mid)= 0.505 Epsilon(end)= 0.01\n"
     ]
    }
   ],
   "source": [
    "def epsilon_by_step(step: int, cfg: Config) -> float:\n",
    "    per_step = (cfg.epsilon_start-cfg.epsilon_end)/cfg.epsilon_decay_steps\n",
    "    if step < cfg.epsilon_decay_steps:\n",
    "        eps = cfg.epsilon_start - (step*per_step)\n",
    "    else:\n",
    "        eps = cfg.epsilon_end\n",
    "    return eps\n",
    "\n",
    "print('Epsilon(0)=', epsilon_by_step(0, cfg), 'Epsilon(mid)=', epsilon_by_step(cfg.epsilon_decay_steps//2, cfg), 'Epsilon(end)=', epsilon_by_step(cfg.epsilon_decay_steps*2, cfg))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.6 Q-Network Architectures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class NoisyLinear(nn.Module):\n",
    "    \"\"\"\n",
    "    Factorized NoisyNet linear layer (Fortunato et al., 2018).\n",
    "    Samples noise during training; uses deterministic weights in eval mode.\n",
    "\n",
    "    Args:\n",
    "        in_features (int): input feature size\n",
    "        out_features (int): output feature size\n",
    "        sigma0 (float): initial scale for noise parameters\n",
    "        bias (bool): include bias term\n",
    "        auto_reset (bool): resample noise every forward() in training mode\n",
    "    \"\"\"\n",
    "    def __init__(self, in_features: int, out_features: int, sigma0: float = 0.5, bias: bool = True, auto_reset: bool = True):\n",
    "        super().__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.sigma0 = sigma0\n",
    "        self.auto_reset = auto_reset\n",
    "\n",
    "        # Learnable parameters (mu and sigma)\n",
    "        self.weight_mu = nn.Parameter(torch.empty(out_features, in_features))\n",
    "        self.weight_sigma = nn.Parameter(torch.empty(out_features, in_features))\n",
    "        if bias:\n",
    "            self.bias_mu = nn.Parameter(torch.empty(out_features))\n",
    "            self.bias_sigma = nn.Parameter(torch.empty(out_features))\n",
    "        else:\n",
    "            self.register_parameter(\"bias_mu\", None)\n",
    "            self.register_parameter(\"bias_sigma\", None)\n",
    "\n",
    "        # Buffers for factorized noise\n",
    "        self.register_buffer(\"eps_in\", torch.empty(1, in_features))\n",
    "        self.register_buffer(\"eps_out\", torch.empty(out_features, 1))\n",
    "\n",
    "        self.reset_parameters()\n",
    "        self.reset_noise()\n",
    "\n",
    "    def reset_parameters(self) -> None:\n",
    "        mu_range = 1.0 / math.sqrt(self.in_features)\n",
    "        self.weight_mu.data.uniform_(-mu_range, mu_range)\n",
    "        self.weight_sigma.data.fill_(self.sigma0 / math.sqrt(self.in_features))\n",
    "        if self.bias_mu is not None:\n",
    "            self.bias_mu.data.uniform_(-mu_range, mu_range)\n",
    "            self.bias_sigma.data.fill_(self.sigma0 / math.sqrt(self.out_features))\n",
    "\n",
    "    @staticmethod\n",
    "    def _scale_noise(size, device):\n",
    "        # f(epsilon) = sign(epsilon) * sqrt(|epsilon|)\n",
    "        x = torch.randn(size, device=device)\n",
    "        return x.sign().mul_(x.abs().sqrt_())\n",
    "\n",
    "    def reset_noise(self) -> None:\n",
    "        device = self.weight_mu.device\n",
    "        self.eps_in.copy_(self._scale_noise((1, self.in_features), device))\n",
    "        self.eps_out.copy_(self._scale_noise((self.out_features, 1), device))\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        if self.training:\n",
    "            if self.auto_reset:\n",
    "                self.reset_noise()\n",
    "            weight = self.weight_mu + self.weight_sigma * (self.eps_out @ self.eps_in)\n",
    "            bias = None\n",
    "            if self.bias_mu is not None:\n",
    "                bias = self.bias_mu + self.bias_sigma * self.eps_out.squeeze(1)\n",
    "        else:\n",
    "            weight = self.weight_mu\n",
    "            bias = self.bias_mu if self.bias_mu is not None else None\n",
    "\n",
    "        return F.linear(x, weight, bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obs space: 121\n",
      "MLPQ(\n",
      "  (feature): Sequential(\n",
      "    (0): Linear(in_features=121, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (3): ReLU()\n",
      "  )\n",
      "  (q_head): Linear(in_features=512, out_features=5, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class MLPQ(nn.Module):\n",
    "    def __init__(self, obs_dim: int, action_dim: int, dueling=False):\n",
    "        super().__init__()\n",
    "        self.hidden_size = cfg.hidden_size  # Typically 128, 256, or 512\n",
    "        self.dueling = dueling\n",
    "        self.action_dim = action_dim\n",
    "\n",
    "        final_layer = (\n",
    "            NoisyLinear(self.hidden_size, action_dim)\n",
    "            if cfg.use_noisy_layer\n",
    "            else nn.Linear(self.hidden_size, action_dim)\n",
    "        )\n",
    "\n",
    "        # Shared feature extractor\n",
    "        self.feature = nn.Sequential(\n",
    "            nn.Linear(obs_dim, self.hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(self.hidden_size, self.hidden_size),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        # Define output heads\n",
    "        if not dueling:\n",
    "            self.q_head = final_layer\n",
    "        else:\n",
    "            self.value_stream = nn.Sequential(\n",
    "                nn.Linear(self.hidden_size, self.hidden_size),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(self.hidden_size, 1),\n",
    "            )\n",
    "            self.advantage_stream = nn.Sequential(\n",
    "                nn.Linear(self.hidden_size, self.hidden_size),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(self.hidden_size, action_dim),\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = self.feature(x)\n",
    "        if not self.dueling:\n",
    "            return self.q_head(features)\n",
    "        else:\n",
    "            value = self.value_stream(features)\n",
    "            advantages = self.advantage_stream(features)\n",
    "            q_values = value + (advantages - advantages.mean(dim=1, keepdim=True))\n",
    "            return q_values\n",
    "\n",
    "\n",
    "def build_q_network(env, cfg: Config):\n",
    "    obs_space = env.single_observation_space\n",
    "    print('Obs space:', obs_space.shape[0])\n",
    "    act_space = env.single_action_space\n",
    "    action_dim = act_space.n\n",
    "    assert isinstance(act_space, gym.spaces.Discrete), 'DQN requires discrete actions'\n",
    "    if len(obs_space.shape) == 1:\n",
    "        return MLPQ(obs_space.shape[0], action_dim, dueling=cfg.use_dueling)\n",
    "\n",
    "\n",
    "# Quick test on debug env\n",
    "net_test_env = test_env\n",
    "test_net = build_q_network(net_test_env, cfg).to(DEVICE)\n",
    "print(test_net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.7 Action Selection\n",
    "Standard epsilon-greedy policy. (Later you might incorporate noisy networks or exploration bonuses.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random/greedy test action: [4]\n"
     ]
    }
   ],
   "source": [
    "@torch.no_grad()\n",
    "def select_action(q_net, obs, epsilon: float, env):\n",
    "    if random.random() < epsilon:\n",
    "        return env.action_space.sample()\n",
    "    if isinstance(obs, torch.Tensor):\n",
    "        obs_t = obs.to(DEVICE, dtype=torch.float32, non_blocking=True)\n",
    "    else:\n",
    "        obs_t = torch.as_tensor(obs, dtype=torch.float32, device=DEVICE)\n",
    "    q_vals = q_net(obs_t)\n",
    "    return int(q_vals.argmax(dim=1).item())\n",
    "\n",
    "# Test call\n",
    "dummy_action = select_action(test_net, obs, 1.0, test_env)\n",
    "print('Random/greedy test action:', dummy_action)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.8 Training Step\n",
    "Compute standard DQN loss or Double DQN target if `cfg.use_double_dqn=True`.\n",
    "\n",
    "Target for vanilla DQN:\n",
    "$$ y = r + (1-d) \\gamma \\max_{a'} Q_{target}(s', a') $$\n",
    "\n",
    "Double DQN variant:\n",
    "$$ a^* = \\arg\\max_{a'} Q_{online}(s', a') \\quad; \\quad y = r + (1-d) \\gamma Q_{target}(s', a^*) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.SmoothL1Loss()\n",
    "\n",
    "def train_step(q_net, target_net, optimizer, replay: ReplayBuffer, cfg: Config):\n",
    "    \"\"\"\n",
    "    Performs one training step of DQN.\n",
    "    \n",
    "    Args:\n",
    "        q_net: Online Q-network (being trained)\n",
    "        target_net: Target Q-network (frozen, updated periodically)\n",
    "        optimizer: Optimizer for q_net\n",
    "        replay: Replay buffer containing transitions\n",
    "        cfg: Configuration object\n",
    "    \n",
    "    Returns:\n",
    "        Loss value or None if buffer not ready\n",
    "    \"\"\"\n",
    "    # Don't train until we have enough experiences\n",
    "    if len(replay) < cfg.min_buffer_size:\n",
    "        return None\n",
    "    \n",
    "    sample = replay.sample(cfg.batch_size)\n",
    "    if len(sample) == 7:\n",
    "        obs, actions, rewards, next_obs, dones, is_w, handle_idx = sample\n",
    "    else:\n",
    "        obs, actions, rewards, next_obs, dones = sample\n",
    "        is_w, handle_idx = None, None\n",
    "        \n",
    "    q_values = q_net(obs)  # Shape: (batch_size, action_dim)\n",
    "\n",
    "    q_sa = q_values.gather(1, actions)  # Shape: (batch_size, 1)\n",
    "    \n",
    "    # ============= Compute Target Q-values =============\n",
    "    with torch.no_grad():  # Don't compute gradients for target\n",
    "        if cfg.use_double_dqn:\n",
    "            \n",
    "            # Step 1: Use online network to find best action in next state\n",
    "            next_q_values_online = q_net(next_obs)  # Shape: (batch_size, action_dim)\n",
    "            best_actions = next_q_values_online.argmax(dim=1, keepdim=True)  # Shape: (batch_size, 1)\n",
    "            \n",
    "            # Step 2: Use target network to evaluate that action\n",
    "            next_q_values_target = target_net(next_obs)  # Shape: (batch_size, action_dim)\n",
    "            next_q_value = next_q_values_target.gather(1, best_actions)  # Shape: (batch_size, 1)\n",
    "            \n",
    "        else:\n",
    "            # Vanilla DQN: Use target network for both selection and evaluation\n",
    "            next_q_values = target_net(next_obs)  # Shape: (batch_size, action_dim)\n",
    "            next_q_value = next_q_values.max(dim=1, keepdim=True)[0]  # Shape: (batch_size, 1)\n",
    "            # Note: max returns (values, indices), we take [0] for values\n",
    "        \n",
    "        # Compute TD target: y = r + γ * max Q(s', a') * (1 - done)\n",
    "        # The (1 - dones) term zeros out the next_q_value if episode ended\n",
    "        target = rewards + (cfg.gamma ** cfg.n_steps) * next_q_value * (1 - dones)  # Shape: (batch_size, 1)\n",
    "\n",
    "    per_sample_loss = loss_fn(q_sa, target)\n",
    "    if is_w is not None:\n",
    "        per_sample_loss = per_sample_loss * is_w\n",
    "    loss = per_sample_loss.mean()\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    nn.utils.clip_grad_norm_(q_net.parameters(), cfg.gradient_clip)\n",
    "    optimizer.step()\n",
    "\n",
    "    # update priorities with new TD values\n",
    "    if handle_idx is not None:\n",
    "        with torch.no_grad():\n",
    "            td_errors = (q_sa - target).abs().cpu().numpy()\n",
    "        replay.update_priorities(handle_idx, td_errors)\n",
    "    \n",
    "    return float(loss.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.9 Reward Processing\n",
    "Add optional reward clipping to stabilize training on environments with high or varied reward magnitudes.\n",
    "\n",
    "If `cfg.reward_clip` is set, clip reward to `[-cfg.reward_clip, cfg.reward_clip]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reward test (no clip): 5.0\n",
      "Reward test (clip=1.0): 1.0\n"
     ]
    }
   ],
   "source": [
    "def process_reward(r: float, cfg: Config):\n",
    "    if cfg.reward_clip is not None:\n",
    "        return max(-cfg.reward_clip, min(cfg.reward_clip, r))\n",
    "    return r\n",
    "\n",
    "# Quick test\n",
    "cfg.reward_clip = None  # Set to 1.0 to try clipping later\n",
    "print('Reward test (no clip):', process_reward(5.0, cfg))\n",
    "cfg.reward_clip = 1.0\n",
    "print('Reward test (clip=1.0):', process_reward(5.0, cfg))\n",
    "cfg.reward_clip = None  # Reset for training; modify if desired"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.10 Evaluation Utilities\n",
    "Evaluation runs with greedy policy (`epsilon=0`).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval utility ready.\n"
     ]
    }
   ],
   "source": [
    "def evaluate(env_fn, q_net, cfg: Config, episodes: int):\n",
    "    env = env_fn\n",
    "    returns = []\n",
    "    for _ in range(episodes):\n",
    "        obs, _ = env.reset()\n",
    "        done = False\n",
    "        ep_ret = 0\n",
    "        steps = 0\n",
    "        while not done and steps < cfg.max_episode_len:\n",
    "            a = select_action(q_net, obs, 0.0, env)\n",
    "            obs2, r, terminated, truncated, _ = env.step(a)\n",
    "            done = terminated or truncated\n",
    "            ep_ret += r\n",
    "            obs = obs2\n",
    "            steps += 1\n",
    "        returns.append(ep_ret)\n",
    "    return float(np.mean(returns))\n",
    "\n",
    "def hard_update(target, source):\n",
    "    target.load_state_dict(source.state_dict())\n",
    "\n",
    "print('Eval utility ready.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.11 Training Loop\n",
    "We iterate over the environments in.\n",
    "Logging per stage:\n",
    "- `episode_rewards`\n",
    "- `losses`\n",
    "- `eps_history`\n",
    "- `eval` (periodic greedy evaluation)\n",
    "\n",
    "Increase `max_steps_per_env` for stronger performance. For quick debugging, you may temporarily reduce it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Stage 1: cartpole ===\n",
      "Obs space: 4\n",
      "Obs space: 4\n",
      "[INFO] Using PrioritizedReplayBuffer\n",
      "Starting training...\n",
      "[DEBUG] Step     12 | Ep    1 | Eps 1.000 | MeanReward(10ep):  11.000 | MeanLoss(100it):  0.00000 | MeanQ:  -0.027 | GradNorm:  0.0000\n",
      "[DEBUG] Step     18 | Ep    2 | Eps 1.000 | MeanReward(10ep):   8.000 | MeanLoss(100it):  0.00000 | MeanQ:  -0.027 | GradNorm:  0.0000\n",
      "[DEBUG] Step     27 | Ep    3 | Eps 1.000 | MeanReward(10ep):   8.000 | MeanLoss(100it):  0.00000 | MeanQ:  -0.027 | GradNorm:  0.0000\n",
      "[DEBUG] Step     34 | Ep    4 | Eps 1.000 | MeanReward(10ep):   7.500 | MeanLoss(100it):  0.00000 | MeanQ:  -0.027 | GradNorm:  0.0000\n",
      "[DEBUG] Step     43 | Ep    5 | Eps 1.000 | MeanReward(10ep):   7.600 | MeanLoss(100it):  0.00000 | MeanQ:  -0.027 | GradNorm:  0.0000\n",
      "[DEBUG] Step     56 | Ep    6 | Eps 0.999 | MeanReward(10ep):   8.333 | MeanLoss(100it):  0.00000 | MeanQ:  -0.027 | GradNorm:  0.0000\n",
      "[DEBUG] Step     74 | Ep    7 | Eps 0.999 | MeanReward(10ep):   9.571 | MeanLoss(100it):  0.00000 | MeanQ:  -0.027 | GradNorm:  0.0000\n",
      "[DEBUG] Step     81 | Ep    8 | Eps 0.999 | MeanReward(10ep):   9.125 | MeanLoss(100it):  0.00000 | MeanQ:  -0.027 | GradNorm:  0.0000\n",
      "[DEBUG] Step     91 | Ep    9 | Eps 0.999 | MeanReward(10ep):   9.111 | MeanLoss(100it):  0.00000 | MeanQ:  -0.027 | GradNorm:  0.0000\n",
      "[DEBUG] Step    100 | Ep   10 | Eps 0.999 | MeanReward(10ep):   9.000 | MeanLoss(100it):  0.00000 | MeanQ:  -0.027 | GradNorm:  0.0000\n",
      "[cartpole] Ep 10 Steps 100 RecentMean 9.00 Eps 0.999\n",
      "[DEBUG] Step    106 | Ep   11 | Eps 0.999 | MeanReward(10ep):   8.400 | MeanLoss(100it):  0.00000 | MeanQ:  -0.027 | GradNorm:  0.0000\n",
      "[DEBUG] Step    120 | Ep   12 | Eps 0.999 | MeanReward(10ep):   9.200 | MeanLoss(100it):  0.00000 | MeanQ:  -0.027 | GradNorm:  0.0000\n",
      "[DEBUG] Step    132 | Ep   13 | Eps 0.999 | MeanReward(10ep):   9.500 | MeanLoss(100it):  0.00000 | MeanQ:  -0.027 | GradNorm:  0.0000\n",
      "[DEBUG] Step    141 | Ep   14 | Eps 0.999 | MeanReward(10ep):   9.700 | MeanLoss(100it):  0.00000 | MeanQ:  -0.027 | GradNorm:  0.0000\n",
      "[DEBUG] Step    150 | Ep   15 | Eps 0.999 | MeanReward(10ep):   9.700 | MeanLoss(100it):  0.00000 | MeanQ:  -0.027 | GradNorm:  0.0000\n",
      "[DEBUG] Step    157 | Ep   16 | Eps 0.998 | MeanReward(10ep):   9.100 | MeanLoss(100it):  0.00000 | MeanQ:  -0.027 | GradNorm:  0.0000\n",
      "[DEBUG] Step    166 | Ep   17 | Eps 0.998 | MeanReward(10ep):   8.200 | MeanLoss(100it):  0.00000 | MeanQ:  -0.027 | GradNorm:  0.0000\n",
      "[DEBUG] Step    171 | Ep   18 | Eps 0.998 | MeanReward(10ep):   8.000 | MeanLoss(100it):  0.00000 | MeanQ:  -0.027 | GradNorm:  0.0000\n",
      "[DEBUG] Step    177 | Ep   19 | Eps 0.998 | MeanReward(10ep):   7.600 | MeanLoss(100it):  0.00000 | MeanQ:  -0.027 | GradNorm:  0.0000\n",
      "[DEBUG] Step    189 | Ep   20 | Eps 0.998 | MeanReward(10ep):   7.900 | MeanLoss(100it):  0.00000 | MeanQ:  -0.027 | GradNorm:  0.0000\n",
      "[cartpole] Ep 20 Steps 189 RecentMean 7.90 Eps 0.998\n",
      "Eval @ Ep 20 and 189 steps => mean return 5.00\n",
      "[DEBUG] Step    195 | Ep   21 | Eps 0.998 | MeanReward(10ep):   7.900 | MeanLoss(100it):  0.00000 | MeanQ:  -0.030 | GradNorm:  0.0000\n",
      "[DEBUG] Step    213 | Ep   22 | Eps 0.998 | MeanReward(10ep):   8.300 | MeanLoss(100it):  0.00000 | MeanQ:  -0.027 | GradNorm:  0.0000\n",
      "[DEBUG] Step    220 | Ep   23 | Eps 0.998 | MeanReward(10ep):   7.800 | MeanLoss(100it):  0.00000 | MeanQ:  -0.027 | GradNorm:  0.0000\n",
      "[DEBUG] Step    232 | Ep   24 | Eps 0.998 | MeanReward(10ep):   8.100 | MeanLoss(100it):  0.00000 | MeanQ:  -0.027 | GradNorm:  0.0000\n",
      "[DEBUG] Step    250 | Ep   25 | Eps 0.998 | MeanReward(10ep):   9.000 | MeanLoss(100it):  0.00000 | MeanQ:  -0.027 | GradNorm:  0.0000\n",
      "[DEBUG] Step    257 | Ep   26 | Eps 0.997 | MeanReward(10ep):   9.000 | MeanLoss(100it):  0.00000 | MeanQ:  -0.027 | GradNorm:  0.0000\n",
      "[DEBUG] Step    276 | Ep   27 | Eps 0.997 | MeanReward(10ep):  10.000 | MeanLoss(100it):  0.00000 | MeanQ:  -0.027 | GradNorm:  0.0000\n",
      "[DEBUG] Step    303 | Ep   28 | Eps 0.997 | MeanReward(10ep):  12.200 | MeanLoss(100it):  0.00000 | MeanQ:  -0.027 | GradNorm:  0.0000\n",
      "[DEBUG] Step    319 | Ep   29 | Eps 0.997 | MeanReward(10ep):  13.200 | MeanLoss(100it):  0.00000 | MeanQ:  -0.027 | GradNorm:  0.0000\n",
      "[DEBUG] Step    339 | Ep   30 | Eps 0.997 | MeanReward(10ep):  14.000 | MeanLoss(100it):  0.00000 | MeanQ:  -0.027 | GradNorm:  0.0000\n",
      "[cartpole] Ep 30 Steps 339 RecentMean 14.00 Eps 0.997\n",
      "[DEBUG] Step    345 | Ep   31 | Eps 0.997 | MeanReward(10ep):  14.000 | MeanLoss(100it):  0.00000 | MeanQ:  -0.027 | GradNorm:  0.0000\n",
      "[DEBUG] Step    351 | Ep   32 | Eps 0.997 | MeanReward(10ep):  12.800 | MeanLoss(100it):  0.00000 | MeanQ:  -0.027 | GradNorm:  0.0000\n",
      "[DEBUG] Step    358 | Ep   33 | Eps 0.996 | MeanReward(10ep):  12.800 | MeanLoss(100it):  0.00000 | MeanQ:  -0.027 | GradNorm:  0.0000\n",
      "[DEBUG] Step    364 | Ep   34 | Eps 0.996 | MeanReward(10ep):  12.200 | MeanLoss(100it):  0.00000 | MeanQ:  -0.027 | GradNorm:  0.0000\n",
      "[DEBUG] Step    372 | Ep   35 | Eps 0.996 | MeanReward(10ep):  11.200 | MeanLoss(100it):  0.00000 | MeanQ:  -0.027 | GradNorm:  0.0000\n",
      "[DEBUG] Step    387 | Ep   36 | Eps 0.996 | MeanReward(10ep):  12.000 | MeanLoss(100it):  0.00000 | MeanQ:  -0.027 | GradNorm:  0.0000\n",
      "[DEBUG] Step    399 | Ep   37 | Eps 0.996 | MeanReward(10ep):  11.300 | MeanLoss(100it):  0.00000 | MeanQ:  -0.027 | GradNorm:  0.0000\n",
      "[DEBUG] Step    405 | Ep   38 | Eps 0.996 | MeanReward(10ep):   9.200 | MeanLoss(100it):  0.00000 | MeanQ:  -0.027 | GradNorm:  0.0000\n",
      "[DEBUG] Step    413 | Ep   39 | Eps 0.996 | MeanReward(10ep):   8.400 | MeanLoss(100it):  0.00000 | MeanQ:  -0.027 | GradNorm:  0.0000\n",
      "[DEBUG] Step    419 | Ep   40 | Eps 0.996 | MeanReward(10ep):   7.000 | MeanLoss(100it):  0.00000 | MeanQ:  -0.027 | GradNorm:  0.0000\n",
      "[cartpole] Ep 40 Steps 419 RecentMean 7.00 Eps 0.996\n",
      "Eval @ Ep 40 and 419 steps => mean return 5.00\n",
      "[DEBUG] Step    442 | Ep   41 | Eps 0.996 | MeanReward(10ep):   8.700 | MeanLoss(100it):  0.00000 | MeanQ:  -0.030 | GradNorm:  0.0000\n",
      "[DEBUG] Step    448 | Ep   42 | Eps 0.996 | MeanReward(10ep):   8.700 | MeanLoss(100it):  0.00000 | MeanQ:  -0.027 | GradNorm:  0.0000\n",
      "[DEBUG] Step    454 | Ep   43 | Eps 0.996 | MeanReward(10ep):   8.600 | MeanLoss(100it):  0.00000 | MeanQ:  -0.027 | GradNorm:  0.0000\n",
      "[DEBUG] Step    479 | Ep   44 | Eps 0.995 | MeanReward(10ep):  10.500 | MeanLoss(100it):  0.00000 | MeanQ:  -0.027 | GradNorm:  0.0000\n",
      "[DEBUG] Step    486 | Ep   45 | Eps 0.995 | MeanReward(10ep):  10.400 | MeanLoss(100it):  0.00000 | MeanQ:  -0.027 | GradNorm:  0.0000\n",
      "[DEBUG] Step    509 | Ep   46 | Eps 0.995 | MeanReward(10ep):  11.200 | MeanLoss(100it):  0.00000 | MeanQ:  -0.027 | GradNorm:  0.0000\n",
      "[DEBUG] Step    524 | Ep   47 | Eps 0.995 | MeanReward(10ep):  11.500 | MeanLoss(100it):  0.00000 | MeanQ:  -0.027 | GradNorm:  0.0000\n",
      "[DEBUG] Step    533 | Ep   48 | Eps 0.995 | MeanReward(10ep):  11.800 | MeanLoss(100it):  0.00000 | MeanQ:  -0.027 | GradNorm:  0.0000\n",
      "[DEBUG] Step    546 | Ep   49 | Eps 0.995 | MeanReward(10ep):  12.300 | MeanLoss(100it):  0.00000 | MeanQ:  -0.027 | GradNorm:  0.0000\n",
      "[DEBUG] Step    559 | Ep   50 | Eps 0.994 | MeanReward(10ep):  13.000 | MeanLoss(100it):  0.00000 | MeanQ:  -0.027 | GradNorm:  0.0000\n",
      "[cartpole] Ep 50 Steps 559 RecentMean 13.00 Eps 0.994\n",
      "[DEBUG] Step    566 | Ep   51 | Eps 0.994 | MeanReward(10ep):  11.400 | MeanLoss(100it):  0.00000 | MeanQ:  -0.027 | GradNorm:  0.0000\n",
      "[DEBUG] Step    580 | Ep   52 | Eps 0.994 | MeanReward(10ep):  12.200 | MeanLoss(100it):  0.00000 | MeanQ:  -0.027 | GradNorm:  0.0000\n",
      "[DEBUG] Step    588 | Ep   53 | Eps 0.994 | MeanReward(10ep):  12.400 | MeanLoss(100it):  0.00000 | MeanQ:  -0.027 | GradNorm:  0.0000\n",
      "[DEBUG] Step    594 | Ep   54 | Eps 0.994 | MeanReward(10ep):  10.500 | MeanLoss(100it):  0.00000 | MeanQ:  -0.027 | GradNorm:  0.0000\n",
      "[DEBUG] Step    613 | Ep   55 | Eps 0.994 | MeanReward(10ep):  11.700 | MeanLoss(100it):  0.00000 | MeanQ:  -0.027 | GradNorm:  0.0000\n",
      "[DEBUG] Step    622 | Ep   56 | Eps 0.994 | MeanReward(10ep):  10.300 | MeanLoss(100it):  0.00000 | MeanQ:  -0.027 | GradNorm:  0.0000\n",
      "[DEBUG] Step    628 | Ep   57 | Eps 0.994 | MeanReward(10ep):   9.400 | MeanLoss(100it):  0.00000 | MeanQ:  -0.027 | GradNorm:  0.0000\n",
      "[DEBUG] Step    634 | Ep   58 | Eps 0.994 | MeanReward(10ep):   9.100 | MeanLoss(100it):  0.00000 | MeanQ:  -0.027 | GradNorm:  0.0000\n",
      "[DEBUG] Step    640 | Ep   59 | Eps 0.994 | MeanReward(10ep):   8.400 | MeanLoss(100it):  0.00000 | MeanQ:  -0.027 | GradNorm:  0.0000\n",
      "[DEBUG] Step    652 | Ep   60 | Eps 0.994 | MeanReward(10ep):   8.300 | MeanLoss(100it):  0.00000 | MeanQ:  -0.027 | GradNorm:  0.0000\n",
      "[cartpole] Ep 60 Steps 652 RecentMean 8.30 Eps 0.994\n",
      "Eval @ Ep 60 and 652 steps => mean return 5.00\n",
      "[DEBUG] Step    676 | Ep   61 | Eps 0.993 | MeanReward(10ep):  10.000 | MeanLoss(100it):  0.00000 | MeanQ:  -0.030 | GradNorm:  0.0000\n",
      "[DEBUG] Step    685 | Ep   62 | Eps 0.993 | MeanReward(10ep):   9.500 | MeanLoss(100it):  0.00000 | MeanQ:  -0.027 | GradNorm:  0.0000\n",
      "[DEBUG] Step    691 | Ep   63 | Eps 0.993 | MeanReward(10ep):   9.300 | MeanLoss(100it):  0.00000 | MeanQ:  -0.027 | GradNorm:  0.0000\n",
      "[DEBUG] Step    704 | Ep   64 | Eps 0.993 | MeanReward(10ep):  10.000 | MeanLoss(100it):  0.00000 | MeanQ:  -0.027 | GradNorm:  0.0000\n",
      "[DEBUG] Step    712 | Ep   65 | Eps 0.993 | MeanReward(10ep):   8.900 | MeanLoss(100it):  0.00000 | MeanQ:  -0.027 | GradNorm:  0.0000\n",
      "[DEBUG] Step    726 | Ep   66 | Eps 0.993 | MeanReward(10ep):   9.400 | MeanLoss(100it):  0.00000 | MeanQ:  -0.027 | GradNorm:  0.0000\n",
      "[DEBUG] Step    736 | Ep   67 | Eps 0.993 | MeanReward(10ep):   9.800 | MeanLoss(100it):  0.00000 | MeanQ:  -0.027 | GradNorm:  0.0000\n",
      "[DEBUG] Step    747 | Ep   68 | Eps 0.993 | MeanReward(10ep):  10.300 | MeanLoss(100it):  0.00000 | MeanQ:  -0.027 | GradNorm:  0.0000\n",
      "[DEBUG] Step    756 | Ep   69 | Eps 0.993 | MeanReward(10ep):  10.600 | MeanLoss(100it):  0.00000 | MeanQ:  -0.027 | GradNorm:  0.0000\n",
      "[DEBUG] Step    762 | Ep   70 | Eps 0.992 | MeanReward(10ep):  10.000 | MeanLoss(100it):  0.00000 | MeanQ:  -0.027 | GradNorm:  0.0000\n",
      "[cartpole] Ep 70 Steps 762 RecentMean 10.00 Eps 0.992\n",
      "[DEBUG] Step    768 | Ep   71 | Eps 0.992 | MeanReward(10ep):   8.200 | MeanLoss(100it):  0.00000 | MeanQ:  -0.027 | GradNorm:  0.0000\n",
      "[DEBUG] Step    779 | Ep   72 | Eps 0.992 | MeanReward(10ep):   8.400 | MeanLoss(100it):  0.00000 | MeanQ:  -0.027 | GradNorm:  0.0000\n",
      "[DEBUG] Step    784 | Ep   73 | Eps 0.992 | MeanReward(10ep):   8.300 | MeanLoss(100it):  0.00000 | MeanQ:  -0.027 | GradNorm:  0.0000\n",
      "[DEBUG] Step    790 | Ep   74 | Eps 0.992 | MeanReward(10ep):   7.600 | MeanLoss(100it):  0.00000 | MeanQ:  -0.027 | GradNorm:  0.0000\n",
      "[DEBUG] Step    801 | Ep   75 | Eps 0.992 | MeanReward(10ep):   7.900 | MeanLoss(100it):  0.00000 | MeanQ:  -0.027 | GradNorm:  0.0000\n",
      "[DEBUG] Step    816 | Ep   76 | Eps 0.992 | MeanReward(10ep):   8.000 | MeanLoss(100it):  0.00000 | MeanQ:  -0.027 | GradNorm:  0.0000\n",
      "[DEBUG] Step    825 | Ep   77 | Eps 0.992 | MeanReward(10ep):   7.900 | MeanLoss(100it):  0.00000 | MeanQ:  -0.027 | GradNorm:  0.0000\n",
      "[DEBUG] Step    831 | Ep   78 | Eps 0.992 | MeanReward(10ep):   7.400 | MeanLoss(100it):  0.00000 | MeanQ:  -0.027 | GradNorm:  0.0000\n",
      "[DEBUG] Step    837 | Ep   79 | Eps 0.992 | MeanReward(10ep):   7.100 | MeanLoss(100it):  0.00000 | MeanQ:  -0.027 | GradNorm:  0.0000\n",
      "[DEBUG] Step    842 | Ep   80 | Eps 0.992 | MeanReward(10ep):   7.000 | MeanLoss(100it):  0.00000 | MeanQ:  -0.027 | GradNorm:  0.0000\n",
      "[cartpole] Ep 80 Steps 842 RecentMean 7.00 Eps 0.992\n",
      "Eval @ Ep 80 and 842 steps => mean return 5.00\n",
      "[DEBUG] Step    852 | Ep   81 | Eps 0.992 | MeanReward(10ep):   7.400 | MeanLoss(100it):  0.00000 | MeanQ:  -0.030 | GradNorm:  0.0000\n",
      "[DEBUG] Step    858 | Ep   82 | Eps 0.992 | MeanReward(10ep):   6.900 | MeanLoss(100it):  0.00000 | MeanQ:  -0.027 | GradNorm:  0.0000\n",
      "[DEBUG] Step    866 | Ep   83 | Eps 0.991 | MeanReward(10ep):   7.200 | MeanLoss(100it):  0.00000 | MeanQ:  -0.027 | GradNorm:  0.0000\n",
      "[DEBUG] Step    875 | Ep   84 | Eps 0.991 | MeanReward(10ep):   7.500 | MeanLoss(100it):  0.00000 | MeanQ:  -0.027 | GradNorm:  0.0000\n",
      "[DEBUG] Step    889 | Ep   85 | Eps 0.991 | MeanReward(10ep):   7.800 | MeanLoss(100it):  0.00000 | MeanQ:  -0.027 | GradNorm:  0.0000\n",
      "[DEBUG] Step    895 | Ep   86 | Eps 0.991 | MeanReward(10ep):   6.900 | MeanLoss(100it):  0.00000 | MeanQ:  -0.027 | GradNorm:  0.0000\n",
      "[DEBUG] Step    904 | Ep   87 | Eps 0.991 | MeanReward(10ep):   6.900 | MeanLoss(100it):  0.00000 | MeanQ:  -0.027 | GradNorm:  0.0000\n",
      "[DEBUG] Step    909 | Ep   88 | Eps 0.991 | MeanReward(10ep):   6.800 | MeanLoss(100it):  0.00000 | MeanQ:  -0.027 | GradNorm:  0.0000\n",
      "[DEBUG] Step    915 | Ep   89 | Eps 0.991 | MeanReward(10ep):   6.800 | MeanLoss(100it):  0.00000 | MeanQ:  -0.027 | GradNorm:  0.0000\n",
      "[DEBUG] Step    922 | Ep   90 | Eps 0.991 | MeanReward(10ep):   7.000 | MeanLoss(100it):  0.00000 | MeanQ:  -0.027 | GradNorm:  0.0000\n",
      "[cartpole] Ep 90 Steps 922 RecentMean 7.00 Eps 0.991\n",
      "[DEBUG] Step    930 | Ep   91 | Eps 0.991 | MeanReward(10ep):   6.800 | MeanLoss(100it):  0.00000 | MeanQ:  -0.027 | GradNorm:  0.0000\n",
      "[DEBUG] Step    936 | Ep   92 | Eps 0.991 | MeanReward(10ep):   6.800 | MeanLoss(100it):  0.00000 | MeanQ:  -0.027 | GradNorm:  0.0000\n",
      "[DEBUG] Step    945 | Ep   93 | Eps 0.991 | MeanReward(10ep):   6.900 | MeanLoss(100it):  0.00000 | MeanQ:  -0.027 | GradNorm:  0.0000\n",
      "[DEBUG] Step    951 | Ep   94 | Eps 0.991 | MeanReward(10ep):   6.600 | MeanLoss(100it):  0.00000 | MeanQ:  -0.027 | GradNorm:  0.0000\n",
      "[DEBUG] Step    964 | Ep   95 | Eps 0.990 | MeanReward(10ep):   6.500 | MeanLoss(100it):  0.00000 | MeanQ:  -0.027 | GradNorm:  0.0000\n",
      "[DEBUG] Step    977 | Ep   96 | Eps 0.990 | MeanReward(10ep):   7.200 | MeanLoss(100it):  0.00000 | MeanQ:  -0.027 | GradNorm:  0.0000\n",
      "[DEBUG] Step    983 | Ep   97 | Eps 0.990 | MeanReward(10ep):   6.900 | MeanLoss(100it):  0.00000 | MeanQ:  -0.027 | GradNorm:  0.0000\n",
      "[DEBUG] Step    993 | Ep   98 | Eps 0.990 | MeanReward(10ep):   7.400 | MeanLoss(100it):  0.00000 | MeanQ:  -0.027 | GradNorm:  0.0000\n",
      "[DEBUG] Step   1002 | Ep   99 | Eps 0.990 | MeanReward(10ep):   7.700 | MeanLoss(100it):  0.00000 | MeanQ:  -0.027 | GradNorm:  0.0000\n",
      "[DEBUG] Step   1012 | Ep  100 | Eps 0.990 | MeanReward(10ep):   8.000 | MeanLoss(100it):  0.00000 | MeanQ:  -0.027 | GradNorm:  0.0000\n",
      "[cartpole] Ep 100 Steps 1012 RecentMean 8.00 Eps 0.990\n",
      "Eval @ Ep 100 and 1012 steps => mean return 5.00\n",
      "[DEBUG] Step   1024 | Ep  101 | Eps 0.990 | MeanReward(10ep):   8.400 | MeanLoss(100it):  0.00000 | MeanQ:  -0.030 | GradNorm:  0.0000\n",
      "[DEBUG] Step   1029 | Ep  102 | Eps 0.990 | MeanReward(10ep):   8.300 | MeanLoss(100it):  0.00000 | MeanQ:  -0.027 | GradNorm:  0.0000\n",
      "[DEBUG] Step   1036 | Ep  103 | Eps 0.990 | MeanReward(10ep):   8.100 | MeanLoss(100it):  0.00000 | MeanQ:  -0.027 | GradNorm:  0.0000\n",
      "[DEBUG] Step   1043 | Ep  104 | Eps 0.990 | MeanReward(10ep):   8.200 | MeanLoss(100it):  0.00000 | MeanQ:  -0.027 | GradNorm:  0.0000\n",
      "[DEBUG] Step   1050 | Ep  105 | Eps 0.990 | MeanReward(10ep):   7.600 | MeanLoss(100it):  0.00000 | MeanQ:  -0.027 | GradNorm:  0.0000\n",
      "[DEBUG] Step   1055 | Ep  106 | Eps 0.990 | MeanReward(10ep):   6.800 | MeanLoss(100it):  0.00000 | MeanQ:  -0.027 | GradNorm:  0.0000\n",
      "[DEBUG] Step   1061 | Ep  107 | Eps 0.990 | MeanReward(10ep):   6.800 | MeanLoss(100it):  0.00000 | MeanQ:  -0.027 | GradNorm:  0.0000\n",
      "[DEBUG] Step   1071 | Ep  108 | Eps 0.989 | MeanReward(10ep):   6.800 | MeanLoss(100it):  0.00000 | MeanQ:  -0.027 | GradNorm:  0.0000\n",
      "[DEBUG] Step   1080 | Ep  109 | Eps 0.989 | MeanReward(10ep):   6.800 | MeanLoss(100it):  0.00000 | MeanQ:  -0.027 | GradNorm:  0.0000\n",
      "[DEBUG] Step   1090 | Ep  110 | Eps 0.989 | MeanReward(10ep):   6.800 | MeanLoss(100it):  0.00000 | MeanQ:  -0.027 | GradNorm:  0.0000\n",
      "[cartpole] Ep 110 Steps 1090 RecentMean 6.80 Eps 0.989\n",
      "[DEBUG] Step   1110 | Ep  111 | Eps 0.989 | MeanReward(10ep):   7.600 | MeanLoss(100it):  0.00000 | MeanQ:  -0.027 | GradNorm:  0.0000\n",
      "[DEBUG] Step   1131 | Ep  112 | Eps 0.989 | MeanReward(10ep):   9.200 | MeanLoss(100it):  0.00000 | MeanQ:  -0.027 | GradNorm:  0.0000\n",
      "[DEBUG] Step   1136 | Ep  113 | Eps 0.989 | MeanReward(10ep):   9.000 | MeanLoss(100it):  0.00000 | MeanQ:  -0.027 | GradNorm:  0.0000\n",
      "[DEBUG] Step   1143 | Ep  114 | Eps 0.989 | MeanReward(10ep):   9.000 | MeanLoss(100it):  0.00000 | MeanQ:  -0.027 | GradNorm:  0.0000\n",
      "[DEBUG] Step   1151 | Ep  115 | Eps 0.989 | MeanReward(10ep):   9.100 | MeanLoss(100it):  0.00000 | MeanQ:  -0.027 | GradNorm:  0.0000\n",
      "[DEBUG] Step   1157 | Ep  116 | Eps 0.989 | MeanReward(10ep):   9.200 | MeanLoss(100it):  0.00000 | MeanQ:  -0.027 | GradNorm:  0.0000\n",
      "[DEBUG] Step   1166 | Ep  117 | Eps 0.988 | MeanReward(10ep):   9.500 | MeanLoss(100it):  0.00000 | MeanQ:  -0.027 | GradNorm:  0.0000\n",
      "[DEBUG] Step   1182 | Ep  118 | Eps 0.988 | MeanReward(10ep):  10.100 | MeanLoss(100it):  0.00000 | MeanQ:  -0.027 | GradNorm:  0.0000\n",
      "[DEBUG] Step   1188 | Ep  119 | Eps 0.988 | MeanReward(10ep):   9.800 | MeanLoss(100it):  0.00000 | MeanQ:  -0.027 | GradNorm:  0.0000\n",
      "[DEBUG] Step   1199 | Ep  120 | Eps 0.988 | MeanReward(10ep):   9.900 | MeanLoss(100it):  0.00000 | MeanQ:  -0.027 | GradNorm:  0.0000\n",
      "[cartpole] Ep 120 Steps 1199 RecentMean 9.90 Eps 0.988\n",
      "Eval @ Ep 120 and 1199 steps => mean return 5.00\n",
      "[DEBUG] Step   1213 | Ep  121 | Eps 0.988 | MeanReward(10ep):   9.300 | MeanLoss(100it):  0.00000 | MeanQ:  -0.030 | GradNorm:  0.0000\n",
      "[DEBUG] Step   1239 | Ep  122 | Eps 0.988 | MeanReward(10ep):   9.800 | MeanLoss(100it):  0.00000 | MeanQ:  -0.027 | GradNorm:  0.0000\n",
      "[DEBUG] Step   1255 | Ep  123 | Eps 0.988 | MeanReward(10ep):  10.900 | MeanLoss(100it):  2.29814 | MeanQ:   0.025 | GradNorm:  0.8921\n",
      "[DEBUG] Step   1264 | Ep  124 | Eps 0.987 | MeanReward(10ep):  11.100 | MeanLoss(100it):  2.13360 | MeanQ:   0.102 | GradNorm:  0.8270\n",
      "[DEBUG] Step   1275 | Ep  125 | Eps 0.987 | MeanReward(10ep):  11.400 | MeanLoss(100it):  2.04642 | MeanQ:   0.153 | GradNorm:  0.8002\n",
      "[DEBUG] Step   1285 | Ep  126 | Eps 0.987 | MeanReward(10ep):  11.800 | MeanLoss(100it):  1.93770 | MeanQ:   0.230 | GradNorm:  0.8807\n",
      "[DEBUG] Step   1297 | Ep  127 | Eps 0.987 | MeanReward(10ep):  12.100 | MeanLoss(100it):  1.85207 | MeanQ:   0.307 | GradNorm:  0.8515\n",
      "[DEBUG] Step   1303 | Ep  128 | Eps 0.987 | MeanReward(10ep):  11.100 | MeanLoss(100it):  1.82003 | MeanQ:   0.332 | GradNorm:  0.8250\n",
      "[DEBUG] Step   1317 | Ep  129 | Eps 0.987 | MeanReward(10ep):  11.900 | MeanLoss(100it):  1.80916 | MeanQ:   0.433 | GradNorm:  0.8697\n",
      "[DEBUG] Step   1331 | Ep  130 | Eps 0.987 | MeanReward(10ep):  12.200 | MeanLoss(100it):  1.79094 | MeanQ:   0.510 | GradNorm:  0.7197\n",
      "[cartpole] Ep 130 Steps 1331 RecentMean 12.20 Eps 0.987\n",
      "[DEBUG] Step   1342 | Ep  131 | Eps 0.987 | MeanReward(10ep):  11.900 | MeanLoss(100it):  1.72951 | MeanQ:   0.586 | GradNorm:  0.6578\n",
      "[DEBUG] Step   1347 | Ep  132 | Eps 0.987 | MeanReward(10ep):   9.800 | MeanLoss(100it):  1.72012 | MeanQ:   0.613 | GradNorm:  0.7300\n",
      "[DEBUG] Step   1359 | Ep  133 | Eps 0.987 | MeanReward(10ep):   9.400 | MeanLoss(100it):  1.69943 | MeanQ:   0.694 | GradNorm:  0.8999\n",
      "[DEBUG] Step   1365 | Ep  134 | Eps 0.986 | MeanReward(10ep):   9.100 | MeanLoss(100it):  1.67974 | MeanQ:   0.751 | GradNorm:  0.7148\n",
      "[DEBUG] Step   1384 | Ep  135 | Eps 0.986 | MeanReward(10ep):   9.900 | MeanLoss(100it):  1.60444 | MeanQ:   0.898 | GradNorm:  0.5403\n",
      "[DEBUG] Step   1392 | Ep  136 | Eps 0.986 | MeanReward(10ep):   9.700 | MeanLoss(100it):  1.57236 | MeanQ:   0.957 | GradNorm:  0.5396\n",
      "[DEBUG] Step   1400 | Ep  137 | Eps 0.986 | MeanReward(10ep):   9.300 | MeanLoss(100it):  1.53115 | MeanQ:   1.016 | GradNorm:  0.4651\n",
      "[DEBUG] Step   1406 | Ep  138 | Eps 0.986 | MeanReward(10ep):   9.300 | MeanLoss(100it):  1.54594 | MeanQ:   1.046 | GradNorm:  0.8749\n",
      "[DEBUG] Step   1412 | Ep  139 | Eps 0.986 | MeanReward(10ep):   8.500 | MeanLoss(100it):  1.56534 | MeanQ:   1.107 | GradNorm:  0.5765\n",
      "[DEBUG] Step   1436 | Ep  140 | Eps 0.986 | MeanReward(10ep):   9.500 | MeanLoss(100it):  1.61583 | MeanQ:   1.304 | GradNorm:  0.9501\n",
      "[cartpole] Ep 140 Steps 1436 RecentMean 9.50 Eps 0.986\n",
      "Eval @ Ep 140 and 1436 steps => mean return 4.00\n",
      "[DEBUG] Step   1451 | Ep  141 | Eps 0.986 | MeanReward(10ep):   9.900 | MeanLoss(100it):  1.61116 | MeanQ:   1.399 | GradNorm:  0.8249\n",
      "[DEBUG] Step   1460 | Ep  142 | Eps 0.986 | MeanReward(10ep):  10.300 | MeanLoss(100it):  1.60115 | MeanQ:   1.513 | GradNorm:  0.6096\n",
      "[DEBUG] Step   1466 | Ep  143 | Eps 0.985 | MeanReward(10ep):   9.700 | MeanLoss(100it):  1.60299 | MeanQ:   1.548 | GradNorm:  0.7145\n",
      "[DEBUG] Step   1471 | Ep  144 | Eps 0.985 | MeanReward(10ep):   9.600 | MeanLoss(100it):  1.61090 | MeanQ:   1.584 | GradNorm:  1.0096\n",
      "[DEBUG] Step   1477 | Ep  145 | Eps 0.985 | MeanReward(10ep):   8.300 | MeanLoss(100it):  1.59914 | MeanQ:   1.654 | GradNorm:  0.4980\n",
      "[DEBUG] Step   1486 | Ep  146 | Eps 0.985 | MeanReward(10ep):   8.400 | MeanLoss(100it):  1.59301 | MeanQ:   1.724 | GradNorm:  0.5227\n",
      "[DEBUG] Step   1492 | Ep  147 | Eps 0.985 | MeanReward(10ep):   8.200 | MeanLoss(100it):  1.57951 | MeanQ:   1.793 | GradNorm:  0.6136\n",
      "[DEBUG] Step   1508 | Ep  148 | Eps 0.985 | MeanReward(10ep):   9.200 | MeanLoss(100it):  1.59977 | MeanQ:   1.933 | GradNorm:  0.5165\n",
      "[DEBUG] Step   1516 | Ep  149 | Eps 0.985 | MeanReward(10ep):   9.400 | MeanLoss(100it):  1.62543 | MeanQ:   2.004 | GradNorm:  0.7289\n",
      "[DEBUG] Step   1522 | Ep  150 | Eps 0.985 | MeanReward(10ep):   7.600 | MeanLoss(100it):  1.63168 | MeanQ:   2.040 | GradNorm:  0.5143\n",
      "[cartpole] Ep 150 Steps 1522 RecentMean 7.60 Eps 0.985\n",
      "[DEBUG] Step   1529 | Ep  151 | Eps 0.985 | MeanReward(10ep):   6.800 | MeanLoss(100it):  1.64895 | MeanQ:   2.111 | GradNorm:  0.6096\n",
      "[DEBUG] Step   1554 | Ep  152 | Eps 0.985 | MeanReward(10ep):   8.400 | MeanLoss(100it):  1.68932 | MeanQ:   2.334 | GradNorm:  0.6380\n",
      "[DEBUG] Step   1560 | Ep  153 | Eps 0.985 | MeanReward(10ep):   8.400 | MeanLoss(100it):  1.69059 | MeanQ:   2.406 | GradNorm:  0.3257\n",
      "[DEBUG] Step   1568 | Ep  154 | Eps 0.984 | MeanReward(10ep):   8.700 | MeanLoss(100it):  1.70988 | MeanQ:   2.480 | GradNorm:  0.5638\n",
      "[DEBUG] Step   1577 | Ep  155 | Eps 0.984 | MeanReward(10ep):   9.000 | MeanLoss(100it):  1.72660 | MeanQ:   2.555 | GradNorm:  0.7596\n",
      "[DEBUG] Step   1585 | Ep  156 | Eps 0.984 | MeanReward(10ep):   8.900 | MeanLoss(100it):  1.73254 | MeanQ:   2.630 | GradNorm:  0.3112\n",
      "[DEBUG] Step   1597 | Ep  157 | Eps 0.984 | MeanReward(10ep):   9.500 | MeanLoss(100it):  1.74434 | MeanQ:   2.739 | GradNorm:  0.7545\n",
      "[DEBUG] Step   1607 | Ep  158 | Eps 0.984 | MeanReward(10ep):   8.900 | MeanLoss(100it):  1.76681 | MeanQ:   2.812 | GradNorm:  0.4540\n",
      "[DEBUG] Step   1615 | Ep  159 | Eps 0.984 | MeanReward(10ep):   8.900 | MeanLoss(100it):  1.79896 | MeanQ:   2.885 | GradNorm:  0.7100\n",
      "[DEBUG] Step   1622 | Ep  160 | Eps 0.984 | MeanReward(10ep):   9.000 | MeanLoss(100it):  1.82241 | MeanQ:   2.961 | GradNorm:  0.6088\n",
      "[cartpole] Ep 160 Steps 1622 RecentMean 9.00 Eps 0.984\n",
      "Eval @ Ep 160 and 1622 steps => mean return 5.00\n",
      "[DEBUG] Step   1628 | Ep  161 | Eps 0.984 | MeanReward(10ep):   8.900 | MeanLoss(100it):  1.84132 | MeanQ:   3.016 | GradNorm:  0.7185\n",
      "[DEBUG] Step   1644 | Ep  162 | Eps 0.984 | MeanReward(10ep):   8.000 | MeanLoss(100it):  1.87963 | MeanQ:   3.204 | GradNorm:  0.5184\n",
      "[DEBUG] Step   1650 | Ep  163 | Eps 0.984 | MeanReward(10ep):   8.000 | MeanLoss(100it):  1.87980 | MeanQ:   3.246 | GradNorm:  0.6694\n",
      "[DEBUG] Step   1657 | Ep  164 | Eps 0.984 | MeanReward(10ep):   7.900 | MeanLoss(100it):  1.89508 | MeanQ:   3.332 | GradNorm:  0.7507\n",
      "[DEBUG] Step   1663 | Ep  165 | Eps 0.984 | MeanReward(10ep):   7.600 | MeanLoss(100it):  1.90157 | MeanQ:   3.376 | GradNorm:  0.7313\n",
      "[DEBUG] Step   1673 | Ep  166 | Eps 0.983 | MeanReward(10ep):   7.800 | MeanLoss(100it):  1.92866 | MeanQ:   3.504 | GradNorm:  0.6936\n",
      "[DEBUG] Step   1682 | Ep  167 | Eps 0.983 | MeanReward(10ep):   7.500 | MeanLoss(100it):  1.95011 | MeanQ:   3.592 | GradNorm:  0.3394\n",
      "[DEBUG] Step   1688 | Ep  168 | Eps 0.983 | MeanReward(10ep):   7.100 | MeanLoss(100it):  1.97387 | MeanQ:   3.678 | GradNorm:  0.5631\n",
      "[DEBUG] Step   1697 | Ep  169 | Eps 0.983 | MeanReward(10ep):   7.200 | MeanLoss(100it):  2.00218 | MeanQ:   3.763 | GradNorm:  0.4594\n",
      "[DEBUG] Step   1707 | Ep  170 | Eps 0.983 | MeanReward(10ep):   7.500 | MeanLoss(100it):  2.02711 | MeanQ:   3.847 | GradNorm:  0.4574\n",
      "[cartpole] Ep 170 Steps 1707 RecentMean 7.50 Eps 0.983\n",
      "[DEBUG] Step   1720 | Ep  171 | Eps 0.983 | MeanReward(10ep):   8.200 | MeanLoss(100it):  2.11992 | MeanQ:   4.018 | GradNorm:  0.3924\n",
      "[DEBUG] Step   1728 | Ep  172 | Eps 0.983 | MeanReward(10ep):   7.400 | MeanLoss(100it):  2.16156 | MeanQ:   4.105 | GradNorm:  0.3757\n",
      "[DEBUG] Step   1734 | Ep  173 | Eps 0.983 | MeanReward(10ep):   7.400 | MeanLoss(100it):  2.18478 | MeanQ:   4.149 | GradNorm:  0.6159\n",
      "[DEBUG] Step   1740 | Ep  174 | Eps 0.983 | MeanReward(10ep):   7.300 | MeanLoss(100it):  2.22301 | MeanQ:   4.238 | GradNorm:  0.6594\n",
      "[DEBUG] Step   1771 | Ep  175 | Eps 0.982 | MeanReward(10ep):   9.800 | MeanLoss(100it):  2.32922 | MeanQ:   4.564 | GradNorm:  0.9333\n",
      "[DEBUG] Step   1781 | Ep  176 | Eps 0.982 | MeanReward(10ep):   9.800 | MeanLoss(100it):  2.37110 | MeanQ:   4.703 | GradNorm:  0.6372\n",
      "[DEBUG] Step   1793 | Ep  177 | Eps 0.982 | MeanReward(10ep):  10.100 | MeanLoss(100it):  2.44440 | MeanQ:   4.848 | GradNorm:  0.7656\n",
      "[DEBUG] Step   1803 | Ep  178 | Eps 0.982 | MeanReward(10ep):  10.500 | MeanLoss(100it):  2.49743 | MeanQ:   4.950 | GradNorm:  0.5758\n",
      "[DEBUG] Step   1809 | Ep  179 | Eps 0.982 | MeanReward(10ep):  10.200 | MeanLoss(100it):  2.50793 | MeanQ:   5.051 | GradNorm:  0.6253\n",
      "[DEBUG] Step   1815 | Ep  180 | Eps 0.982 | MeanReward(10ep):   9.800 | MeanLoss(100it):  2.51566 | MeanQ:   5.099 | GradNorm:  0.2242\n",
      "[cartpole] Ep 180 Steps 1815 RecentMean 9.80 Eps 0.982\n",
      "Eval @ Ep 180 and 1815 steps => mean return 5.00\n",
      "[DEBUG] Step   1850 | Ep  181 | Eps 0.982 | MeanReward(10ep):  12.000 | MeanLoss(100it):  2.75336 | MeanQ:   5.506 | GradNorm:  0.6830\n",
      "[DEBUG] Step   1857 | Ep  182 | Eps 0.982 | MeanReward(10ep):  11.900 | MeanLoss(100it):  2.81262 | MeanQ:   5.665 | GradNorm:  0.6536\n",
      "[DEBUG] Step   1867 | Ep  183 | Eps 0.982 | MeanReward(10ep):  12.300 | MeanLoss(100it):  2.86560 | MeanQ:   5.780 | GradNorm:  0.5591\n",
      "[DEBUG] Step   1877 | Ep  184 | Eps 0.981 | MeanReward(10ep):  12.700 | MeanLoss(100it):  2.95232 | MeanQ:   5.952 | GradNorm:  0.4930\n",
      "[DEBUG] Step   1889 | Ep  185 | Eps 0.981 | MeanReward(10ep):  10.800 | MeanLoss(100it):  3.04315 | MeanQ:   6.123 | GradNorm:  0.3379\n",
      "[DEBUG] Step   1897 | Ep  186 | Eps 0.981 | MeanReward(10ep):  10.600 | MeanLoss(100it):  3.10017 | MeanQ:   6.236 | GradNorm:  0.8341\n",
      "[DEBUG] Step   1904 | Ep  187 | Eps 0.981 | MeanReward(10ep):  10.100 | MeanLoss(100it):  3.11918 | MeanQ:   6.351 | GradNorm:  0.7175\n",
      "[DEBUG] Step   1920 | Ep  188 | Eps 0.981 | MeanReward(10ep):  10.700 | MeanLoss(100it):  3.19842 | MeanQ:   6.582 | GradNorm:  0.5347\n",
      "[DEBUG] Step   1927 | Ep  189 | Eps 0.981 | MeanReward(10ep):  10.800 | MeanLoss(100it):  3.22039 | MeanQ:   6.642 | GradNorm:  0.7534\n",
      "[DEBUG] Step   1943 | Ep  190 | Eps 0.981 | MeanReward(10ep):  11.800 | MeanLoss(100it):  3.31269 | MeanQ:   6.890 | GradNorm:  0.9624\n",
      "[cartpole] Ep 190 Steps 1943 RecentMean 11.80 Eps 0.981\n",
      "[DEBUG] Step   1950 | Ep  191 | Eps 0.981 | MeanReward(10ep):   9.000 | MeanLoss(100it):  3.35201 | MeanQ:   7.018 | GradNorm:  0.3047\n",
      "[DEBUG] Step   1975 | Ep  192 | Eps 0.980 | MeanReward(10ep):  10.800 | MeanLoss(100it):  3.52382 | MeanQ:   7.417 | GradNorm:  0.4056\n",
      "[DEBUG] Step   1998 | Ep  193 | Eps 0.980 | MeanReward(10ep):  12.100 | MeanLoss(100it):  3.62098 | MeanQ:   7.777 | GradNorm:  0.6523\n",
      "[DEBUG] Step   2006 | Ep  194 | Eps 0.980 | MeanReward(10ep):  11.900 | MeanLoss(100it):  3.67628 | MeanQ:   7.908 | GradNorm:  0.7097\n",
      "[DEBUG] Step   2016 | Ep  195 | Eps 0.980 | MeanReward(10ep):  11.700 | MeanLoss(100it):  3.73477 | MeanQ:   8.111 | GradNorm:  0.5476\n",
      "[DEBUG] Step   2022 | Ep  196 | Eps 0.980 | MeanReward(10ep):  11.500 | MeanLoss(100it):  3.74234 | MeanQ:   8.177 | GradNorm:  0.4602\n",
      "[DEBUG] Step   2034 | Ep  197 | Eps 0.980 | MeanReward(10ep):  12.000 | MeanLoss(100it):  3.83969 | MeanQ:   8.405 | GradNorm:  1.2479\n",
      "[DEBUG] Step   2040 | Ep  198 | Eps 0.980 | MeanReward(10ep):  11.000 | MeanLoss(100it):  3.87485 | MeanQ:   8.566 | GradNorm:  0.4695\n",
      "[DEBUG] Step   2047 | Ep  199 | Eps 0.980 | MeanReward(10ep):  11.000 | MeanLoss(100it):  3.90014 | MeanQ:   8.648 | GradNorm:  0.8812\n",
      "[DEBUG] Step   2059 | Ep  200 | Eps 0.980 | MeanReward(10ep):  10.600 | MeanLoss(100it):  3.96984 | MeanQ:   8.891 | GradNorm:  0.6416\n",
      "[cartpole] Ep 200 Steps 2059 RecentMean 10.60 Eps 0.980\n",
      "Eval @ Ep 200 and 2059 steps => mean return 5.00\n",
      "[DEBUG] Step   2067 | Ep  201 | Eps 0.980 | MeanReward(10ep):  10.700 | MeanLoss(100it):  4.02274 | MeanQ:   8.966 | GradNorm:  0.5373\n",
      "[DEBUG] Step   2077 | Ep  202 | Eps 0.979 | MeanReward(10ep):   9.200 | MeanLoss(100it):  4.09662 | MeanQ:   9.302 | GradNorm:  0.8152\n",
      "[DEBUG] Step   2086 | Ep  203 | Eps 0.979 | MeanReward(10ep):   7.800 | MeanLoss(100it):  4.12615 | MeanQ:   9.462 | GradNorm:  0.5594\n",
      "[DEBUG] Step   2102 | Ep  204 | Eps 0.979 | MeanReward(10ep):   8.600 | MeanLoss(100it):  4.21810 | MeanQ:   9.755 | GradNorm:  0.5218\n",
      "[DEBUG] Step   2109 | Ep  205 | Eps 0.979 | MeanReward(10ep):   8.300 | MeanLoss(100it):  4.25142 | MeanQ:   9.896 | GradNorm:  0.9731\n",
      "[DEBUG] Step   2114 | Ep  206 | Eps 0.979 | MeanReward(10ep):   8.200 | MeanLoss(100it):  4.25859 | MeanQ:   9.968 | GradNorm:  0.5518\n",
      "[DEBUG] Step   2121 | Ep  207 | Eps 0.979 | MeanReward(10ep):   7.700 | MeanLoss(100it):  4.31700 | MeanQ:  10.108 | GradNorm:  0.5208\n",
      "[DEBUG] Step   2129 | Ep  208 | Eps 0.979 | MeanReward(10ep):   7.900 | MeanLoss(100it):  4.33917 | MeanQ:  10.254 | GradNorm:  0.9249\n",
      "[DEBUG] Step   2139 | Ep  209 | Eps 0.979 | MeanReward(10ep):   8.200 | MeanLoss(100it):  4.38802 | MeanQ:  10.408 | GradNorm:  1.1044\n",
      "[DEBUG] Step   2145 | Ep  210 | Eps 0.979 | MeanReward(10ep):   7.600 | MeanLoss(100it):  4.41134 | MeanQ:  10.567 | GradNorm:  0.5093\n",
      "[cartpole] Ep 210 Steps 2145 RecentMean 7.60 Eps 0.979\n",
      "[DEBUG] Step   2161 | Ep  211 | Eps 0.979 | MeanReward(10ep):   8.400 | MeanLoss(100it):  4.53062 | MeanQ:  10.900 | GradNorm:  0.6472\n",
      "[DEBUG] Step   2167 | Ep  212 | Eps 0.979 | MeanReward(10ep):   8.000 | MeanLoss(100it):  4.54235 | MeanQ:  10.981 | GradNorm:  0.5887\n",
      "[DEBUG] Step   2173 | Ep  213 | Eps 0.978 | MeanReward(10ep):   7.700 | MeanLoss(100it):  4.61971 | MeanQ:  11.138 | GradNorm:  0.3970\n",
      "[DEBUG] Step   2209 | Ep  214 | Eps 0.978 | MeanReward(10ep):   9.700 | MeanLoss(100it):  4.81752 | MeanQ:  11.835 | GradNorm:  0.7348\n",
      "[DEBUG] Step   2222 | Ep  215 | Eps 0.978 | MeanReward(10ep):  10.300 | MeanLoss(100it):  4.89298 | MeanQ:  12.095 | GradNorm:  1.0347\n",
      "[DEBUG] Step   2238 | Ep  216 | Eps 0.978 | MeanReward(10ep):  11.400 | MeanLoss(100it):  4.92245 | MeanQ:  12.447 | GradNorm:  0.6414\n",
      "[DEBUG] Step   2244 | Ep  217 | Eps 0.978 | MeanReward(10ep):  11.300 | MeanLoss(100it):  4.95598 | MeanQ:  12.622 | GradNorm:  0.9691\n",
      "[DEBUG] Step   2253 | Ep  218 | Eps 0.978 | MeanReward(10ep):  11.400 | MeanLoss(100it):  4.95669 | MeanQ:  12.802 | GradNorm:  0.6655\n",
      "[DEBUG] Step   2259 | Ep  219 | Eps 0.978 | MeanReward(10ep):  11.000 | MeanLoss(100it):  4.98032 | MeanQ:  12.892 | GradNorm:  0.6836\n",
      "[DEBUG] Step   2267 | Ep  220 | Eps 0.978 | MeanReward(10ep):  11.200 | MeanLoss(100it):  5.03089 | MeanQ:  13.079 | GradNorm:  0.7142\n",
      "[cartpole] Ep 220 Steps 2267 RecentMean 11.20 Eps 0.978\n",
      "Eval @ Ep 220 and 2267 steps => mean return 5.00\n",
      "[DEBUG] Step   2279 | Ep  221 | Eps 0.977 | MeanReward(10ep):  10.800 | MeanLoss(100it):  5.05000 | MeanQ:  13.212 | GradNorm:  0.4545\n",
      "[DEBUG] Step   2284 | Ep  222 | Eps 0.977 | MeanReward(10ep):  10.700 | MeanLoss(100it):  5.11783 | MeanQ:  13.535 | GradNorm:  1.0497\n",
      "[DEBUG] Step   2293 | Ep  223 | Eps 0.977 | MeanReward(10ep):  11.000 | MeanLoss(100it):  5.17573 | MeanQ:  13.740 | GradNorm:  1.2106\n",
      "[DEBUG] Step   2300 | Ep  224 | Eps 0.977 | MeanReward(10ep):   8.100 | MeanLoss(100it):  5.20495 | MeanQ:  13.940 | GradNorm:  0.6791\n",
      "[DEBUG] Step   2312 | Ep  225 | Eps 0.977 | MeanReward(10ep):   8.000 | MeanLoss(100it):  5.26183 | MeanQ:  14.217 | GradNorm:  0.5977\n",
      "[DEBUG] Step   2319 | Ep  226 | Eps 0.977 | MeanReward(10ep):   7.100 | MeanLoss(100it):  5.31129 | MeanQ:  14.309 | GradNorm:  0.9762\n",
      "[DEBUG] Step   2324 | Ep  227 | Eps 0.977 | MeanReward(10ep):   7.000 | MeanLoss(100it):  5.33053 | MeanQ:  14.489 | GradNorm:  0.5858\n",
      "[DEBUG] Step   2342 | Ep  228 | Eps 0.977 | MeanReward(10ep):   7.900 | MeanLoss(100it):  5.39592 | MeanQ:  14.881 | GradNorm:  0.8705\n",
      "[DEBUG] Step   2353 | Ep  229 | Eps 0.977 | MeanReward(10ep):   8.400 | MeanLoss(100it):  5.45739 | MeanQ:  15.191 | GradNorm:  1.0584\n",
      "[DEBUG] Step   2361 | Ep  230 | Eps 0.977 | MeanReward(10ep):   8.400 | MeanLoss(100it):  5.48045 | MeanQ:  15.408 | GradNorm:  0.7715\n",
      "[cartpole] Ep 230 Steps 2361 RecentMean 8.40 Eps 0.977\n",
      "[DEBUG] Step   2369 | Ep  231 | Eps 0.977 | MeanReward(10ep):   8.000 | MeanLoss(100it):  5.47885 | MeanQ:  15.633 | GradNorm:  0.7367\n",
      "[DEBUG] Step   2376 | Ep  232 | Eps 0.976 | MeanReward(10ep):   8.200 | MeanLoss(100it):  5.52639 | MeanQ:  15.874 | GradNorm:  0.6916\n",
      "[DEBUG] Step   2383 | Ep  233 | Eps 0.976 | MeanReward(10ep):   8.000 | MeanLoss(100it):  5.55440 | MeanQ:  15.990 | GradNorm:  0.5793\n",
      "[DEBUG] Step   2405 | Ep  234 | Eps 0.976 | MeanReward(10ep):   9.500 | MeanLoss(100it):  5.64720 | MeanQ:  16.656 | GradNorm:  0.5582\n",
      "[DEBUG] Step   2411 | Ep  235 | Eps 0.976 | MeanReward(10ep):   8.900 | MeanLoss(100it):  5.68166 | MeanQ:  16.765 | GradNorm:  1.0694\n",
      "[DEBUG] Step   2432 | Ep  236 | Eps 0.976 | MeanReward(10ep):  10.300 | MeanLoss(100it):  5.76957 | MeanQ:  17.434 | GradNorm:  0.8735\n",
      "[DEBUG] Step   2442 | Ep  237 | Eps 0.976 | MeanReward(10ep):  10.800 | MeanLoss(100it):  5.80820 | MeanQ:  17.653 | GradNorm:  1.2675\n",
      "[DEBUG] Step   2451 | Ep  238 | Eps 0.976 | MeanReward(10ep):   9.900 | MeanLoss(100it):  5.81775 | MeanQ:  17.872 | GradNorm:  0.4489\n",
      "[DEBUG] Step   2477 | Ep  239 | Eps 0.975 | MeanReward(10ep):  11.400 | MeanLoss(100it):  5.84224 | MeanQ:  18.595 | GradNorm:  0.8474\n",
      "[DEBUG] Step   2483 | Ep  240 | Eps 0.975 | MeanReward(10ep):  11.200 | MeanLoss(100it):  5.85162 | MeanQ:  18.697 | GradNorm:  0.6728\n",
      "[cartpole] Ep 240 Steps 2483 RecentMean 11.20 Eps 0.975\n",
      "Eval @ Ep 240 and 2483 steps => mean return 5.00\n",
      "[DEBUG] Step   2491 | Ep  241 | Eps 0.975 | MeanReward(10ep):  11.200 | MeanLoss(100it):  5.90049 | MeanQ:  18.709 | GradNorm:  1.0929\n",
      "[DEBUG] Step   2501 | Ep  242 | Eps 0.975 | MeanReward(10ep):  11.500 | MeanLoss(100it):  5.94098 | MeanQ:  19.220 | GradNorm:  0.8622\n",
      "[DEBUG] Step   2507 | Ep  243 | Eps 0.975 | MeanReward(10ep):  11.400 | MeanLoss(100it):  5.97495 | MeanQ:  19.335 | GradNorm:  1.4760\n",
      "[DEBUG] Step   2515 | Ep  244 | Eps 0.975 | MeanReward(10ep):  10.000 | MeanLoss(100it):  6.00692 | MeanQ:  19.573 | GradNorm:  0.4187\n",
      "[DEBUG] Step   2522 | Ep  245 | Eps 0.975 | MeanReward(10ep):  10.100 | MeanLoss(100it):  6.00960 | MeanQ:  19.828 | GradNorm:  0.8890\n",
      "[DEBUG] Step   2527 | Ep  246 | Eps 0.975 | MeanReward(10ep):   8.500 | MeanLoss(100it):  6.06019 | MeanQ:  19.970 | GradNorm:  1.6094\n",
      "[DEBUG] Step   2535 | Ep  247 | Eps 0.975 | MeanReward(10ep):   8.300 | MeanLoss(100it):  6.09997 | MeanQ:  20.248 | GradNorm:  0.8718\n",
      "[DEBUG] Step   2544 | Ep  248 | Eps 0.975 | MeanReward(10ep):   8.300 | MeanLoss(100it):  6.10973 | MeanQ:  20.648 | GradNorm:  1.0992\n",
      "[DEBUG] Step   2560 | Ep  249 | Eps 0.975 | MeanReward(10ep):   7.300 | MeanLoss(100it):  6.11496 | MeanQ:  21.154 | GradNorm:  0.8573\n",
      "[DEBUG] Step   2570 | Ep  250 | Eps 0.975 | MeanReward(10ep):   7.700 | MeanLoss(100it):  6.16608 | MeanQ:  21.423 | GradNorm:  1.4641\n",
      "[cartpole] Ep 250 Steps 2570 RecentMean 7.70 Eps 0.975\n",
      "[DEBUG] Step   2576 | Ep  251 | Eps 0.975 | MeanReward(10ep):   7.500 | MeanLoss(100it):  6.16393 | MeanQ:  21.684 | GradNorm:  1.0214\n",
      "[DEBUG] Step   2584 | Ep  252 | Eps 0.974 | MeanReward(10ep):   7.300 | MeanLoss(100it):  6.14772 | MeanQ:  21.929 | GradNorm:  0.5906\n",
      "[DEBUG] Step   2591 | Ep  253 | Eps 0.974 | MeanReward(10ep):   7.400 | MeanLoss(100it):  6.14126 | MeanQ:  22.049 | GradNorm:  0.7135\n",
      "[DEBUG] Step   2600 | Ep  254 | Eps 0.974 | MeanReward(10ep):   7.500 | MeanLoss(100it):  6.19186 | MeanQ:  22.411 | GradNorm:  1.1575\n",
      "[DEBUG] Step   2606 | Ep  255 | Eps 0.974 | MeanReward(10ep):   7.400 | MeanLoss(100it):  6.21170 | MeanQ:  22.538 | GradNorm:  0.8747\n",
      "[DEBUG] Step   2619 | Ep  256 | Eps 0.974 | MeanReward(10ep):   8.200 | MeanLoss(100it):  6.24390 | MeanQ:  22.941 | GradNorm:  1.1011\n",
      "[DEBUG] Step   2635 | Ep  257 | Eps 0.974 | MeanReward(10ep):   9.000 | MeanLoss(100it):  6.29159 | MeanQ:  23.534 | GradNorm:  1.4428\n",
      "[DEBUG] Step   2643 | Ep  258 | Eps 0.974 | MeanReward(10ep):   8.900 | MeanLoss(100it):  6.31553 | MeanQ:  23.862 | GradNorm:  1.3907\n",
      "[DEBUG] Step   2653 | Ep  259 | Eps 0.974 | MeanReward(10ep):   8.300 | MeanLoss(100it):  6.37971 | MeanQ:  24.409 | GradNorm:  1.8503\n",
      "[DEBUG] Step   2665 | Ep  260 | Eps 0.974 | MeanReward(10ep):   8.500 | MeanLoss(100it):  6.40288 | MeanQ:  24.991 | GradNorm:  1.3566\n",
      "[cartpole] Ep 260 Steps 2665 RecentMean 8.50 Eps 0.974\n",
      "Eval @ Ep 260 and 2665 steps => mean return 5.00\n",
      "[DEBUG] Step   2673 | Ep  261 | Eps 0.974 | MeanReward(10ep):   8.700 | MeanLoss(100it):  6.40082 | MeanQ:  25.074 | GradNorm:  1.0899\n",
      "[DEBUG] Step   2686 | Ep  262 | Eps 0.973 | MeanReward(10ep):   9.200 | MeanLoss(100it):  6.35770 | MeanQ:  25.897 | GradNorm:  0.8136\n",
      "[DEBUG] Step   2693 | Ep  263 | Eps 0.973 | MeanReward(10ep):   9.200 | MeanLoss(100it):  6.34828 | MeanQ:  26.220 | GradNorm:  0.8057\n",
      "[DEBUG] Step   2699 | Ep  264 | Eps 0.973 | MeanReward(10ep):   8.900 | MeanLoss(100it):  6.39944 | MeanQ:  26.398 | GradNorm:  2.1191\n",
      "[DEBUG] Step   2709 | Ep  265 | Eps 0.973 | MeanReward(10ep):   9.300 | MeanLoss(100it):  6.39404 | MeanQ:  26.919 | GradNorm:  0.7407\n",
      "[DEBUG] Step   2719 | Ep  266 | Eps 0.973 | MeanReward(10ep):   9.000 | MeanLoss(100it):  6.39439 | MeanQ:  27.248 | GradNorm:  0.7997\n",
      "[DEBUG] Step   2727 | Ep  267 | Eps 0.973 | MeanReward(10ep):   8.200 | MeanLoss(100it):  6.41478 | MeanQ:  27.568 | GradNorm:  1.0966\n",
      "[DEBUG] Step   2733 | Ep  268 | Eps 0.973 | MeanReward(10ep):   8.000 | MeanLoss(100it):  6.45630 | MeanQ:  27.894 | GradNorm:  1.2372\n",
      "[DEBUG] Step   2741 | Ep  269 | Eps 0.973 | MeanReward(10ep):   7.800 | MeanLoss(100it):  6.44264 | MeanQ:  28.205 | GradNorm:  0.6209\n",
      "[DEBUG] Step   2747 | Ep  270 | Eps 0.973 | MeanReward(10ep):   7.200 | MeanLoss(100it):  6.45590 | MeanQ:  28.370 | GradNorm:  1.6680\n",
      "[cartpole] Ep 270 Steps 2747 RecentMean 7.20 Eps 0.973\n",
      "[DEBUG] Step   2765 | Ep  271 | Eps 0.973 | MeanReward(10ep):   8.200 | MeanLoss(100it):  6.50235 | MeanQ:  29.213 | GradNorm:  1.4125\n",
      "[DEBUG] Step   2783 | Ep  272 | Eps 0.972 | MeanReward(10ep):   8.700 | MeanLoss(100it):  6.51484 | MeanQ:  29.916 | GradNorm:  0.6701\n",
      "[DEBUG] Step   2791 | Ep  273 | Eps 0.972 | MeanReward(10ep):   8.800 | MeanLoss(100it):  6.50153 | MeanQ:  30.230 | GradNorm:  0.5810\n",
      "[DEBUG] Step   2797 | Ep  274 | Eps 0.972 | MeanReward(10ep):   8.800 | MeanLoss(100it):  6.49473 | MeanQ:  30.540 | GradNorm:  0.6303\n",
      "[DEBUG] Step   2804 | Ep  275 | Eps 0.972 | MeanReward(10ep):   8.500 | MeanLoss(100it):  6.48487 | MeanQ:  30.836 | GradNorm:  0.7588\n",
      "[DEBUG] Step   2810 | Ep  276 | Eps 0.972 | MeanReward(10ep):   8.100 | MeanLoss(100it):  6.44191 | MeanQ:  30.984 | GradNorm:  0.8627\n",
      "[DEBUG] Step   2825 | Ep  277 | Eps 0.972 | MeanReward(10ep):   8.800 | MeanLoss(100it):  6.40309 | MeanQ:  31.576 | GradNorm:  1.6569\n",
      "[DEBUG] Step   2831 | Ep  278 | Eps 0.972 | MeanReward(10ep):   8.800 | MeanLoss(100it):  6.43292 | MeanQ:  31.742 | GradNorm:  1.2793\n",
      "[DEBUG] Step   2840 | Ep  279 | Eps 0.972 | MeanReward(10ep):   8.900 | MeanLoss(100it):  6.41399 | MeanQ:  32.257 | GradNorm:  0.9822\n",
      "[DEBUG] Step   2851 | Ep  280 | Eps 0.972 | MeanReward(10ep):   9.400 | MeanLoss(100it):  6.46678 | MeanQ:  32.638 | GradNorm:  1.4940\n",
      "[cartpole] Ep 280 Steps 2851 RecentMean 9.40 Eps 0.972\n",
      "Eval @ Ep 280 and 2851 steps => mean return 5.00\n",
      "[DEBUG] Step   2869 | Ep  281 | Eps 0.972 | MeanReward(10ep):   9.400 | MeanLoss(100it):  6.49510 | MeanQ:  33.143 | GradNorm:  1.0434\n",
      "[DEBUG] Step   2888 | Ep  282 | Eps 0.971 | MeanReward(10ep):   9.500 | MeanLoss(100it):  6.45829 | MeanQ:  34.434 | GradNorm:  1.6551\n",
      "[DEBUG] Step   2915 | Ep  283 | Eps 0.971 | MeanReward(10ep):  11.400 | MeanLoss(100it):  6.36196 | MeanQ:  35.429 | GradNorm:  0.8591\n",
      "[DEBUG] Step   2922 | Ep  284 | Eps 0.971 | MeanReward(10ep):  11.500 | MeanLoss(100it):  6.35243 | MeanQ:  35.786 | GradNorm:  1.1804\n",
      "[DEBUG] Step   2929 | Ep  285 | Eps 0.971 | MeanReward(10ep):  11.500 | MeanLoss(100it):  6.33323 | MeanQ:  36.157 | GradNorm:  0.9675\n",
      "[DEBUG] Step   2938 | Ep  286 | Eps 0.971 | MeanReward(10ep):  11.800 | MeanLoss(100it):  6.30642 | MeanQ:  36.536 | GradNorm:  1.4893\n",
      "[DEBUG] Step   2944 | Ep  287 | Eps 0.971 | MeanReward(10ep):  10.900 | MeanLoss(100it):  6.33962 | MeanQ:  36.925 | GradNorm:  1.7874\n",
      "[DEBUG] Step   2963 | Ep  288 | Eps 0.971 | MeanReward(10ep):  12.200 | MeanLoss(100it):  6.37621 | MeanQ:  37.755 | GradNorm:  2.2376\n",
      "[DEBUG] Step   2984 | Ep  289 | Eps 0.970 | MeanReward(10ep):  13.400 | MeanLoss(100it):  6.35726 | MeanQ:  38.945 | GradNorm:  0.8817\n",
      "[DEBUG] Step   2990 | Ep  290 | Eps 0.970 | MeanReward(10ep):  12.900 | MeanLoss(100it):  6.36952 | MeanQ:  39.133 | GradNorm:  0.8042\n",
      "[cartpole] Ep 290 Steps 2990 RecentMean 12.90 Eps 0.970\n",
      "[DEBUG] Step   3003 | Ep  291 | Eps 0.970 | MeanReward(10ep):  12.400 | MeanLoss(100it):  6.36688 | MeanQ:  39.761 | GradNorm:  1.1445\n",
      "[DEBUG] Step   3010 | Ep  292 | Eps 0.970 | MeanReward(10ep):  11.200 | MeanLoss(100it):  6.32782 | MeanQ:  40.162 | GradNorm:  0.9772\n",
      "[DEBUG] Step   3022 | Ep  293 | Eps 0.970 | MeanReward(10ep):   9.700 | MeanLoss(100it):  6.29218 | MeanQ:  40.737 | GradNorm:  0.5528\n",
      "[DEBUG] Step   3028 | Ep  294 | Eps 0.970 | MeanReward(10ep):   9.600 | MeanLoss(100it):  6.25319 | MeanQ:  41.094 | GradNorm:  2.1921\n",
      "[DEBUG] Step   3037 | Ep  295 | Eps 0.970 | MeanReward(10ep):   9.800 | MeanLoss(100it):  6.20958 | MeanQ:  41.444 | GradNorm:  1.4275\n",
      "[DEBUG] Step   3043 | Ep  296 | Eps 0.970 | MeanReward(10ep):   9.500 | MeanLoss(100it):  6.18321 | MeanQ:  41.618 | GradNorm:  0.7602\n",
      "[DEBUG] Step   3049 | Ep  297 | Eps 0.970 | MeanReward(10ep):   9.500 | MeanLoss(100it):  6.13765 | MeanQ:  42.016 | GradNorm:  1.2756\n",
      "[DEBUG] Step   3054 | Ep  298 | Eps 0.970 | MeanReward(10ep):   8.100 | MeanLoss(100it):  6.13525 | MeanQ:  42.242 | GradNorm:  2.0378\n",
      "[DEBUG] Step   3061 | Ep  299 | Eps 0.970 | MeanReward(10ep):   6.700 | MeanLoss(100it):  6.09572 | MeanQ:  42.660 | GradNorm:  0.7644\n",
      "[DEBUG] Step   3069 | Ep  300 | Eps 0.970 | MeanReward(10ep):   6.900 | MeanLoss(100it):  6.06190 | MeanQ:  43.064 | GradNorm:  1.7213\n",
      "[cartpole] Ep 300 Steps 3069 RecentMean 6.90 Eps 0.970\n",
      "Eval @ Ep 300 and 3069 steps => mean return 5.00\n",
      "[DEBUG] Step   3078 | Ep  301 | Eps 0.970 | MeanReward(10ep):   6.500 | MeanLoss(100it):  6.06148 | MeanQ:  42.915 | GradNorm:  1.7103\n",
      "[DEBUG] Step   3085 | Ep  302 | Eps 0.969 | MeanReward(10ep):   6.500 | MeanLoss(100it):  6.07425 | MeanQ:  43.889 | GradNorm:  0.6688\n",
      "[DEBUG] Step   3091 | Ep  303 | Eps 0.969 | MeanReward(10ep):   5.900 | MeanLoss(100it):  6.06658 | MeanQ:  44.087 | GradNorm:  1.2229\n",
      "[DEBUG] Step   3100 | Ep  304 | Eps 0.969 | MeanReward(10ep):   6.200 | MeanLoss(100it):  6.03851 | MeanQ:  44.655 | GradNorm:  0.5313\n",
      "[DEBUG] Step   3111 | Ep  305 | Eps 0.969 | MeanReward(10ep):   6.400 | MeanLoss(100it):  6.02276 | MeanQ:  45.050 | GradNorm:  1.9076\n",
      "[DEBUG] Step   3117 | Ep  306 | Eps 0.969 | MeanReward(10ep):   6.400 | MeanLoss(100it):  6.01183 | MeanQ:  45.524 | GradNorm:  1.6905\n",
      "[DEBUG] Step   3124 | Ep  307 | Eps 0.969 | MeanReward(10ep):   6.500 | MeanLoss(100it):  5.99690 | MeanQ:  46.040 | GradNorm:  2.0624\n",
      "[DEBUG] Step   3131 | Ep  308 | Eps 0.969 | MeanReward(10ep):   6.700 | MeanLoss(100it):  5.94973 | MeanQ:  46.303 | GradNorm:  1.2950\n",
      "[DEBUG] Step   3143 | Ep  309 | Eps 0.969 | MeanReward(10ep):   7.200 | MeanLoss(100it):  5.90378 | MeanQ:  47.062 | GradNorm:  0.8071\n",
      "[DEBUG] Step   3149 | Ep  310 | Eps 0.969 | MeanReward(10ep):   7.000 | MeanLoss(100it):  5.81519 | MeanQ:  47.510 | GradNorm:  0.6808\n",
      "[cartpole] Ep 310 Steps 3149 RecentMean 7.00 Eps 0.969\n",
      "[DEBUG] Step   3154 | Ep  311 | Eps 0.969 | MeanReward(10ep):   6.600 | MeanLoss(100it):  5.81335 | MeanQ:  47.749 | GradNorm:  2.3019\n",
      "[DEBUG] Step   3177 | Ep  312 | Eps 0.969 | MeanReward(10ep):   8.200 | MeanLoss(100it):  5.71853 | MeanQ:  49.075 | GradNorm:  2.1985\n",
      "[DEBUG] Step   3186 | Ep  313 | Eps 0.968 | MeanReward(10ep):   8.500 | MeanLoss(100it):  5.70749 | MeanQ:  49.479 | GradNorm:  0.9447\n",
      "[DEBUG] Step   3192 | Ep  314 | Eps 0.968 | MeanReward(10ep):   8.200 | MeanLoss(100it):  5.70915 | MeanQ:  49.872 | GradNorm:  0.8690\n",
      "[DEBUG] Step   3197 | Ep  315 | Eps 0.968 | MeanReward(10ep):   7.600 | MeanLoss(100it):  5.71332 | MeanQ:  50.052 | GradNorm:  1.0248\n",
      "[DEBUG] Step   3208 | Ep  316 | Eps 0.968 | MeanReward(10ep):   8.100 | MeanLoss(100it):  5.68565 | MeanQ:  50.526 | GradNorm:  1.4661\n",
      "[DEBUG] Step   3229 | Ep  317 | Eps 0.968 | MeanReward(10ep):   9.500 | MeanLoss(100it):  5.53301 | MeanQ:  51.597 | GradNorm:  1.5893\n",
      "[DEBUG] Step   3236 | Ep  318 | Eps 0.968 | MeanReward(10ep):   9.500 | MeanLoss(100it):  5.47737 | MeanQ:  52.067 | GradNorm:  1.2439\n",
      "[DEBUG] Step   3242 | Ep  319 | Eps 0.968 | MeanReward(10ep):   8.900 | MeanLoss(100it):  5.45036 | MeanQ:  52.313 | GradNorm:  1.5008\n",
      "[DEBUG] Step   3248 | Ep  320 | Eps 0.968 | MeanReward(10ep):   8.900 | MeanLoss(100it):  5.33722 | MeanQ:  52.800 | GradNorm:  1.1615\n",
      "[cartpole] Ep 320 Steps 3248 RecentMean 8.90 Eps 0.968\n",
      "Eval @ Ep 320 and 3248 steps => mean return 5.00\n",
      "[DEBUG] Step   3256 | Ep  321 | Eps 0.968 | MeanReward(10ep):   9.200 | MeanLoss(100it):  5.32596 | MeanQ:  52.559 | GradNorm:  1.5507\n",
      "[DEBUG] Step   3278 | Ep  322 | Eps 0.968 | MeanReward(10ep):   9.100 | MeanLoss(100it):  5.21540 | MeanQ:  54.359 | GradNorm:  0.8035\n",
      "[DEBUG] Step   3290 | Ep  323 | Eps 0.967 | MeanReward(10ep):   9.400 | MeanLoss(100it):  5.15101 | MeanQ:  54.939 | GradNorm:  1.4523\n",
      "[DEBUG] Step   3303 | Ep  324 | Eps 0.967 | MeanReward(10ep):  10.100 | MeanLoss(100it):  5.09715 | MeanQ:  55.488 | GradNorm:  0.7787\n",
      "[DEBUG] Step   3314 | Ep  325 | Eps 0.967 | MeanReward(10ep):  10.700 | MeanLoss(100it):  5.00724 | MeanQ:  56.117 | GradNorm:  2.2397\n",
      "[DEBUG] Step   3322 | Ep  326 | Eps 0.967 | MeanReward(10ep):  10.400 | MeanLoss(100it):  4.92729 | MeanQ:  56.592 | GradNorm:  1.3309\n",
      "[DEBUG] Step   3328 | Ep  327 | Eps 0.967 | MeanReward(10ep):   8.900 | MeanLoss(100it):  4.83140 | MeanQ:  57.031 | GradNorm:  0.5261\n",
      "[DEBUG] Step   3337 | Ep  328 | Eps 0.967 | MeanReward(10ep):   9.100 | MeanLoss(100it):  4.75602 | MeanQ:  57.420 | GradNorm:  1.1563\n",
      "[DEBUG] Step   3350 | Ep  329 | Eps 0.967 | MeanReward(10ep):   9.800 | MeanLoss(100it):  4.60594 | MeanQ:  57.895 | GradNorm:  0.9116\n",
      "[DEBUG] Step   3357 | Ep  330 | Eps 0.967 | MeanReward(10ep):   9.900 | MeanLoss(100it):  4.50205 | MeanQ:  58.091 | GradNorm:  0.7261\n",
      "[cartpole] Ep 330 Steps 3357 RecentMean 9.90 Eps 0.967\n",
      "[DEBUG] Step   3373 | Ep  331 | Eps 0.967 | MeanReward(10ep):  10.700 | MeanLoss(100it):  4.33666 | MeanQ:  58.353 | GradNorm:  0.6248\n",
      "[DEBUG] Step   3379 | Ep  332 | Eps 0.967 | MeanReward(10ep):   9.100 | MeanLoss(100it):  4.29970 | MeanQ:  58.403 | GradNorm:  0.7218\n",
      "[DEBUG] Step   3392 | Ep  333 | Eps 0.966 | MeanReward(10ep):   9.200 | MeanLoss(100it):  4.07916 | MeanQ:  58.629 | GradNorm:  0.8765\n",
      "[DEBUG] Step   3401 | Ep  334 | Eps 0.966 | MeanReward(10ep):   8.800 | MeanLoss(100it):  3.99747 | MeanQ:  58.763 | GradNorm:  0.6637\n",
      "[DEBUG] Step   3408 | Ep  335 | Eps 0.966 | MeanReward(10ep):   8.400 | MeanLoss(100it):  3.93060 | MeanQ:  58.810 | GradNorm:  1.6813\n",
      "[DEBUG] Step   3415 | Ep  336 | Eps 0.966 | MeanReward(10ep):   8.300 | MeanLoss(100it):  3.88558 | MeanQ:  58.799 | GradNorm:  1.7268\n",
      "[DEBUG] Step   3423 | Ep  337 | Eps 0.966 | MeanReward(10ep):   8.500 | MeanLoss(100it):  3.80381 | MeanQ:  58.765 | GradNorm:  0.6132\n",
      "[DEBUG] Step   3429 | Ep  338 | Eps 0.966 | MeanReward(10ep):   8.200 | MeanLoss(100it):  3.71519 | MeanQ:  58.785 | GradNorm:  0.8369\n",
      "[DEBUG] Step   3442 | Ep  339 | Eps 0.966 | MeanReward(10ep):   8.200 | MeanLoss(100it):  3.62126 | MeanQ:  58.889 | GradNorm:  1.9323\n",
      "[DEBUG] Step   3452 | Ep  340 | Eps 0.966 | MeanReward(10ep):   8.500 | MeanLoss(100it):  3.46189 | MeanQ:  59.075 | GradNorm:  0.7182\n",
      "[cartpole] Ep 340 Steps 3452 RecentMean 8.50 Eps 0.966\n",
      "Eval @ Ep 340 and 3452 steps => mean return 5.00\n",
      "[DEBUG] Step   3458 | Ep  341 | Eps 0.966 | MeanReward(10ep):   7.500 | MeanLoss(100it):  3.39819 | MeanQ:  58.267 | GradNorm:  0.9829\n",
      "[DEBUG] Step   3469 | Ep  342 | Eps 0.966 | MeanReward(10ep):   8.000 | MeanLoss(100it):  3.29656 | MeanQ:  59.100 | GradNorm:  1.1715\n",
      "[DEBUG] Step   3483 | Ep  343 | Eps 0.966 | MeanReward(10ep):   8.100 | MeanLoss(100it):  3.13090 | MeanQ:  58.931 | GradNorm:  1.4191\n",
      "[DEBUG] Step   3493 | Ep  344 | Eps 0.965 | MeanReward(10ep):   8.200 | MeanLoss(100it):  3.02138 | MeanQ:  58.585 | GradNorm:  2.1980\n",
      "[DEBUG] Step   3500 | Ep  345 | Eps 0.965 | MeanReward(10ep):   8.200 | MeanLoss(100it):  2.92018 | MeanQ:  58.386 | GradNorm:  1.4989\n",
      "[DEBUG] Step   3505 | Ep  346 | Eps 0.965 | MeanReward(10ep):   8.000 | MeanLoss(100it):  2.88015 | MeanQ:  58.252 | GradNorm:  2.6431\n",
      "[DEBUG] Step   3523 | Ep  347 | Eps 0.965 | MeanReward(10ep):   9.000 | MeanLoss(100it):  2.64758 | MeanQ:  57.685 | GradNorm:  0.9643\n",
      "[DEBUG] Step   3529 | Ep  348 | Eps 0.965 | MeanReward(10ep):   9.000 | MeanLoss(100it):  2.56285 | MeanQ:  57.473 | GradNorm:  1.0180\n",
      "[DEBUG] Step   3534 | Ep  349 | Eps 0.965 | MeanReward(10ep):   8.200 | MeanLoss(100it):  2.49725 | MeanQ:  57.393 | GradNorm:  0.7384\n",
      "[DEBUG] Step   3547 | Ep  350 | Eps 0.965 | MeanReward(10ep):   8.500 | MeanLoss(100it):  2.40476 | MeanQ:  57.332 | GradNorm:  0.8275\n",
      "[cartpole] Ep 350 Steps 3547 RecentMean 8.50 Eps 0.965\n",
      "[DEBUG] Step   3562 | Ep  351 | Eps 0.965 | MeanReward(10ep):   9.400 | MeanLoss(100it):  2.24376 | MeanQ:  57.387 | GradNorm:  1.0781\n",
      "[DEBUG] Step   3567 | Ep  352 | Eps 0.965 | MeanReward(10ep):   8.800 | MeanLoss(100it):  2.21515 | MeanQ:  57.376 | GradNorm:  0.7216\n",
      "[DEBUG] Step   3573 | Ep  353 | Eps 0.965 | MeanReward(10ep):   8.000 | MeanLoss(100it):  2.12836 | MeanQ:  57.303 | GradNorm:  0.7605\n",
      "[DEBUG] Step   3579 | Ep  354 | Eps 0.965 | MeanReward(10ep):   7.600 | MeanLoss(100it):  2.06897 | MeanQ:  57.243 | GradNorm:  1.3390\n",
      "[DEBUG] Step   3588 | Ep  355 | Eps 0.964 | MeanReward(10ep):   7.800 | MeanLoss(100it):  1.92481 | MeanQ:  57.055 | GradNorm:  0.9237\n",
      "[DEBUG] Step   3596 | Ep  356 | Eps 0.964 | MeanReward(10ep):   8.100 | MeanLoss(100it):  1.83548 | MeanQ:  56.961 | GradNorm:  0.8333\n",
      "[DEBUG] Step   3602 | Ep  357 | Eps 0.964 | MeanReward(10ep):   6.900 | MeanLoss(100it):  1.77592 | MeanQ:  56.914 | GradNorm:  0.5043\n",
      "[DEBUG] Step   3620 | Ep  358 | Eps 0.964 | MeanReward(10ep):   8.100 | MeanLoss(100it):  1.61297 | MeanQ:  56.587 | GradNorm:  0.7797\n",
      "[DEBUG] Step   3648 | Ep  359 | Eps 0.964 | MeanReward(10ep):  10.400 | MeanLoss(100it):  1.41687 | MeanQ:  56.878 | GradNorm:  0.6203\n",
      "[DEBUG] Step   3660 | Ep  360 | Eps 0.964 | MeanReward(10ep):  10.300 | MeanLoss(100it):  1.28259 | MeanQ:  57.129 | GradNorm:  0.5123\n",
      "[cartpole] Ep 360 Steps 3660 RecentMean 10.30 Eps 0.964\n",
      "Eval @ Ep 360 and 3660 steps => mean return 8.00\n",
      "[DEBUG] Step   3667 | Ep  361 | Eps 0.964 | MeanReward(10ep):   9.500 | MeanLoss(100it):  1.26053 | MeanQ:  56.406 | GradNorm:  0.8032\n",
      "[DEBUG] Step   3675 | Ep  362 | Eps 0.964 | MeanReward(10ep):   9.800 | MeanLoss(100it):  1.18687 | MeanQ:  57.266 | GradNorm:  0.5130\n",
      "[DEBUG] Step   3688 | Ep  363 | Eps 0.963 | MeanReward(10ep):  10.500 | MeanLoss(100it):  1.07429 | MeanQ:  57.219 | GradNorm:  0.2473\n",
      "[DEBUG] Step   3699 | Ep  364 | Eps 0.963 | MeanReward(10ep):  11.000 | MeanLoss(100it):  1.01711 | MeanQ:  57.155 | GradNorm:  0.3431\n",
      "[DEBUG] Step   3705 | Ep  365 | Eps 0.963 | MeanReward(10ep):  10.700 | MeanLoss(100it):  0.95656 | MeanQ:  57.150 | GradNorm:  0.9970\n",
      "[DEBUG] Step   3717 | Ep  366 | Eps 0.963 | MeanReward(10ep):  11.100 | MeanLoss(100it):  0.90274 | MeanQ:  57.039 | GradNorm:  0.4034\n",
      "[DEBUG] Step   3724 | Ep  367 | Eps 0.963 | MeanReward(10ep):  11.200 | MeanLoss(100it):  0.86714 | MeanQ:  56.944 | GradNorm:  0.8140\n",
      "[DEBUG] Step   3733 | Ep  368 | Eps 0.963 | MeanReward(10ep):  10.300 | MeanLoss(100it):  0.84085 | MeanQ:  56.862 | GradNorm:  0.6144\n",
      "[DEBUG] Step   3741 | Ep  369 | Eps 0.963 | MeanReward(10ep):   8.300 | MeanLoss(100it):  0.80934 | MeanQ:  56.826 | GradNorm:  0.4666\n",
      "[DEBUG] Step   3752 | Ep  370 | Eps 0.963 | MeanReward(10ep):   8.200 | MeanLoss(100it):  0.76511 | MeanQ:  56.853 | GradNorm:  0.3468\n",
      "[cartpole] Ep 370 Steps 3752 RecentMean 8.20 Eps 0.963\n",
      "[DEBUG] Step   3770 | Ep  371 | Eps 0.963 | MeanReward(10ep):   9.300 | MeanLoss(100it):  0.71857 | MeanQ:  56.847 | GradNorm:  0.5746\n",
      "[DEBUG] Step   3784 | Ep  372 | Eps 0.963 | MeanReward(10ep):   9.900 | MeanLoss(100it):  0.66971 | MeanQ:  56.726 | GradNorm:  0.4510\n",
      "[DEBUG] Step   3807 | Ep  373 | Eps 0.962 | MeanReward(10ep):  10.900 | MeanLoss(100it):  0.60274 | MeanQ:  56.455 | GradNorm:  1.2487\n",
      "[DEBUG] Step   3825 | Ep  374 | Eps 0.962 | MeanReward(10ep):  11.600 | MeanLoss(100it):  0.56454 | MeanQ:  55.934 | GradNorm:  0.8440\n",
      "[DEBUG] Step   3831 | Ep  375 | Eps 0.962 | MeanReward(10ep):  11.600 | MeanLoss(100it):  0.55763 | MeanQ:  55.899 | GradNorm:  1.2873\n",
      "[DEBUG] Step   3838 | Ep  376 | Eps 0.962 | MeanReward(10ep):  11.100 | MeanLoss(100it):  0.54329 | MeanQ:  55.927 | GradNorm:  1.4870\n",
      "[DEBUG] Step   3846 | Ep  377 | Eps 0.962 | MeanReward(10ep):  11.200 | MeanLoss(100it):  0.52974 | MeanQ:  56.042 | GradNorm:  1.1156\n",
      "[DEBUG] Step   3855 | Ep  378 | Eps 0.962 | MeanReward(10ep):  11.200 | MeanLoss(100it):  0.51692 | MeanQ:  56.148 | GradNorm:  0.8030\n",
      "[DEBUG] Step   3860 | Ep  379 | Eps 0.962 | MeanReward(10ep):  10.900 | MeanLoss(100it):  0.50497 | MeanQ:  56.221 | GradNorm:  0.8981\n",
      "[DEBUG] Step   3888 | Ep  380 | Eps 0.962 | MeanReward(10ep):  12.600 | MeanLoss(100it):  0.46838 | MeanQ:  56.096 | GradNorm:  0.5509\n",
      "[cartpole] Ep 380 Steps 3888 RecentMean 12.60 Eps 0.962\n",
      "Eval @ Ep 380 and 3888 steps => mean return 8.00\n",
      "[DEBUG] Step   3897 | Ep  381 | Eps 0.961 | MeanReward(10ep):  11.700 | MeanLoss(100it):  0.45499 | MeanQ:  55.453 | GradNorm:  1.1134\n",
      "[DEBUG] Step   3905 | Ep  382 | Eps 0.961 | MeanReward(10ep):  11.100 | MeanLoss(100it):  0.44521 | MeanQ:  56.219 | GradNorm:  1.5836\n",
      "[DEBUG] Step   3917 | Ep  383 | Eps 0.961 | MeanReward(10ep):  10.000 | MeanLoss(100it):  0.43949 | MeanQ:  56.106 | GradNorm:  1.7046\n",
      "[DEBUG] Step   3928 | Ep  384 | Eps 0.961 | MeanReward(10ep):   9.300 | MeanLoss(100it):  0.43256 | MeanQ:  55.991 | GradNorm:  1.0989\n",
      "[DEBUG] Step   3943 | Ep  385 | Eps 0.961 | MeanReward(10ep):  10.200 | MeanLoss(100it):  0.42317 | MeanQ:  55.994 | GradNorm:  1.5790\n",
      "[DEBUG] Step   3953 | Ep  386 | Eps 0.961 | MeanReward(10ep):  10.500 | MeanLoss(100it):  0.41659 | MeanQ:  56.099 | GradNorm:  0.6959\n",
      "[DEBUG] Step   3959 | Ep  387 | Eps 0.961 | MeanReward(10ep):  10.300 | MeanLoss(100it):  0.41478 | MeanQ:  56.115 | GradNorm:  0.7381\n",
      "[DEBUG] Step   3965 | Ep  388 | Eps 0.961 | MeanReward(10ep):  10.000 | MeanLoss(100it):  0.41176 | MeanQ:  56.105 | GradNorm:  0.6116\n",
      "[DEBUG] Step   3973 | Ep  389 | Eps 0.961 | MeanReward(10ep):  10.300 | MeanLoss(100it):  0.40707 | MeanQ:  56.028 | GradNorm:  1.0857\n",
      "[DEBUG] Step   3993 | Ep  390 | Eps 0.960 | MeanReward(10ep):   9.500 | MeanLoss(100it):  0.39226 | MeanQ:  55.702 | GradNorm:  0.5194\n",
      "[cartpole] Ep 390 Steps 3993 RecentMean 9.50 Eps 0.960\n",
      "[DEBUG] Step   4003 | Ep  391 | Eps 0.960 | MeanReward(10ep):   9.600 | MeanLoss(100it):  0.38947 | MeanQ:  55.669 | GradNorm:  1.3389\n",
      "[DEBUG] Step   4010 | Ep  392 | Eps 0.960 | MeanReward(10ep):   9.500 | MeanLoss(100it):  0.38425 | MeanQ:  55.652 | GradNorm:  1.6872\n",
      "[DEBUG] Step   4029 | Ep  393 | Eps 0.960 | MeanReward(10ep):  10.200 | MeanLoss(100it):  0.37680 | MeanQ:  55.436 | GradNorm:  0.9047\n",
      "[DEBUG] Step   4041 | Ep  394 | Eps 0.960 | MeanReward(10ep):  10.300 | MeanLoss(100it):  0.37449 | MeanQ:  55.314 | GradNorm:  0.5391\n",
      "[DEBUG] Step   4047 | Ep  395 | Eps 0.960 | MeanReward(10ep):   9.400 | MeanLoss(100it):  0.37366 | MeanQ:  55.266 | GradNorm:  0.6617\n",
      "[DEBUG] Step   4058 | Ep  396 | Eps 0.960 | MeanReward(10ep):   9.500 | MeanLoss(100it):  0.37120 | MeanQ:  55.143 | GradNorm:  0.6416\n",
      "[DEBUG] Step   4064 | Ep  397 | Eps 0.960 | MeanReward(10ep):   9.500 | MeanLoss(100it):  0.36910 | MeanQ:  55.099 | GradNorm:  0.5589\n",
      "[DEBUG] Step   4070 | Ep  398 | Eps 0.960 | MeanReward(10ep):   9.500 | MeanLoss(100it):  0.36765 | MeanQ:  55.072 | GradNorm:  0.8064\n",
      "[DEBUG] Step   4080 | Ep  399 | Eps 0.960 | MeanReward(10ep):   9.700 | MeanLoss(100it):  0.36400 | MeanQ:  54.973 | GradNorm:  0.9910\n",
      "[DEBUG] Step   4085 | Ep  400 | Eps 0.960 | MeanReward(10ep):   8.200 | MeanLoss(100it):  0.36056 | MeanQ:  54.923 | GradNorm:  0.5580\n",
      "[cartpole] Ep 400 Steps 4085 RecentMean 8.20 Eps 0.960\n",
      "Eval @ Ep 400 and 4085 steps => mean return 9.00\n",
      "[DEBUG] Step   4094 | Ep  401 | Eps 0.959 | MeanReward(10ep):   8.100 | MeanLoss(100it):  0.35673 | MeanQ:  54.233 | GradNorm:  0.8414\n",
      "[DEBUG] Step   4099 | Ep  402 | Eps 0.959 | MeanReward(10ep):   7.900 | MeanLoss(100it):  0.35482 | MeanQ:  54.818 | GradNorm:  0.6032\n",
      "[DEBUG] Step   4108 | Ep  403 | Eps 0.959 | MeanReward(10ep):   6.900 | MeanLoss(100it):  0.34674 | MeanQ:  54.719 | GradNorm:  1.9671\n",
      "[DEBUG] Step   4114 | Ep  404 | Eps 0.959 | MeanReward(10ep):   6.300 | MeanLoss(100it):  0.34460 | MeanQ:  54.668 | GradNorm:  0.6433\n",
      "[DEBUG] Step   4120 | Ep  405 | Eps 0.959 | MeanReward(10ep):   6.300 | MeanLoss(100it):  0.33742 | MeanQ:  54.585 | GradNorm:  0.6152\n",
      "[DEBUG] Step   4128 | Ep  406 | Eps 0.959 | MeanReward(10ep):   6.000 | MeanLoss(100it):  0.33148 | MeanQ:  54.556 | GradNorm:  1.0434\n",
      "[DEBUG] Step   4135 | Ep  407 | Eps 0.959 | MeanReward(10ep):   6.100 | MeanLoss(100it):  0.32861 | MeanQ:  54.567 | GradNorm:  0.9239\n",
      "[DEBUG] Step   4149 | Ep  408 | Eps 0.959 | MeanReward(10ep):   6.900 | MeanLoss(100it):  0.31632 | MeanQ:  54.729 | GradNorm:  0.3370\n",
      "[DEBUG] Step   4157 | Ep  409 | Eps 0.959 | MeanReward(10ep):   6.700 | MeanLoss(100it):  0.31254 | MeanQ:  54.757 | GradNorm:  0.8114\n",
      "[DEBUG] Step   4165 | Ep  410 | Eps 0.959 | MeanReward(10ep):   7.000 | MeanLoss(100it):  0.30618 | MeanQ:  54.748 | GradNorm:  0.5350\n",
      "[cartpole] Ep 410 Steps 4165 RecentMean 7.00 Eps 0.959\n",
      "[DEBUG] Step   4171 | Ep  411 | Eps 0.959 | MeanReward(10ep):   6.700 | MeanLoss(100it):  0.30321 | MeanQ:  54.746 | GradNorm:  0.7810\n",
      "[DEBUG] Step   4182 | Ep  412 | Eps 0.959 | MeanReward(10ep):   7.300 | MeanLoss(100it):  0.29656 | MeanQ:  54.732 | GradNorm:  1.1081\n",
      "[DEBUG] Step   4190 | Ep  413 | Eps 0.959 | MeanReward(10ep):   7.200 | MeanLoss(100it):  0.29131 | MeanQ:  54.750 | GradNorm:  0.3775\n",
      "[DEBUG] Step   4203 | Ep  414 | Eps 0.958 | MeanReward(10ep):   7.900 | MeanLoss(100it):  0.28597 | MeanQ:  54.734 | GradNorm:  0.6059\n",
      "[DEBUG] Step   4213 | Ep  415 | Eps 0.958 | MeanReward(10ep):   8.300 | MeanLoss(100it):  0.28275 | MeanQ:  54.465 | GradNorm:  1.7300\n",
      "[DEBUG] Step   4221 | Ep  416 | Eps 0.958 | MeanReward(10ep):   8.300 | MeanLoss(100it):  0.28059 | MeanQ:  54.260 | GradNorm:  1.0039\n",
      "[DEBUG] Step   4238 | Ep  417 | Eps 0.958 | MeanReward(10ep):   9.300 | MeanLoss(100it):  0.27584 | MeanQ:  54.243 | GradNorm:  1.0091\n",
      "[DEBUG] Step   4247 | Ep  418 | Eps 0.958 | MeanReward(10ep):   8.800 | MeanLoss(100it):  0.27349 | MeanQ:  54.364 | GradNorm:  0.3947\n",
      "[DEBUG] Step   4256 | Ep  419 | Eps 0.958 | MeanReward(10ep):   8.900 | MeanLoss(100it):  0.26970 | MeanQ:  54.439 | GradNorm:  0.9376\n",
      "[DEBUG] Step   4270 | Ep  420 | Eps 0.958 | MeanReward(10ep):   9.500 | MeanLoss(100it):  0.26467 | MeanQ:  54.292 | GradNorm:  1.3104\n",
      "[cartpole] Ep 420 Steps 4270 RecentMean 9.50 Eps 0.958\n",
      "Eval @ Ep 420 and 4270 steps => mean return 18.00\n",
      "[DEBUG] Step   4280 | Ep  421 | Eps 0.958 | MeanReward(10ep):   9.900 | MeanLoss(100it):  0.26091 | MeanQ:  53.491 | GradNorm:  0.8378\n",
      "[DEBUG] Step   4286 | Ep  422 | Eps 0.958 | MeanReward(10ep):   9.400 | MeanLoss(100it):  0.25971 | MeanQ:  53.998 | GradNorm:  1.0673\n",
      "[DEBUG] Step   4301 | Ep  423 | Eps 0.957 | MeanReward(10ep):  10.100 | MeanLoss(100it):  0.25368 | MeanQ:  54.105 | GradNorm:  2.0727\n",
      "[DEBUG] Step   4315 | Ep  424 | Eps 0.957 | MeanReward(10ep):  10.200 | MeanLoss(100it):  0.24635 | MeanQ:  54.069 | GradNorm:  1.9240\n",
      "[DEBUG] Step   4323 | Ep  425 | Eps 0.957 | MeanReward(10ep):  10.000 | MeanLoss(100it):  0.24100 | MeanQ:  53.868 | GradNorm:  0.5888\n",
      "[DEBUG] Step   4334 | Ep  426 | Eps 0.957 | MeanReward(10ep):  10.300 | MeanLoss(100it):  0.23436 | MeanQ:  53.703 | GradNorm:  1.3538\n",
      "[DEBUG] Step   4351 | Ep  427 | Eps 0.957 | MeanReward(10ep):  10.300 | MeanLoss(100it):  0.22497 | MeanQ:  53.833 | GradNorm:  0.8695\n",
      "[DEBUG] Step   4362 | Ep  428 | Eps 0.957 | MeanReward(10ep):  10.500 | MeanLoss(100it):  0.21759 | MeanQ:  53.877 | GradNorm:  1.9146\n",
      "[DEBUG] Step   4369 | Ep  429 | Eps 0.957 | MeanReward(10ep):  10.300 | MeanLoss(100it):  0.21220 | MeanQ:  53.761 | GradNorm:  0.9046\n",
      "[DEBUG] Step   4404 | Ep  430 | Eps 0.956 | MeanReward(10ep):  12.400 | MeanLoss(100it):  0.19412 | MeanQ:  53.672 | GradNorm:  1.4473\n",
      "[cartpole] Ep 430 Steps 4404 RecentMean 12.40 Eps 0.956\n",
      "[DEBUG] Step   4419 | Ep  431 | Eps 0.956 | MeanReward(10ep):  12.900 | MeanLoss(100it):  0.18925 | MeanQ:  53.483 | GradNorm:  1.0874\n",
      "[DEBUG] Step   4437 | Ep  432 | Eps 0.956 | MeanReward(10ep):  14.100 | MeanLoss(100it):  0.18054 | MeanQ:  53.253 | GradNorm:  1.5360\n",
      "[DEBUG] Step   4444 | Ep  433 | Eps 0.956 | MeanReward(10ep):  13.300 | MeanLoss(100it):  0.17754 | MeanQ:  53.336 | GradNorm:  1.0188\n",
      "[DEBUG] Step   4450 | Ep  434 | Eps 0.956 | MeanReward(10ep):  12.500 | MeanLoss(100it):  0.17613 | MeanQ:  53.383 | GradNorm:  0.3364\n",
      "[DEBUG] Step   4478 | Ep  435 | Eps 0.956 | MeanReward(10ep):  14.500 | MeanLoss(100it):  0.16800 | MeanQ:  53.305 | GradNorm:  0.2784\n",
      "[DEBUG] Step   4489 | Ep  436 | Eps 0.956 | MeanReward(10ep):  14.500 | MeanLoss(100it):  0.16532 | MeanQ:  53.246 | GradNorm:  0.8189\n",
      "[DEBUG] Step   4502 | Ep  437 | Eps 0.955 | MeanReward(10ep):  14.100 | MeanLoss(100it):  0.16195 | MeanQ:  53.295 | GradNorm:  0.4769\n",
      "[DEBUG] Step   4508 | Ep  438 | Eps 0.955 | MeanReward(10ep):  13.600 | MeanLoss(100it):  0.16098 | MeanQ:  53.188 | GradNorm:  1.9455\n",
      "[DEBUG] Step   4514 | Ep  439 | Eps 0.955 | MeanReward(10ep):  13.500 | MeanLoss(100it):  0.16081 | MeanQ:  53.100 | GradNorm:  1.1243\n",
      "[DEBUG] Step   4531 | Ep  440 | Eps 0.955 | MeanReward(10ep):  11.700 | MeanLoss(100it):  0.15943 | MeanQ:  52.881 | GradNorm:  2.0124\n",
      "[cartpole] Ep 440 Steps 4531 RecentMean 11.70 Eps 0.955\n",
      "Eval @ Ep 440 and 4531 steps => mean return 178.00\n",
      "[DEBUG] Step   4539 | Ep  441 | Eps 0.955 | MeanReward(10ep):  11.000 | MeanLoss(100it):  0.15967 | MeanQ:  52.444 | GradNorm:  0.9885\n",
      "[DEBUG] Step   4556 | Ep  442 | Eps 0.955 | MeanReward(10ep):  10.900 | MeanLoss(100it):  0.15956 | MeanQ:  52.960 | GradNorm:  1.8972\n",
      "[DEBUG] Step   4572 | Ep  443 | Eps 0.955 | MeanReward(10ep):  11.800 | MeanLoss(100it):  0.15860 | MeanQ:  52.746 | GradNorm:  0.9448\n",
      "[DEBUG] Step   4585 | Ep  444 | Eps 0.955 | MeanReward(10ep):  12.500 | MeanLoss(100it):  0.15561 | MeanQ:  52.651 | GradNorm:  0.5330\n",
      "[DEBUG] Step   4592 | Ep  445 | Eps 0.955 | MeanReward(10ep):  10.400 | MeanLoss(100it):  0.15580 | MeanQ:  52.678 | GradNorm:  1.4142\n",
      "[DEBUG] Step   4601 | Ep  446 | Eps 0.954 | MeanReward(10ep):  10.200 | MeanLoss(100it):  0.15488 | MeanQ:  52.746 | GradNorm:  1.2536\n",
      "[DEBUG] Step   4607 | Ep  447 | Eps 0.954 | MeanReward(10ep):   9.500 | MeanLoss(100it):  0.15464 | MeanQ:  52.759 | GradNorm:  1.0612\n",
      "[DEBUG] Step   4615 | Ep  448 | Eps 0.954 | MeanReward(10ep):   9.700 | MeanLoss(100it):  0.15408 | MeanQ:  52.655 | GradNorm:  1.8461\n",
      "[DEBUG] Step   4620 | Ep  449 | Eps 0.954 | MeanReward(10ep):   9.600 | MeanLoss(100it):  0.15275 | MeanQ:  52.508 | GradNorm:  0.7814\n",
      "[DEBUG] Step   4625 | Ep  450 | Eps 0.954 | MeanReward(10ep):   8.400 | MeanLoss(100it):  0.15232 | MeanQ:  52.448 | GradNorm:  0.8530\n",
      "[cartpole] Ep 450 Steps 4625 RecentMean 8.40 Eps 0.954\n",
      "[DEBUG] Step   4634 | Ep  451 | Eps 0.954 | MeanReward(10ep):   8.500 | MeanLoss(100it):  0.15103 | MeanQ:  52.393 | GradNorm:  0.7329\n",
      "[DEBUG] Step   4642 | Ep  452 | Eps 0.954 | MeanReward(10ep):   7.600 | MeanLoss(100it):  0.15070 | MeanQ:  52.439 | GradNorm:  1.5952\n",
      "[DEBUG] Step   4648 | Ep  453 | Eps 0.954 | MeanReward(10ep):   6.600 | MeanLoss(100it):  0.15015 | MeanQ:  52.515 | GradNorm:  0.6118\n",
      "[DEBUG] Step   4673 | Ep  454 | Eps 0.954 | MeanReward(10ep):   7.800 | MeanLoss(100it):  0.14759 | MeanQ:  52.415 | GradNorm:  0.4364\n",
      "[DEBUG] Step   4680 | Ep  455 | Eps 0.954 | MeanReward(10ep):   7.800 | MeanLoss(100it):  0.14753 | MeanQ:  52.285 | GradNorm:  0.8426\n",
      "[DEBUG] Step   4697 | Ep  456 | Eps 0.954 | MeanReward(10ep):   8.600 | MeanLoss(100it):  0.14670 | MeanQ:  52.210 | GradNorm:  1.1964\n",
      "[DEBUG] Step   4704 | Ep  457 | Eps 0.953 | MeanReward(10ep):   8.700 | MeanLoss(100it):  0.14524 | MeanQ:  52.226 | GradNorm:  1.7125\n",
      "[DEBUG] Step   4717 | Ep  458 | Eps 0.953 | MeanReward(10ep):   9.200 | MeanLoss(100it):  0.14402 | MeanQ:  52.036 | GradNorm:  0.8039\n",
      "[DEBUG] Step   4750 | Ep  459 | Eps 0.953 | MeanReward(10ep):  12.000 | MeanLoss(100it):  0.14035 | MeanQ:  51.914 | GradNorm:  0.7785\n",
      "[DEBUG] Step   4766 | Ep  460 | Eps 0.953 | MeanReward(10ep):  13.100 | MeanLoss(100it):  0.14018 | MeanQ:  51.973 | GradNorm:  0.2166\n",
      "[cartpole] Ep 460 Steps 4766 RecentMean 13.10 Eps 0.953\n",
      "Eval @ Ep 460 and 4766 steps => mean return 21.00\n",
      "[DEBUG] Step   4772 | Ep  461 | Eps 0.953 | MeanReward(10ep):  12.800 | MeanLoss(100it):  0.13990 | MeanQ:  51.485 | GradNorm:  0.7721\n",
      "[DEBUG] Step   4782 | Ep  462 | Eps 0.953 | MeanReward(10ep):  13.000 | MeanLoss(100it):  0.13915 | MeanQ:  51.860 | GradNorm:  0.4634\n",
      "[DEBUG] Step   4794 | Ep  463 | Eps 0.953 | MeanReward(10ep):  13.600 | MeanLoss(100it):  0.13721 | MeanQ:  51.785 | GradNorm:  0.7420\n",
      "[DEBUG] Step   4807 | Ep  464 | Eps 0.952 | MeanReward(10ep):  12.400 | MeanLoss(100it):  0.13628 | MeanQ:  51.796 | GradNorm:  1.8901\n",
      "[DEBUG] Step   4814 | Ep  465 | Eps 0.952 | MeanReward(10ep):  12.400 | MeanLoss(100it):  0.13518 | MeanQ:  51.677 | GradNorm:  0.6583\n",
      "[DEBUG] Step   4820 | Ep  466 | Eps 0.952 | MeanReward(10ep):  11.300 | MeanLoss(100it):  0.13472 | MeanQ:  51.574 | GradNorm:  0.4017\n",
      "[DEBUG] Step   4829 | Ep  467 | Eps 0.952 | MeanReward(10ep):  11.500 | MeanLoss(100it):  0.13395 | MeanQ:  51.514 | GradNorm:  0.4757\n",
      "[DEBUG] Step   4839 | Ep  468 | Eps 0.952 | MeanReward(10ep):  11.200 | MeanLoss(100it):  0.13334 | MeanQ:  51.507 | GradNorm:  0.7949\n",
      "[DEBUG] Step   4851 | Ep  469 | Eps 0.952 | MeanReward(10ep):   9.100 | MeanLoss(100it):  0.13295 | MeanQ:  51.529 | GradNorm:  0.3045\n",
      "[DEBUG] Step   4857 | Ep  470 | Eps 0.952 | MeanReward(10ep):   8.100 | MeanLoss(100it):  0.13241 | MeanQ:  51.505 | GradNorm:  0.6672\n",
      "[cartpole] Ep 470 Steps 4857 RecentMean 8.10 Eps 0.952\n",
      "[DEBUG] Step   4863 | Ep  471 | Eps 0.952 | MeanReward(10ep):   8.100 | MeanLoss(100it):  0.13181 | MeanQ:  51.469 | GradNorm:  0.9141\n",
      "[DEBUG] Step   4876 | Ep  472 | Eps 0.952 | MeanReward(10ep):   8.400 | MeanLoss(100it):  0.13042 | MeanQ:  51.353 | GradNorm:  0.7350\n",
      "[DEBUG] Step   4885 | Ep  473 | Eps 0.952 | MeanReward(10ep):   8.100 | MeanLoss(100it):  0.12959 | MeanQ:  51.340 | GradNorm:  0.8050\n",
      "[DEBUG] Step   4901 | Ep  474 | Eps 0.951 | MeanReward(10ep):   8.400 | MeanLoss(100it):  0.12885 | MeanQ:  51.425 | GradNorm:  0.4830\n",
      "[DEBUG] Step   4919 | Ep  475 | Eps 0.951 | MeanReward(10ep):   9.500 | MeanLoss(100it):  0.12727 | MeanQ:  51.304 | GradNorm:  0.4777\n",
      "[DEBUG] Step   4925 | Ep  476 | Eps 0.951 | MeanReward(10ep):   9.500 | MeanLoss(100it):  0.12721 | MeanQ:  51.256 | GradNorm:  0.2463\n",
      "[DEBUG] Step   4930 | Ep  477 | Eps 0.951 | MeanReward(10ep):   9.100 | MeanLoss(100it):  0.12663 | MeanQ:  51.256 | GradNorm:  0.9175\n",
      "[DEBUG] Step   4936 | Ep  478 | Eps 0.951 | MeanReward(10ep):   8.700 | MeanLoss(100it):  0.12519 | MeanQ:  51.269 | GradNorm:  0.3205\n",
      "[DEBUG] Step   4942 | Ep  479 | Eps 0.951 | MeanReward(10ep):   8.100 | MeanLoss(100it):  0.12474 | MeanQ:  51.284 | GradNorm:  0.5558\n",
      "[DEBUG] Step   4949 | Ep  480 | Eps 0.951 | MeanReward(10ep):   8.200 | MeanLoss(100it):  0.12419 | MeanQ:  51.322 | GradNorm:  0.7684\n",
      "[cartpole] Ep 480 Steps 4949 RecentMean 8.20 Eps 0.951\n",
      "Eval @ Ep 480 and 4949 steps => mean return 37.00\n",
      "[DEBUG] Step   4955 | Ep  481 | Eps 0.951 | MeanReward(10ep):   8.200 | MeanLoss(100it):  0.12401 | MeanQ:  50.903 | GradNorm:  0.5921\n",
      "[DEBUG] Step   4964 | Ep  482 | Eps 0.951 | MeanReward(10ep):   7.800 | MeanLoss(100it):  0.12342 | MeanQ:  51.316 | GradNorm:  0.4641\n",
      "[DEBUG] Step   4970 | Ep  483 | Eps 0.951 | MeanReward(10ep):   7.500 | MeanLoss(100it):  0.12291 | MeanQ:  51.300 | GradNorm:  0.1520\n",
      "[DEBUG] Step   4976 | Ep  484 | Eps 0.951 | MeanReward(10ep):   6.500 | MeanLoss(100it):  0.12238 | MeanQ:  51.267 | GradNorm:  0.6718\n",
      "[DEBUG] Step   4984 | Ep  485 | Eps 0.951 | MeanReward(10ep):   5.500 | MeanLoss(100it):  0.12272 | MeanQ:  51.247 | GradNorm:  0.3164\n",
      "[DEBUG] Step   4994 | Ep  486 | Eps 0.951 | MeanReward(10ep):   5.900 | MeanLoss(100it):  0.12217 | MeanQ:  51.254 | GradNorm:  0.6449\n",
      "[DEBUG] Step   4999 | Ep  487 | Eps 0.951 | MeanReward(10ep):   5.900 | MeanLoss(100it):  0.12186 | MeanQ:  51.266 | GradNorm:  0.2151\n",
      "[DEBUG] Step   5009 | Ep  488 | Eps 0.950 | MeanReward(10ep):   6.300 | MeanLoss(100it):  0.12140 | MeanQ:  51.156 | GradNorm:  1.3520\n",
      "[DEBUG] Step   5047 | Ep  489 | Eps 0.950 | MeanReward(10ep):   9.500 | MeanLoss(100it):  0.11964 | MeanQ:  50.976 | GradNorm:  1.0391\n",
      "[DEBUG] Step   5054 | Ep  490 | Eps 0.950 | MeanReward(10ep):   9.500 | MeanLoss(100it):  0.11887 | MeanQ:  51.030 | GradNorm:  1.1929\n",
      "[cartpole] Ep 490 Steps 5054 RecentMean 9.50 Eps 0.950\n",
      "[DEBUG] Step   5062 | Ep  491 | Eps 0.950 | MeanReward(10ep):   9.700 | MeanLoss(100it):  0.11812 | MeanQ:  51.007 | GradNorm:  1.0141\n",
      "[DEBUG] Step   5069 | Ep  492 | Eps 0.950 | MeanReward(10ep):   9.500 | MeanLoss(100it):  0.11796 | MeanQ:  50.909 | GradNorm:  1.4298\n",
      "[DEBUG] Step   5087 | Ep  493 | Eps 0.950 | MeanReward(10ep):  10.700 | MeanLoss(100it):  0.11697 | MeanQ:  50.851 | GradNorm:  0.9775\n",
      "[DEBUG] Step   5106 | Ep  494 | Eps 0.949 | MeanReward(10ep):  12.000 | MeanLoss(100it):  0.11627 | MeanQ:  51.041 | GradNorm:  0.8342\n",
      "[DEBUG] Step   5128 | Ep  495 | Eps 0.949 | MeanReward(10ep):  13.400 | MeanLoss(100it):  0.11661 | MeanQ:  50.847 | GradNorm:  0.2961\n",
      "[DEBUG] Step   5142 | Ep  496 | Eps 0.949 | MeanReward(10ep):  13.800 | MeanLoss(100it):  0.11617 | MeanQ:  50.854 | GradNorm:  0.7796\n",
      "[DEBUG] Step   5155 | Ep  497 | Eps 0.949 | MeanReward(10ep):  14.600 | MeanLoss(100it):  0.11671 | MeanQ:  50.954 | GradNorm:  0.9843\n",
      "[DEBUG] Step   5169 | Ep  498 | Eps 0.949 | MeanReward(10ep):  15.000 | MeanLoss(100it):  0.11574 | MeanQ:  50.972 | GradNorm:  0.4319\n",
      "[DEBUG] Step   5178 | Ep  499 | Eps 0.949 | MeanReward(10ep):  12.100 | MeanLoss(100it):  0.11553 | MeanQ:  50.907 | GradNorm:  0.5269\n",
      "[DEBUG] Step   5186 | Ep  500 | Eps 0.949 | MeanReward(10ep):  12.200 | MeanLoss(100it):  0.11578 | MeanQ:  50.839 | GradNorm:  0.5065\n",
      "[cartpole] Ep 500 Steps 5186 RecentMean 12.20 Eps 0.949\n",
      "Eval @ Ep 500 and 5186 steps => mean return 34.00\n",
      "[DEBUG] Step   5198 | Ep  501 | Eps 0.949 | MeanReward(10ep):  12.600 | MeanLoss(100it):  0.11609 | MeanQ:  50.379 | GradNorm:  0.8346\n",
      "[DEBUG] Step   5212 | Ep  502 | Eps 0.948 | MeanReward(10ep):  13.300 | MeanLoss(100it):  0.11593 | MeanQ:  50.709 | GradNorm:  1.4020\n",
      "[DEBUG] Step   5218 | Ep  503 | Eps 0.948 | MeanReward(10ep):  12.100 | MeanLoss(100it):  0.11595 | MeanQ:  50.631 | GradNorm:  0.7776\n",
      "[DEBUG] Step   5236 | Ep  504 | Eps 0.948 | MeanReward(10ep):  12.000 | MeanLoss(100it):  0.11611 | MeanQ:  50.535 | GradNorm:  0.8061\n",
      "[DEBUG] Step   5250 | Ep  505 | Eps 0.948 | MeanReward(10ep):  11.200 | MeanLoss(100it):  0.11530 | MeanQ:  50.618 | GradNorm:  0.3124\n",
      "[DEBUG] Step   5256 | Ep  506 | Eps 0.948 | MeanReward(10ep):  10.400 | MeanLoss(100it):  0.11533 | MeanQ:  50.599 | GradNorm:  1.0013\n",
      "[DEBUG] Step   5262 | Ep  507 | Eps 0.948 | MeanReward(10ep):   9.700 | MeanLoss(100it):  0.11561 | MeanQ:  50.578 | GradNorm:  0.3652\n",
      "[DEBUG] Step   5272 | Ep  508 | Eps 0.948 | MeanReward(10ep):   9.300 | MeanLoss(100it):  0.11492 | MeanQ:  50.445 | GradNorm:  0.4350\n",
      "[DEBUG] Step   5286 | Ep  509 | Eps 0.948 | MeanReward(10ep):   9.800 | MeanLoss(100it):  0.11501 | MeanQ:  50.482 | GradNorm:  1.0914\n",
      "[DEBUG] Step   5293 | Ep  510 | Eps 0.948 | MeanReward(10ep):   9.700 | MeanLoss(100it):  0.11441 | MeanQ:  50.566 | GradNorm:  0.4724\n",
      "[cartpole] Ep 510 Steps 5293 RecentMean 9.70 Eps 0.948\n",
      "[DEBUG] Step   5302 | Ep  511 | Eps 0.948 | MeanReward(10ep):   9.400 | MeanLoss(100it):  0.11447 | MeanQ:  50.607 | GradNorm:  0.4864\n",
      "[DEBUG] Step   5307 | Ep  512 | Eps 0.947 | MeanReward(10ep):   8.500 | MeanLoss(100it):  0.11431 | MeanQ:  50.609 | GradNorm:  0.6481\n",
      "[DEBUG] Step   5313 | Ep  513 | Eps 0.947 | MeanReward(10ep):   8.500 | MeanLoss(100it):  0.11376 | MeanQ:  50.562 | GradNorm:  0.8945\n",
      "[DEBUG] Step   5323 | Ep  514 | Eps 0.947 | MeanReward(10ep):   7.700 | MeanLoss(100it):  0.11367 | MeanQ:  50.490 | GradNorm:  0.5605\n",
      "[DEBUG] Step   5332 | Ep  515 | Eps 0.947 | MeanReward(10ep):   7.200 | MeanLoss(100it):  0.11314 | MeanQ:  50.507 | GradNorm:  0.7568\n",
      "[DEBUG] Step   5338 | Ep  516 | Eps 0.947 | MeanReward(10ep):   7.200 | MeanLoss(100it):  0.11301 | MeanQ:  50.541 | GradNorm:  0.8009\n",
      "[DEBUG] Step   5366 | Ep  517 | Eps 0.947 | MeanReward(10ep):   9.400 | MeanLoss(100it):  0.11155 | MeanQ:  50.582 | GradNorm:  0.7763\n",
      "[DEBUG] Step   5379 | Ep  518 | Eps 0.947 | MeanReward(10ep):   9.700 | MeanLoss(100it):  0.11097 | MeanQ:  50.557 | GradNorm:  0.2199\n",
      "[DEBUG] Step   5395 | Ep  519 | Eps 0.947 | MeanReward(10ep):   9.900 | MeanLoss(100it):  0.10961 | MeanQ:  50.605 | GradNorm:  0.2218\n",
      "[DEBUG] Step   5401 | Ep  520 | Eps 0.947 | MeanReward(10ep):   9.800 | MeanLoss(100it):  0.10888 | MeanQ:  50.625 | GradNorm:  0.7777\n",
      "[cartpole] Ep 520 Steps 5401 RecentMean 9.80 Eps 0.947\n",
      "Eval @ Ep 520 and 5401 steps => mean return 41.00\n",
      "[DEBUG] Step   5412 | Ep  521 | Eps 0.946 | MeanReward(10ep):  10.000 | MeanLoss(100it):  0.10861 | MeanQ:  50.061 | GradNorm:  1.3143\n",
      "[DEBUG] Step   5421 | Ep  522 | Eps 0.946 | MeanReward(10ep):  10.400 | MeanLoss(100it):  0.10812 | MeanQ:  50.346 | GradNorm:  0.2180\n",
      "[DEBUG] Step   5428 | Ep  523 | Eps 0.946 | MeanReward(10ep):  10.500 | MeanLoss(100it):  0.10812 | MeanQ:  50.288 | GradNorm:  1.3460\n",
      "[DEBUG] Step   5435 | Ep  524 | Eps 0.946 | MeanReward(10ep):  10.200 | MeanLoss(100it):  0.10824 | MeanQ:  50.291 | GradNorm:  0.6807\n",
      "[DEBUG] Step   5457 | Ep  525 | Eps 0.946 | MeanReward(10ep):  11.500 | MeanLoss(100it):  0.10728 | MeanQ:  50.396 | GradNorm:  0.5199\n",
      "[DEBUG] Step   5465 | Ep  526 | Eps 0.946 | MeanReward(10ep):  11.700 | MeanLoss(100it):  0.10694 | MeanQ:  50.367 | GradNorm:  0.6446\n",
      "[DEBUG] Step   5480 | Ep  527 | Eps 0.946 | MeanReward(10ep):  10.400 | MeanLoss(100it):  0.10630 | MeanQ:  50.237 | GradNorm:  0.3537\n",
      "[DEBUG] Step   5497 | Ep  528 | Eps 0.946 | MeanReward(10ep):  10.800 | MeanLoss(100it):  0.10490 | MeanQ:  50.275 | GradNorm:  0.5356\n",
      "[DEBUG] Step   5511 | Ep  529 | Eps 0.945 | MeanReward(10ep):  10.600 | MeanLoss(100it):  0.10446 | MeanQ:  50.301 | GradNorm:  0.6299\n",
      "[DEBUG] Step   5516 | Ep  530 | Eps 0.945 | MeanReward(10ep):  10.500 | MeanLoss(100it):  0.10393 | MeanQ:  50.255 | GradNorm:  0.6248\n",
      "[cartpole] Ep 530 Steps 5516 RecentMean 10.50 Eps 0.945\n",
      "[DEBUG] Step   5522 | Ep  531 | Eps 0.945 | MeanReward(10ep):  10.000 | MeanLoss(100it):  0.10360 | MeanQ:  50.216 | GradNorm:  0.5123\n",
      "[DEBUG] Step   5547 | Ep  532 | Eps 0.945 | MeanReward(10ep):  11.600 | MeanLoss(100it):  0.10282 | MeanQ:  50.181 | GradNorm:  0.1463\n",
      "[DEBUG] Step   5554 | Ep  533 | Eps 0.945 | MeanReward(10ep):  11.600 | MeanLoss(100it):  0.10187 | MeanQ:  50.215 | GradNorm:  0.1211\n",
      "[DEBUG] Step   5560 | Ep  534 | Eps 0.945 | MeanReward(10ep):  11.500 | MeanLoss(100it):  0.10139 | MeanQ:  50.194 | GradNorm:  0.7110\n",
      "[DEBUG] Step   5567 | Ep  535 | Eps 0.945 | MeanReward(10ep):  10.000 | MeanLoss(100it):  0.10124 | MeanQ:  50.168 | GradNorm:  0.5927\n",
      "[DEBUG] Step   5575 | Ep  536 | Eps 0.945 | MeanReward(10ep):  10.000 | MeanLoss(100it):  0.10048 | MeanQ:  50.112 | GradNorm:  0.4720\n",
      "[DEBUG] Step   5582 | Ep  537 | Eps 0.945 | MeanReward(10ep):   9.200 | MeanLoss(100it):  0.09989 | MeanQ:  50.106 | GradNorm:  0.7842\n",
      "[DEBUG] Step   5612 | Ep  538 | Eps 0.944 | MeanReward(10ep):  10.500 | MeanLoss(100it):  0.09847 | MeanQ:  50.246 | GradNorm:  1.0634\n",
      "[DEBUG] Step   5626 | Ep  539 | Eps 0.944 | MeanReward(10ep):  10.500 | MeanLoss(100it):  0.09791 | MeanQ:  50.132 | GradNorm:  0.4123\n",
      "[DEBUG] Step   5633 | Ep  540 | Eps 0.944 | MeanReward(10ep):  10.700 | MeanLoss(100it):  0.09733 | MeanQ:  50.093 | GradNorm:  0.4864\n",
      "[cartpole] Ep 540 Steps 5633 RecentMean 10.70 Eps 0.944\n",
      "Eval @ Ep 540 and 5633 steps => mean return 37.00\n",
      "[DEBUG] Step   5644 | Ep  541 | Eps 0.944 | MeanReward(10ep):  11.200 | MeanLoss(100it):  0.09719 | MeanQ:  49.679 | GradNorm:  0.3131\n",
      "[DEBUG] Step   5654 | Ep  542 | Eps 0.944 | MeanReward(10ep):   9.700 | MeanLoss(100it):  0.09701 | MeanQ:  50.150 | GradNorm:  0.6629\n",
      "[DEBUG] Step   5662 | Ep  543 | Eps 0.944 | MeanReward(10ep):   9.800 | MeanLoss(100it):  0.09663 | MeanQ:  50.192 | GradNorm:  0.5068\n",
      "[DEBUG] Step   5668 | Ep  544 | Eps 0.944 | MeanReward(10ep):   9.800 | MeanLoss(100it):  0.09680 | MeanQ:  50.168 | GradNorm:  0.8828\n",
      "[DEBUG] Step   5681 | Ep  545 | Eps 0.944 | MeanReward(10ep):  10.400 | MeanLoss(100it):  0.09647 | MeanQ:  50.080 | GradNorm:  0.4554\n",
      "[DEBUG] Step   5687 | Ep  546 | Eps 0.944 | MeanReward(10ep):  10.200 | MeanLoss(100it):  0.09602 | MeanQ:  50.058 | GradNorm:  0.4937\n",
      "[DEBUG] Step   5693 | Ep  547 | Eps 0.944 | MeanReward(10ep):  10.100 | MeanLoss(100it):  0.09613 | MeanQ:  50.062 | GradNorm:  0.5624\n",
      "[DEBUG] Step   5701 | Ep  548 | Eps 0.944 | MeanReward(10ep):   7.900 | MeanLoss(100it):  0.09636 | MeanQ:  50.082 | GradNorm:  1.0756\n",
      "[DEBUG] Step   5708 | Ep  549 | Eps 0.944 | MeanReward(10ep):   7.200 | MeanLoss(100it):  0.09682 | MeanQ:  50.033 | GradNorm:  1.2617\n",
      "[DEBUG] Step   5714 | Ep  550 | Eps 0.943 | MeanReward(10ep):   7.100 | MeanLoss(100it):  0.09674 | MeanQ:  49.982 | GradNorm:  0.6344\n",
      "[cartpole] Ep 550 Steps 5714 RecentMean 7.10 Eps 0.943\n",
      "[DEBUG] Step   5721 | Ep  551 | Eps 0.943 | MeanReward(10ep):   6.700 | MeanLoss(100it):  0.09648 | MeanQ:  49.887 | GradNorm:  0.3442\n",
      "[DEBUG] Step   5734 | Ep  552 | Eps 0.943 | MeanReward(10ep):   7.000 | MeanLoss(100it):  0.09587 | MeanQ:  49.862 | GradNorm:  0.4729\n",
      "[DEBUG] Step   5744 | Ep  553 | Eps 0.943 | MeanReward(10ep):   7.200 | MeanLoss(100it):  0.09615 | MeanQ:  49.866 | GradNorm:  0.4786\n",
      "[DEBUG] Step   5750 | Ep  554 | Eps 0.943 | MeanReward(10ep):   7.200 | MeanLoss(100it):  0.09610 | MeanQ:  49.859 | GradNorm:  0.3496\n",
      "[DEBUG] Step   5756 | Ep  555 | Eps 0.943 | MeanReward(10ep):   6.500 | MeanLoss(100it):  0.09585 | MeanQ:  49.834 | GradNorm:  0.4133\n",
      "[DEBUG] Step   5769 | Ep  556 | Eps 0.943 | MeanReward(10ep):   7.200 | MeanLoss(100it):  0.09520 | MeanQ:  49.783 | GradNorm:  0.3114\n",
      "[DEBUG] Step   5777 | Ep  557 | Eps 0.943 | MeanReward(10ep):   7.400 | MeanLoss(100it):  0.09545 | MeanQ:  49.742 | GradNorm:  0.2745\n",
      "[DEBUG] Step   5788 | Ep  558 | Eps 0.943 | MeanReward(10ep):   7.700 | MeanLoss(100it):  0.09497 | MeanQ:  49.730 | GradNorm:  0.5392\n",
      "[DEBUG] Step   5803 | Ep  559 | Eps 0.943 | MeanReward(10ep):   8.500 | MeanLoss(100it):  0.09496 | MeanQ:  49.748 | GradNorm:  0.2497\n",
      "[DEBUG] Step   5811 | Ep  560 | Eps 0.942 | MeanReward(10ep):   8.700 | MeanLoss(100it):  0.09476 | MeanQ:  49.710 | GradNorm:  1.1607\n",
      "[cartpole] Ep 560 Steps 5811 RecentMean 8.70 Eps 0.942\n",
      "Eval @ Ep 560 and 5811 steps => mean return 109.00\n",
      "[DEBUG] Step   5826 | Ep  561 | Eps 0.942 | MeanReward(10ep):   9.500 | MeanLoss(100it):  0.09359 | MeanQ:  49.266 | GradNorm:  0.6110\n",
      "[DEBUG] Step   5856 | Ep  562 | Eps 0.942 | MeanReward(10ep):  11.200 | MeanLoss(100it):  0.09276 | MeanQ:  49.742 | GradNorm:  0.5242\n",
      "[DEBUG] Step   5866 | Ep  563 | Eps 0.942 | MeanReward(10ep):  11.200 | MeanLoss(100it):  0.09239 | MeanQ:  49.680 | GradNorm:  0.5935\n",
      "[DEBUG] Step   5874 | Ep  564 | Eps 0.942 | MeanReward(10ep):  11.400 | MeanLoss(100it):  0.09186 | MeanQ:  49.625 | GradNorm:  0.3825\n",
      "[DEBUG] Step   5883 | Ep  565 | Eps 0.942 | MeanReward(10ep):  11.700 | MeanLoss(100it):  0.09170 | MeanQ:  49.641 | GradNorm:  0.7398\n",
      "[DEBUG] Step   5893 | Ep  566 | Eps 0.942 | MeanReward(10ep):  11.400 | MeanLoss(100it):  0.09213 | MeanQ:  49.751 | GradNorm:  0.6208\n",
      "[DEBUG] Step   5904 | Ep  567 | Eps 0.942 | MeanReward(10ep):  11.700 | MeanLoss(100it):  0.09256 | MeanQ:  49.700 | GradNorm:  1.2023\n",
      "[DEBUG] Step   5910 | Ep  568 | Eps 0.942 | MeanReward(10ep):  11.200 | MeanLoss(100it):  0.09236 | MeanQ:  49.640 | GradNorm:  0.9221\n",
      "[DEBUG] Step   5941 | Ep  569 | Eps 0.941 | MeanReward(10ep):  12.800 | MeanLoss(100it):  0.09158 | MeanQ:  49.630 | GradNorm:  0.5348\n",
      "[DEBUG] Step   5955 | Ep  570 | Eps 0.941 | MeanReward(10ep):  13.400 | MeanLoss(100it):  0.09140 | MeanQ:  49.592 | GradNorm:  0.9013\n",
      "[cartpole] Ep 570 Steps 5955 RecentMean 13.40 Eps 0.941\n",
      "[DEBUG] Step   5968 | Ep  571 | Eps 0.941 | MeanReward(10ep):  13.200 | MeanLoss(100it):  0.09159 | MeanQ:  49.466 | GradNorm:  0.4782\n",
      "[DEBUG] Step   5978 | Ep  572 | Eps 0.941 | MeanReward(10ep):  11.200 | MeanLoss(100it):  0.09147 | MeanQ:  49.453 | GradNorm:  0.6793\n",
      "[DEBUG] Step   6007 | Ep  573 | Eps 0.941 | MeanReward(10ep):  13.100 | MeanLoss(100it):  0.09094 | MeanQ:  49.576 | GradNorm:  0.7140\n",
      "[DEBUG] Step   6017 | Ep  574 | Eps 0.940 | MeanReward(10ep):  13.300 | MeanLoss(100it):  0.09041 | MeanQ:  49.509 | GradNorm:  0.8374\n",
      "[DEBUG] Step   6024 | Ep  575 | Eps 0.940 | MeanReward(10ep):  13.100 | MeanLoss(100it):  0.09029 | MeanQ:  49.452 | GradNorm:  0.7973\n",
      "[DEBUG] Step   6035 | Ep  576 | Eps 0.940 | MeanReward(10ep):  13.200 | MeanLoss(100it):  0.09008 | MeanQ:  49.464 | GradNorm:  0.5949\n",
      "[DEBUG] Step   6043 | Ep  577 | Eps 0.940 | MeanReward(10ep):  12.900 | MeanLoss(100it):  0.09016 | MeanQ:  49.534 | GradNorm:  0.4127\n",
      "[DEBUG] Step   6067 | Ep  578 | Eps 0.940 | MeanReward(10ep):  14.700 | MeanLoss(100it):  0.09018 | MeanQ:  49.469 | GradNorm:  0.6937\n",
      "[DEBUG] Step   6076 | Ep  579 | Eps 0.940 | MeanReward(10ep):  12.500 | MeanLoss(100it):  0.08992 | MeanQ:  49.374 | GradNorm:  0.4072\n",
      "[DEBUG] Step   6091 | Ep  580 | Eps 0.940 | MeanReward(10ep):  12.600 | MeanLoss(100it):  0.08940 | MeanQ:  49.402 | GradNorm:  0.3621\n",
      "[cartpole] Ep 580 Steps 6091 RecentMean 12.60 Eps 0.940\n",
      "Eval @ Ep 580 and 6091 steps => mean return 94.00\n",
      "[DEBUG] Step   6097 | Ep  581 | Eps 0.940 | MeanReward(10ep):  11.900 | MeanLoss(100it):  0.08871 | MeanQ:  49.016 | GradNorm:  0.6676\n",
      "[DEBUG] Step   6105 | Ep  582 | Eps 0.940 | MeanReward(10ep):  11.700 | MeanLoss(100it):  0.08797 | MeanQ:  49.439 | GradNorm:  0.6513\n",
      "[DEBUG] Step   6110 | Ep  583 | Eps 0.940 | MeanReward(10ep):   9.300 | MeanLoss(100it):  0.08765 | MeanQ:  49.417 | GradNorm:  0.7391\n",
      "[DEBUG] Step   6118 | Ep  584 | Eps 0.939 | MeanReward(10ep):   9.100 | MeanLoss(100it):  0.08795 | MeanQ:  49.367 | GradNorm:  0.3506\n",
      "[DEBUG] Step   6125 | Ep  585 | Eps 0.939 | MeanReward(10ep):   9.100 | MeanLoss(100it):  0.08776 | MeanQ:  49.305 | GradNorm:  0.2135\n",
      "[DEBUG] Step   6134 | Ep  586 | Eps 0.939 | MeanReward(10ep):   8.900 | MeanLoss(100it):  0.08813 | MeanQ:  49.289 | GradNorm:  0.5337\n",
      "[DEBUG] Step   6154 | Ep  587 | Eps 0.939 | MeanReward(10ep):  10.100 | MeanLoss(100it):  0.08739 | MeanQ:  49.359 | GradNorm:  0.5671\n",
      "[DEBUG] Step   6162 | Ep  588 | Eps 0.939 | MeanReward(10ep):   8.500 | MeanLoss(100it):  0.08739 | MeanQ:  49.356 | GradNorm:  0.4911\n",
      "[DEBUG] Step   6169 | Ep  589 | Eps 0.939 | MeanReward(10ep):   8.300 | MeanLoss(100it):  0.08741 | MeanQ:  49.328 | GradNorm:  0.2658\n",
      "[DEBUG] Step   6177 | Ep  590 | Eps 0.939 | MeanReward(10ep):   7.600 | MeanLoss(100it):  0.08742 | MeanQ:  49.298 | GradNorm:  0.4102\n",
      "[cartpole] Ep 590 Steps 6177 RecentMean 7.60 Eps 0.939\n",
      "[DEBUG] Step   6199 | Ep  591 | Eps 0.939 | MeanReward(10ep):   9.200 | MeanLoss(100it):  0.08679 | MeanQ:  49.382 | GradNorm:  0.4280\n",
      "[DEBUG] Step   6208 | Ep  592 | Eps 0.939 | MeanReward(10ep):   9.300 | MeanLoss(100it):  0.08684 | MeanQ:  49.348 | GradNorm:  1.2991\n",
      "[DEBUG] Step   6234 | Ep  593 | Eps 0.938 | MeanReward(10ep):  11.400 | MeanLoss(100it):  0.08684 | MeanQ:  49.200 | GradNorm:  0.8840\n",
      "[DEBUG] Step   6243 | Ep  594 | Eps 0.938 | MeanReward(10ep):  11.500 | MeanLoss(100it):  0.08640 | MeanQ:  49.248 | GradNorm:  0.6184\n",
      "[DEBUG] Step   6248 | Ep  595 | Eps 0.938 | MeanReward(10ep):  11.300 | MeanLoss(100it):  0.08665 | MeanQ:  49.311 | GradNorm:  0.4751\n",
      "[DEBUG] Step   6269 | Ep  596 | Eps 0.938 | MeanReward(10ep):  12.500 | MeanLoss(100it):  0.08685 | MeanQ:  49.298 | GradNorm:  0.4303\n",
      "[DEBUG] Step   6295 | Ep  597 | Eps 0.938 | MeanReward(10ep):  13.100 | MeanLoss(100it):  0.08647 | MeanQ:  49.264 | GradNorm:  0.1752\n",
      "[DEBUG] Step   6301 | Ep  598 | Eps 0.938 | MeanReward(10ep):  12.900 | MeanLoss(100it):  0.08641 | MeanQ:  49.284 | GradNorm:  0.4614\n",
      "[DEBUG] Step   6307 | Ep  599 | Eps 0.938 | MeanReward(10ep):  12.800 | MeanLoss(100it):  0.08639 | MeanQ:  49.262 | GradNorm:  1.0030\n",
      "[DEBUG] Step   6315 | Ep  600 | Eps 0.937 | MeanReward(10ep):  12.800 | MeanLoss(100it):  0.08673 | MeanQ:  49.180 | GradNorm:  0.1426\n",
      "[cartpole] Ep 600 Steps 6315 RecentMean 12.80 Eps 0.937\n",
      "Eval @ Ep 600 and 6315 steps => mean return 125.00\n",
      "[DEBUG] Step   6339 | Ep  601 | Eps 0.937 | MeanReward(10ep):  13.000 | MeanLoss(100it):  0.08642 | MeanQ:  48.767 | GradNorm:  0.4019\n",
      "[DEBUG] Step   6346 | Ep  602 | Eps 0.937 | MeanReward(10ep):  12.800 | MeanLoss(100it):  0.08658 | MeanQ:  49.213 | GradNorm:  0.1521\n",
      "[DEBUG] Step   6353 | Ep  603 | Eps 0.937 | MeanReward(10ep):  10.900 | MeanLoss(100it):  0.08601 | MeanQ:  49.201 | GradNorm:  0.6258\n",
      "[DEBUG] Step   6358 | Ep  604 | Eps 0.937 | MeanReward(10ep):  10.500 | MeanLoss(100it):  0.08575 | MeanQ:  49.172 | GradNorm:  0.6763\n",
      "[DEBUG] Step   6364 | Ep  605 | Eps 0.937 | MeanReward(10ep):  10.600 | MeanLoss(100it):  0.08579 | MeanQ:  49.126 | GradNorm:  0.4474\n",
      "[DEBUG] Step   6370 | Ep  606 | Eps 0.937 | MeanReward(10ep):   9.100 | MeanLoss(100it):  0.08606 | MeanQ:  49.098 | GradNorm:  0.4503\n",
      "[DEBUG] Step   6381 | Ep  607 | Eps 0.937 | MeanReward(10ep):   7.600 | MeanLoss(100it):  0.08652 | MeanQ:  49.093 | GradNorm:  0.4521\n",
      "[DEBUG] Step   6391 | Ep  608 | Eps 0.937 | MeanReward(10ep):   8.000 | MeanLoss(100it):  0.08660 | MeanQ:  49.137 | GradNorm:  0.3581\n",
      "[DEBUG] Step   6397 | Ep  609 | Eps 0.937 | MeanReward(10ep):   8.000 | MeanLoss(100it):  0.08703 | MeanQ:  49.216 | GradNorm:  0.2249\n",
      "[DEBUG] Step   6402 | Ep  610 | Eps 0.937 | MeanReward(10ep):   7.700 | MeanLoss(100it):  0.08704 | MeanQ:  49.229 | GradNorm:  0.8264\n",
      "[cartpole] Ep 610 Steps 6402 RecentMean 7.70 Eps 0.937\n",
      "[DEBUG] Step   6416 | Ep  611 | Eps 0.936 | MeanReward(10ep):   6.700 | MeanLoss(100it):  0.08660 | MeanQ:  49.140 | GradNorm:  0.3016\n",
      "[DEBUG] Step   6421 | Ep  612 | Eps 0.936 | MeanReward(10ep):   6.500 | MeanLoss(100it):  0.08668 | MeanQ:  49.102 | GradNorm:  0.3986\n",
      "[DEBUG] Step   6430 | Ep  613 | Eps 0.936 | MeanReward(10ep):   6.700 | MeanLoss(100it):  0.08680 | MeanQ:  49.077 | GradNorm:  0.5369\n",
      "[DEBUG] Step   6440 | Ep  614 | Eps 0.936 | MeanReward(10ep):   7.200 | MeanLoss(100it):  0.08639 | MeanQ:  49.140 | GradNorm:  0.4583\n",
      "[DEBUG] Step   6453 | Ep  615 | Eps 0.936 | MeanReward(10ep):   7.900 | MeanLoss(100it):  0.08599 | MeanQ:  49.209 | GradNorm:  0.3204\n",
      "[DEBUG] Step   6470 | Ep  616 | Eps 0.936 | MeanReward(10ep):   9.000 | MeanLoss(100it):  0.08561 | MeanQ:  49.163 | GradNorm:  1.2822\n",
      "[DEBUG] Step   6481 | Ep  617 | Eps 0.936 | MeanReward(10ep):   9.000 | MeanLoss(100it):  0.08574 | MeanQ:  49.050 | GradNorm:  0.2695\n",
      "[DEBUG] Step   6487 | Ep  618 | Eps 0.936 | MeanReward(10ep):   8.600 | MeanLoss(100it):  0.08571 | MeanQ:  49.038 | GradNorm:  0.7547\n",
      "[DEBUG] Step   6499 | Ep  619 | Eps 0.936 | MeanReward(10ep):   9.200 | MeanLoss(100it):  0.08612 | MeanQ:  49.134 | GradNorm:  1.1076\n",
      "[DEBUG] Step   6513 | Ep  620 | Eps 0.936 | MeanReward(10ep):  10.100 | MeanLoss(100it):  0.08600 | MeanQ:  49.180 | GradNorm:  0.9172\n",
      "[cartpole] Ep 620 Steps 6513 RecentMean 10.10 Eps 0.936\n",
      "Eval @ Ep 620 and 6513 steps => mean return 149.00\n",
      "[DEBUG] Step   6520 | Ep  621 | Eps 0.935 | MeanReward(10ep):   9.400 | MeanLoss(100it):  0.08610 | MeanQ:  48.703 | GradNorm:  0.5993\n",
      "[DEBUG] Step   6558 | Ep  622 | Eps 0.935 | MeanReward(10ep):  12.700 | MeanLoss(100it):  0.08518 | MeanQ:  49.151 | GradNorm:  0.1725\n",
      "[DEBUG] Step   6568 | Ep  623 | Eps 0.935 | MeanReward(10ep):  12.800 | MeanLoss(100it):  0.08487 | MeanQ:  49.116 | GradNorm:  0.4740\n",
      "[DEBUG] Step   6574 | Ep  624 | Eps 0.935 | MeanReward(10ep):  12.400 | MeanLoss(100it):  0.08485 | MeanQ:  49.087 | GradNorm:  0.3949\n",
      "[DEBUG] Step   6583 | Ep  625 | Eps 0.935 | MeanReward(10ep):  12.000 | MeanLoss(100it):  0.08474 | MeanQ:  49.063 | GradNorm:  0.8002\n",
      "[DEBUG] Step   6593 | Ep  626 | Eps 0.935 | MeanReward(10ep):  11.300 | MeanLoss(100it):  0.08474 | MeanQ:  49.092 | GradNorm:  0.4538\n",
      "[DEBUG] Step   6612 | Ep  627 | Eps 0.935 | MeanReward(10ep):  12.100 | MeanLoss(100it):  0.08458 | MeanQ:  49.068 | GradNorm:  0.8500\n",
      "[DEBUG] Step   6636 | Ep  628 | Eps 0.934 | MeanReward(10ep):  13.900 | MeanLoss(100it):  0.08507 | MeanQ:  49.054 | GradNorm:  0.1857\n",
      "[DEBUG] Step   6646 | Ep  629 | Eps 0.934 | MeanReward(10ep):  13.700 | MeanLoss(100it):  0.08505 | MeanQ:  49.103 | GradNorm:  0.6300\n",
      "[DEBUG] Step   6652 | Ep  630 | Eps 0.934 | MeanReward(10ep):  12.900 | MeanLoss(100it):  0.08480 | MeanQ:  49.112 | GradNorm:  0.2660\n",
      "[cartpole] Ep 630 Steps 6652 RecentMean 12.90 Eps 0.934\n",
      "[DEBUG] Step   6663 | Ep  631 | Eps 0.934 | MeanReward(10ep):  13.300 | MeanLoss(100it):  0.08472 | MeanQ:  49.074 | GradNorm:  0.8068\n",
      "[DEBUG] Step   6674 | Ep  632 | Eps 0.934 | MeanReward(10ep):  10.600 | MeanLoss(100it):  0.08479 | MeanQ:  49.003 | GradNorm:  0.4078\n",
      "[DEBUG] Step   6686 | Ep  633 | Eps 0.934 | MeanReward(10ep):  10.800 | MeanLoss(100it):  0.08463 | MeanQ:  48.953 | GradNorm:  0.4972\n",
      "[DEBUG] Step   6694 | Ep  634 | Eps 0.934 | MeanReward(10ep):  11.000 | MeanLoss(100it):  0.08465 | MeanQ:  48.955 | GradNorm:  0.5665\n",
      "[DEBUG] Step   6713 | Ep  635 | Eps 0.934 | MeanReward(10ep):  12.000 | MeanLoss(100it):  0.08378 | MeanQ:  48.899 | GradNorm:  0.5745\n",
      "[DEBUG] Step   6721 | Ep  636 | Eps 0.933 | MeanReward(10ep):  11.800 | MeanLoss(100it):  0.08356 | MeanQ:  48.808 | GradNorm:  0.2485\n",
      "[DEBUG] Step   6733 | Ep  637 | Eps 0.933 | MeanReward(10ep):  11.100 | MeanLoss(100it):  0.08351 | MeanQ:  48.818 | GradNorm:  0.6748\n",
      "[DEBUG] Step   6749 | Ep  638 | Eps 0.933 | MeanReward(10ep):  10.300 | MeanLoss(100it):  0.08299 | MeanQ:  48.898 | GradNorm:  0.4787\n",
      "[DEBUG] Step   6756 | Ep  639 | Eps 0.933 | MeanReward(10ep):  10.000 | MeanLoss(100it):  0.08307 | MeanQ:  48.899 | GradNorm:  0.3028\n",
      "[DEBUG] Step   6766 | Ep  640 | Eps 0.933 | MeanReward(10ep):  10.400 | MeanLoss(100it):  0.08299 | MeanQ:  48.892 | GradNorm:  0.4085\n",
      "[cartpole] Ep 640 Steps 6766 RecentMean 10.40 Eps 0.933\n",
      "Eval @ Ep 640 and 6766 steps => mean return 395.00\n",
      "[DEBUG] Step   6772 | Ep  641 | Eps 0.933 | MeanReward(10ep):   9.900 | MeanLoss(100it):  0.08249 | MeanQ:  49.158 | GradNorm:  0.6464\n",
      "[DEBUG] Step   6781 | Ep  642 | Eps 0.933 | MeanReward(10ep):   9.700 | MeanLoss(100it):  0.08237 | MeanQ:  48.884 | GradNorm:  0.1963\n",
      "[DEBUG] Step   6797 | Ep  643 | Eps 0.933 | MeanReward(10ep):  10.100 | MeanLoss(100it):  0.08231 | MeanQ:  48.945 | GradNorm:  0.8488\n",
      "[DEBUG] Step   6804 | Ep  644 | Eps 0.933 | MeanReward(10ep):  10.000 | MeanLoss(100it):  0.08249 | MeanQ:  48.987 | GradNorm:  0.6516\n",
      "[DEBUG] Step   6810 | Ep  645 | Eps 0.933 | MeanReward(10ep):   8.700 | MeanLoss(100it):  0.08232 | MeanQ:  48.978 | GradNorm:  0.9495\n",
      "[DEBUG] Step   6829 | Ep  646 | Eps 0.932 | MeanReward(10ep):   9.800 | MeanLoss(100it):  0.08234 | MeanQ:  48.845 | GradNorm:  0.6363\n",
      "[DEBUG] Step   6834 | Ep  647 | Eps 0.932 | MeanReward(10ep):   9.100 | MeanLoss(100it):  0.08233 | MeanQ:  48.853 | GradNorm:  0.7483\n",
      "[DEBUG] Step   6842 | Ep  648 | Eps 0.932 | MeanReward(10ep):   8.300 | MeanLoss(100it):  0.08238 | MeanQ:  48.901 | GradNorm:  0.4419\n",
      "[DEBUG] Step   6853 | Ep  649 | Eps 0.932 | MeanReward(10ep):   8.700 | MeanLoss(100it):  0.08162 | MeanQ:  48.937 | GradNorm:  0.3369\n",
      "[DEBUG] Step   6902 | Ep  650 | Eps 0.932 | MeanReward(10ep):  12.600 | MeanLoss(100it):  0.08202 | MeanQ:  48.890 | GradNorm:  0.3787\n",
      "[cartpole] Ep 650 Steps 6902 RecentMean 12.60 Eps 0.932\n",
      "[DEBUG] Step   6908 | Ep  651 | Eps 0.932 | MeanReward(10ep):  12.600 | MeanLoss(100it):  0.08202 | MeanQ:  48.848 | GradNorm:  0.9490\n",
      "[DEBUG] Step   6914 | Ep  652 | Eps 0.932 | MeanReward(10ep):  12.300 | MeanLoss(100it):  0.08195 | MeanQ:  48.808 | GradNorm:  0.6024\n",
      "[DEBUG] Step   6921 | Ep  653 | Eps 0.931 | MeanReward(10ep):  11.400 | MeanLoss(100it):  0.08154 | MeanQ:  48.739 | GradNorm:  0.5357\n",
      "[DEBUG] Step   6947 | Ep  654 | Eps 0.931 | MeanReward(10ep):  13.300 | MeanLoss(100it):  0.08178 | MeanQ:  48.710 | GradNorm:  0.5961\n",
      "[DEBUG] Step   6961 | Ep  655 | Eps 0.931 | MeanReward(10ep):  14.100 | MeanLoss(100it):  0.08226 | MeanQ:  48.694 | GradNorm:  0.4979\n",
      "[DEBUG] Step   6984 | Ep  656 | Eps 0.931 | MeanReward(10ep):  14.500 | MeanLoss(100it):  0.08238 | MeanQ:  48.699 | GradNorm:  0.3879\n",
      "[DEBUG] Step   7000 | Ep  657 | Eps 0.931 | MeanReward(10ep):  15.600 | MeanLoss(100it):  0.08201 | MeanQ:  48.726 | GradNorm:  0.5758\n",
      "[DEBUG] Step   7008 | Ep  658 | Eps 0.931 | MeanReward(10ep):  15.600 | MeanLoss(100it):  0.08120 | MeanQ:  48.722 | GradNorm:  0.5916\n",
      "[DEBUG] Step   7019 | Ep  659 | Eps 0.931 | MeanReward(10ep):  15.600 | MeanLoss(100it):  0.08081 | MeanQ:  48.705 | GradNorm:  0.4289\n",
      "[DEBUG] Step   7045 | Ep  660 | Eps 0.930 | MeanReward(10ep):  13.300 | MeanLoss(100it):  0.07959 | MeanQ:  48.790 | GradNorm:  0.4330\n",
      "[cartpole] Ep 660 Steps 7045 RecentMean 13.30 Eps 0.930\n",
      "Eval @ Ep 660 and 7045 steps => mean return 268.00\n",
      "[DEBUG] Step   7054 | Ep  661 | Eps 0.930 | MeanReward(10ep):  13.600 | MeanLoss(100it):  0.07948 | MeanQ:  49.071 | GradNorm:  0.2099\n",
      "[DEBUG] Step   7060 | Ep  662 | Eps 0.930 | MeanReward(10ep):  13.600 | MeanLoss(100it):  0.07911 | MeanQ:  48.844 | GradNorm:  0.4957\n",
      "[DEBUG] Step   7072 | Ep  663 | Eps 0.930 | MeanReward(10ep):  14.100 | MeanLoss(100it):  0.07884 | MeanQ:  48.792 | GradNorm:  0.3952\n",
      "[DEBUG] Step   7081 | Ep  664 | Eps 0.930 | MeanReward(10ep):  12.400 | MeanLoss(100it):  0.07874 | MeanQ:  48.778 | GradNorm:  0.3700\n",
      "[DEBUG] Step   7088 | Ep  665 | Eps 0.930 | MeanReward(10ep):  11.700 | MeanLoss(100it):  0.07865 | MeanQ:  48.787 | GradNorm:  0.2666\n",
      "[DEBUG] Step   7119 | Ep  666 | Eps 0.930 | MeanReward(10ep):  12.500 | MeanLoss(100it):  0.07875 | MeanQ:  48.608 | GradNorm:  0.6045\n",
      "[DEBUG] Step   7129 | Ep  667 | Eps 0.929 | MeanReward(10ep):  11.900 | MeanLoss(100it):  0.07904 | MeanQ:  48.597 | GradNorm:  0.2101\n",
      "[DEBUG] Step   7136 | Ep  668 | Eps 0.929 | MeanReward(10ep):  11.800 | MeanLoss(100it):  0.07924 | MeanQ:  48.625 | GradNorm:  0.1446\n",
      "[DEBUG] Step   7142 | Ep  669 | Eps 0.929 | MeanReward(10ep):  11.300 | MeanLoss(100it):  0.07924 | MeanQ:  48.630 | GradNorm:  0.4327\n",
      "[DEBUG] Step   7153 | Ep  670 | Eps 0.929 | MeanReward(10ep):   9.800 | MeanLoss(100it):  0.07963 | MeanQ:  48.611 | GradNorm:  0.5074\n",
      "[cartpole] Ep 670 Steps 7153 RecentMean 9.80 Eps 0.929\n",
      "[DEBUG] Step   7163 | Ep  671 | Eps 0.929 | MeanReward(10ep):   9.900 | MeanLoss(100it):  0.07944 | MeanQ:  48.544 | GradNorm:  0.5353\n",
      "[DEBUG] Step   7170 | Ep  672 | Eps 0.929 | MeanReward(10ep):  10.000 | MeanLoss(100it):  0.07968 | MeanQ:  48.480 | GradNorm:  0.3829\n",
      "[DEBUG] Step   7178 | Ep  673 | Eps 0.929 | MeanReward(10ep):   9.600 | MeanLoss(100it):  0.07979 | MeanQ:  48.492 | GradNorm:  0.3786\n",
      "[DEBUG] Step   7184 | Ep  674 | Eps 0.929 | MeanReward(10ep):   9.300 | MeanLoss(100it):  0.07996 | MeanQ:  48.518 | GradNorm:  0.3749\n",
      "[DEBUG] Step   7190 | Ep  675 | Eps 0.929 | MeanReward(10ep):   9.200 | MeanLoss(100it):  0.07962 | MeanQ:  48.533 | GradNorm:  0.4132\n",
      "[DEBUG] Step   7201 | Ep  676 | Eps 0.929 | MeanReward(10ep):   7.200 | MeanLoss(100it):  0.07956 | MeanQ:  48.569 | GradNorm:  0.6645\n",
      "[DEBUG] Step   7207 | Ep  677 | Eps 0.929 | MeanReward(10ep):   6.800 | MeanLoss(100it):  0.07957 | MeanQ:  48.563 | GradNorm:  0.4637\n",
      "[DEBUG] Step   7214 | Ep  678 | Eps 0.929 | MeanReward(10ep):   6.800 | MeanLoss(100it):  0.07961 | MeanQ:  48.549 | GradNorm:  0.5086\n",
      "[DEBUG] Step   7221 | Ep  679 | Eps 0.929 | MeanReward(10ep):   6.900 | MeanLoss(100it):  0.07941 | MeanQ:  48.543 | GradNorm:  0.3937\n",
      "[DEBUG] Step   7229 | Ep  680 | Eps 0.928 | MeanReward(10ep):   6.600 | MeanLoss(100it):  0.07901 | MeanQ:  48.567 | GradNorm:  0.3238\n",
      "[cartpole] Ep 680 Steps 7229 RecentMean 6.60 Eps 0.928\n",
      "Eval @ Ep 680 and 7229 steps => mean return 64.00\n",
      "[DEBUG] Step   7239 | Ep  681 | Eps 0.928 | MeanReward(10ep):   6.600 | MeanLoss(100it):  0.07921 | MeanQ:  48.252 | GradNorm:  0.4426\n",
      "[DEBUG] Step   7251 | Ep  682 | Eps 0.928 | MeanReward(10ep):   7.100 | MeanLoss(100it):  0.07929 | MeanQ:  48.707 | GradNorm:  0.6688\n",
      "[DEBUG] Step   7273 | Ep  683 | Eps 0.928 | MeanReward(10ep):   8.500 | MeanLoss(100it):  0.07899 | MeanQ:  48.605 | GradNorm:  0.2506\n",
      "[DEBUG] Step   7284 | Ep  684 | Eps 0.928 | MeanReward(10ep):   9.000 | MeanLoss(100it):  0.07856 | MeanQ:  48.625 | GradNorm:  0.8566\n",
      "[DEBUG] Step   7295 | Ep  685 | Eps 0.928 | MeanReward(10ep):   9.500 | MeanLoss(100it):  0.07845 | MeanQ:  48.675 | GradNorm:  0.6674\n",
      "[DEBUG] Step   7312 | Ep  686 | Eps 0.928 | MeanReward(10ep):  10.100 | MeanLoss(100it):  0.07825 | MeanQ:  48.650 | GradNorm:  1.0769\n",
      "[DEBUG] Step   7334 | Ep  687 | Eps 0.927 | MeanReward(10ep):  11.700 | MeanLoss(100it):  0.07879 | MeanQ:  48.549 | GradNorm:  0.5476\n",
      "[DEBUG] Step   7350 | Ep  688 | Eps 0.927 | MeanReward(10ep):  12.600 | MeanLoss(100it):  0.07863 | MeanQ:  48.607 | GradNorm:  0.3319\n",
      "[DEBUG] Step   7356 | Ep  689 | Eps 0.927 | MeanReward(10ep):  12.500 | MeanLoss(100it):  0.07869 | MeanQ:  48.587 | GradNorm:  0.2846\n",
      "[DEBUG] Step   7367 | Ep  690 | Eps 0.927 | MeanReward(10ep):  12.800 | MeanLoss(100it):  0.07874 | MeanQ:  48.559 | GradNorm:  0.1248\n",
      "[cartpole] Ep 690 Steps 7367 RecentMean 12.80 Eps 0.927\n",
      "[DEBUG] Step   7376 | Ep  691 | Eps 0.927 | MeanReward(10ep):  12.700 | MeanLoss(100it):  0.07863 | MeanQ:  48.488 | GradNorm:  0.5891\n",
      "[DEBUG] Step   7391 | Ep  692 | Eps 0.927 | MeanReward(10ep):  13.000 | MeanLoss(100it):  0.07853 | MeanQ:  48.479 | GradNorm:  0.9058\n",
      "[DEBUG] Step   7403 | Ep  693 | Eps 0.927 | MeanReward(10ep):  12.000 | MeanLoss(100it):  0.07886 | MeanQ:  48.515 | GradNorm:  0.4109\n",
      "[DEBUG] Step   7413 | Ep  694 | Eps 0.927 | MeanReward(10ep):  11.900 | MeanLoss(100it):  0.07904 | MeanQ:  48.493 | GradNorm:  0.4826\n",
      "[DEBUG] Step   7420 | Ep  695 | Eps 0.927 | MeanReward(10ep):  11.500 | MeanLoss(100it):  0.07921 | MeanQ:  48.451 | GradNorm:  0.4621\n",
      "[DEBUG] Step   7427 | Ep  696 | Eps 0.926 | MeanReward(10ep):  10.500 | MeanLoss(100it):  0.07899 | MeanQ:  48.431 | GradNorm:  0.3622\n",
      "[DEBUG] Step   7432 | Ep  697 | Eps 0.926 | MeanReward(10ep):   8.800 | MeanLoss(100it):  0.07901 | MeanQ:  48.437 | GradNorm:  0.3730\n",
      "[DEBUG] Step   7438 | Ep  698 | Eps 0.926 | MeanReward(10ep):   7.800 | MeanLoss(100it):  0.07914 | MeanQ:  48.461 | GradNorm:  0.7213\n",
      "[DEBUG] Step   7458 | Ep  699 | Eps 0.926 | MeanReward(10ep):   9.200 | MeanLoss(100it):  0.07991 | MeanQ:  48.576 | GradNorm:  0.9669\n",
      "[DEBUG] Step   7464 | Ep  700 | Eps 0.926 | MeanReward(10ep):   8.700 | MeanLoss(100it):  0.07997 | MeanQ:  48.524 | GradNorm:  0.5039\n",
      "[cartpole] Ep 700 Steps 7464 RecentMean 8.70 Eps 0.926\n",
      "Eval @ Ep 700 and 7464 steps => mean return 41.00\n",
      "[DEBUG] Step   7482 | Ep  701 | Eps 0.926 | MeanReward(10ep):   9.600 | MeanLoss(100it):  0.07997 | MeanQ:  48.144 | GradNorm:  0.5238\n",
      "[DEBUG] Step   7490 | Ep  702 | Eps 0.926 | MeanReward(10ep):   8.900 | MeanLoss(100it):  0.08000 | MeanQ:  48.555 | GradNorm:  0.4688\n",
      "[DEBUG] Step   7496 | Ep  703 | Eps 0.926 | MeanReward(10ep):   8.300 | MeanLoss(100it):  0.08036 | MeanQ:  48.565 | GradNorm:  0.4616\n",
      "[DEBUG] Step   7522 | Ep  704 | Eps 0.926 | MeanReward(10ep):   9.900 | MeanLoss(100it):  0.08048 | MeanQ:  48.401 | GradNorm:  0.5820\n",
      "[DEBUG] Step   7529 | Ep  705 | Eps 0.925 | MeanReward(10ep):   9.900 | MeanLoss(100it):  0.08040 | MeanQ:  48.408 | GradNorm:  0.3854\n",
      "[DEBUG] Step   7536 | Ep  706 | Eps 0.925 | MeanReward(10ep):   9.900 | MeanLoss(100it):  0.08059 | MeanQ:  48.454 | GradNorm:  0.4892\n",
      "[DEBUG] Step   7542 | Ep  707 | Eps 0.925 | MeanReward(10ep):  10.000 | MeanLoss(100it):  0.08065 | MeanQ:  48.484 | GradNorm:  0.2456\n",
      "[DEBUG] Step   7559 | Ep  708 | Eps 0.925 | MeanReward(10ep):  11.100 | MeanLoss(100it):  0.08100 | MeanQ:  48.425 | GradNorm:  1.1019\n",
      "[DEBUG] Step   7575 | Ep  709 | Eps 0.925 | MeanReward(10ep):  10.700 | MeanLoss(100it):  0.08147 | MeanQ:  48.408 | GradNorm:  1.3502\n",
      "[DEBUG] Step   7583 | Ep  710 | Eps 0.925 | MeanReward(10ep):  10.900 | MeanLoss(100it):  0.08187 | MeanQ:  48.471 | GradNorm:  0.4793\n",
      "[cartpole] Ep 710 Steps 7583 RecentMean 10.90 Eps 0.925\n",
      "[DEBUG] Step   7589 | Ep  711 | Eps 0.925 | MeanReward(10ep):   9.700 | MeanLoss(100it):  0.08183 | MeanQ:  48.494 | GradNorm:  0.6181\n",
      "[DEBUG] Step   7595 | Ep  712 | Eps 0.925 | MeanReward(10ep):   9.500 | MeanLoss(100it):  0.08171 | MeanQ:  48.492 | GradNorm:  0.3968\n",
      "[DEBUG] Step   7601 | Ep  713 | Eps 0.925 | MeanReward(10ep):   9.500 | MeanLoss(100it):  0.08184 | MeanQ:  48.454 | GradNorm:  0.2493\n",
      "[DEBUG] Step   7607 | Ep  714 | Eps 0.925 | MeanReward(10ep):   7.500 | MeanLoss(100it):  0.08217 | MeanQ:  48.419 | GradNorm:  0.5805\n",
      "[DEBUG] Step   7626 | Ep  715 | Eps 0.925 | MeanReward(10ep):   8.700 | MeanLoss(100it):  0.08282 | MeanQ:  48.399 | GradNorm:  0.1840\n",
      "[DEBUG] Step   7631 | Ep  716 | Eps 0.924 | MeanReward(10ep):   8.500 | MeanLoss(100it):  0.08276 | MeanQ:  48.422 | GradNorm:  0.5653\n",
      "[DEBUG] Step   7651 | Ep  717 | Eps 0.924 | MeanReward(10ep):   9.900 | MeanLoss(100it):  0.08249 | MeanQ:  48.425 | GradNorm:  0.9939\n",
      "[DEBUG] Step   7661 | Ep  718 | Eps 0.924 | MeanReward(10ep):   9.200 | MeanLoss(100it):  0.08234 | MeanQ:  48.307 | GradNorm:  0.5495\n",
      "[DEBUG] Step   7669 | Ep  719 | Eps 0.924 | MeanReward(10ep):   8.400 | MeanLoss(100it):  0.08227 | MeanQ:  48.269 | GradNorm:  0.5145\n",
      "[DEBUG] Step   7677 | Ep  720 | Eps 0.924 | MeanReward(10ep):   8.400 | MeanLoss(100it):  0.08284 | MeanQ:  48.317 | GradNorm:  1.1277\n",
      "[cartpole] Ep 720 Steps 7677 RecentMean 8.40 Eps 0.924\n",
      "Eval @ Ep 720 and 7677 steps => mean return 77.00\n",
      "[DEBUG] Step   7684 | Ep  721 | Eps 0.924 | MeanReward(10ep):   8.500 | MeanLoss(100it):  0.08273 | MeanQ:  48.003 | GradNorm:  0.4097\n",
      "[DEBUG] Step   7689 | Ep  722 | Eps 0.924 | MeanReward(10ep):   8.400 | MeanLoss(100it):  0.08261 | MeanQ:  48.388 | GradNorm:  0.2090\n",
      "[DEBUG] Step   7697 | Ep  723 | Eps 0.924 | MeanReward(10ep):   8.600 | MeanLoss(100it):  0.08272 | MeanQ:  48.401 | GradNorm:  0.6012\n",
      "[DEBUG] Step   7703 | Ep  724 | Eps 0.924 | MeanReward(10ep):   8.600 | MeanLoss(100it):  0.08268 | MeanQ:  48.394 | GradNorm:  0.4052\n",
      "[DEBUG] Step   7709 | Ep  725 | Eps 0.924 | MeanReward(10ep):   7.300 | MeanLoss(100it):  0.08270 | MeanQ:  48.335 | GradNorm:  0.3108\n",
      "[DEBUG] Step   7721 | Ep  726 | Eps 0.924 | MeanReward(10ep):   8.000 | MeanLoss(100it):  0.08188 | MeanQ:  48.312 | GradNorm:  0.4052\n",
      "[DEBUG] Step   7733 | Ep  727 | Eps 0.923 | MeanReward(10ep):   7.200 | MeanLoss(100it):  0.08168 | MeanQ:  48.336 | GradNorm:  0.3106\n",
      "[DEBUG] Step   7743 | Ep  728 | Eps 0.923 | MeanReward(10ep):   7.200 | MeanLoss(100it):  0.08167 | MeanQ:  48.342 | GradNorm:  0.5442\n",
      "[DEBUG] Step   7751 | Ep  729 | Eps 0.923 | MeanReward(10ep):   7.200 | MeanLoss(100it):  0.08119 | MeanQ:  48.357 | GradNorm:  0.5530\n",
      "[DEBUG] Step   7762 | Ep  730 | Eps 0.923 | MeanReward(10ep):   7.500 | MeanLoss(100it):  0.08098 | MeanQ:  48.324 | GradNorm:  0.4081\n",
      "[cartpole] Ep 730 Steps 7762 RecentMean 7.50 Eps 0.923\n",
      "[DEBUG] Step   7772 | Ep  731 | Eps 0.923 | MeanReward(10ep):   7.800 | MeanLoss(100it):  0.08066 | MeanQ:  48.271 | GradNorm:  0.3948\n",
      "[DEBUG] Step   7777 | Ep  732 | Eps 0.923 | MeanReward(10ep):   7.800 | MeanLoss(100it):  0.08061 | MeanQ:  48.275 | GradNorm:  0.4113\n",
      "[DEBUG] Step   7790 | Ep  733 | Eps 0.923 | MeanReward(10ep):   8.300 | MeanLoss(100it):  0.08057 | MeanQ:  48.385 | GradNorm:  0.7291\n",
      "[DEBUG] Step   7807 | Ep  734 | Eps 0.923 | MeanReward(10ep):   9.400 | MeanLoss(100it):  0.08054 | MeanQ:  48.486 | GradNorm:  0.9247\n",
      "[DEBUG] Step   7817 | Ep  735 | Eps 0.923 | MeanReward(10ep):   9.800 | MeanLoss(100it):  0.08078 | MeanQ:  48.421 | GradNorm:  0.1677\n",
      "[DEBUG] Step   7824 | Ep  736 | Eps 0.923 | MeanReward(10ep):   9.300 | MeanLoss(100it):  0.08100 | MeanQ:  48.396 | GradNorm:  0.4789\n",
      "[DEBUG] Step   7830 | Ep  737 | Eps 0.922 | MeanReward(10ep):   8.700 | MeanLoss(100it):  0.08083 | MeanQ:  48.383 | GradNorm:  0.2754\n",
      "[DEBUG] Step   7836 | Ep  738 | Eps 0.922 | MeanReward(10ep):   8.300 | MeanLoss(100it):  0.08080 | MeanQ:  48.386 | GradNorm:  0.2675\n",
      "[DEBUG] Step   7843 | Ep  739 | Eps 0.922 | MeanReward(10ep):   8.200 | MeanLoss(100it):  0.08045 | MeanQ:  48.394 | GradNorm:  0.4449\n",
      "[DEBUG] Step   7848 | Ep  740 | Eps 0.922 | MeanReward(10ep):   7.600 | MeanLoss(100it):  0.08019 | MeanQ:  48.431 | GradNorm:  0.4813\n",
      "[cartpole] Ep 740 Steps 7848 RecentMean 7.60 Eps 0.922\n",
      "Eval @ Ep 740 and 7848 steps => mean return 169.00\n",
      "[DEBUG] Step   7859 | Ep  741 | Eps 0.922 | MeanReward(10ep):   7.700 | MeanLoss(100it):  0.07975 | MeanQ:  48.085 | GradNorm:  0.6190\n",
      "[DEBUG] Step   7865 | Ep  742 | Eps 0.922 | MeanReward(10ep):   7.800 | MeanLoss(100it):  0.08006 | MeanQ:  48.407 | GradNorm:  0.3720\n",
      "[DEBUG] Step   7902 | Ep  743 | Eps 0.922 | MeanReward(10ep):  10.200 | MeanLoss(100it):  0.07950 | MeanQ:  48.435 | GradNorm:  0.3960\n",
      "[DEBUG] Step   7908 | Ep  744 | Eps 0.922 | MeanReward(10ep):   9.100 | MeanLoss(100it):  0.07909 | MeanQ:  48.467 | GradNorm:  0.3828\n",
      "[DEBUG] Step   7915 | Ep  745 | Eps 0.922 | MeanReward(10ep):   8.800 | MeanLoss(100it):  0.07908 | MeanQ:  48.459 | GradNorm:  0.7077\n",
      "[DEBUG] Step   7941 | Ep  746 | Eps 0.921 | MeanReward(10ep):  10.700 | MeanLoss(100it):  0.07740 | MeanQ:  48.437 | GradNorm:  0.4271\n",
      "[DEBUG] Step   7948 | Ep  747 | Eps 0.921 | MeanReward(10ep):  10.800 | MeanLoss(100it):  0.07708 | MeanQ:  48.487 | GradNorm:  0.1473\n",
      "[DEBUG] Step   7956 | Ep  748 | Eps 0.921 | MeanReward(10ep):  11.000 | MeanLoss(100it):  0.07656 | MeanQ:  48.505 | GradNorm:  0.8196\n",
      "[DEBUG] Step   7980 | Ep  749 | Eps 0.921 | MeanReward(10ep):  12.700 | MeanLoss(100it):  0.07487 | MeanQ:  48.488 | GradNorm:  0.4779\n",
      "[DEBUG] Step   7986 | Ep  750 | Eps 0.921 | MeanReward(10ep):  12.800 | MeanLoss(100it):  0.07491 | MeanQ:  48.487 | GradNorm:  0.6777\n",
      "[cartpole] Ep 750 Steps 7986 RecentMean 12.80 Eps 0.921\n",
      "[DEBUG] Step   8014 | Ep  751 | Eps 0.921 | MeanReward(10ep):  14.500 | MeanLoss(100it):  0.07377 | MeanQ:  48.276 | GradNorm:  0.6408\n",
      "[DEBUG] Step   8044 | Ep  752 | Eps 0.920 | MeanReward(10ep):  16.900 | MeanLoss(100it):  0.07479 | MeanQ:  48.252 | GradNorm:  0.4037\n",
      "[DEBUG] Step   8050 | Ep  753 | Eps 0.920 | MeanReward(10ep):  13.800 | MeanLoss(100it):  0.07484 | MeanQ:  48.235 | GradNorm:  0.6054\n",
      "[DEBUG] Step   8057 | Ep  754 | Eps 0.920 | MeanReward(10ep):  13.900 | MeanLoss(100it):  0.07500 | MeanQ:  48.201 | GradNorm:  0.7757\n",
      "[DEBUG] Step   8071 | Ep  755 | Eps 0.920 | MeanReward(10ep):  14.600 | MeanLoss(100it):  0.07542 | MeanQ:  48.147 | GradNorm:  0.6518\n",
      "[DEBUG] Step   8083 | Ep  756 | Eps 0.920 | MeanReward(10ep):  13.200 | MeanLoss(100it):  0.07541 | MeanQ:  48.193 | GradNorm:  0.3987\n",
      "[DEBUG] Step   8091 | Ep  757 | Eps 0.920 | MeanReward(10ep):  13.300 | MeanLoss(100it):  0.07567 | MeanQ:  48.226 | GradNorm:  0.3037\n",
      "[DEBUG] Step   8097 | Ep  758 | Eps 0.920 | MeanReward(10ep):  13.100 | MeanLoss(100it):  0.07579 | MeanQ:  48.235 | GradNorm:  0.6978\n",
      "[DEBUG] Step   8110 | Ep  759 | Eps 0.920 | MeanReward(10ep):  12.000 | MeanLoss(100it):  0.07567 | MeanQ:  48.206 | GradNorm:  0.5644\n",
      "[DEBUG] Step   8121 | Ep  760 | Eps 0.920 | MeanReward(10ep):  12.500 | MeanLoss(100it):  0.07530 | MeanQ:  48.170 | GradNorm:  0.6242\n",
      "[cartpole] Ep 760 Steps 8121 RecentMean 12.50 Eps 0.920\n",
      "Eval @ Ep 760 and 8121 steps => mean return 149.00\n",
      "[DEBUG] Step   8128 | Ep  761 | Eps 0.920 | MeanReward(10ep):  10.400 | MeanLoss(100it):  0.07523 | MeanQ:  47.851 | GradNorm:  0.3591\n",
      "[DEBUG] Step   8139 | Ep  762 | Eps 0.919 | MeanReward(10ep):   8.500 | MeanLoss(100it):  0.07479 | MeanQ:  48.268 | GradNorm:  0.4835\n",
      "[DEBUG] Step   8145 | Ep  763 | Eps 0.919 | MeanReward(10ep):   8.500 | MeanLoss(100it):  0.07540 | MeanQ:  48.293 | GradNorm:  0.7587\n",
      "[DEBUG] Step   8155 | Ep  764 | Eps 0.919 | MeanReward(10ep):   8.800 | MeanLoss(100it):  0.07542 | MeanQ:  48.261 | GradNorm:  0.2268\n",
      "[DEBUG] Step   8165 | Ep  765 | Eps 0.919 | MeanReward(10ep):   8.400 | MeanLoss(100it):  0.07531 | MeanQ:  48.167 | GradNorm:  0.4101\n",
      "[DEBUG] Step   8185 | Ep  766 | Eps 0.919 | MeanReward(10ep):   9.200 | MeanLoss(100it):  0.07512 | MeanQ:  48.213 | GradNorm:  0.3639\n",
      "[DEBUG] Step   8192 | Ep  767 | Eps 0.919 | MeanReward(10ep):   9.100 | MeanLoss(100it):  0.07496 | MeanQ:  48.241 | GradNorm:  0.3941\n",
      "[DEBUG] Step   8202 | Ep  768 | Eps 0.919 | MeanReward(10ep):   9.500 | MeanLoss(100it):  0.07492 | MeanQ:  48.279 | GradNorm:  0.5758\n",
      "[DEBUG] Step   8210 | Ep  769 | Eps 0.919 | MeanReward(10ep):   9.000 | MeanLoss(100it):  0.07459 | MeanQ:  48.281 | GradNorm:  0.8290\n",
      "[DEBUG] Step   8218 | Ep  770 | Eps 0.919 | MeanReward(10ep):   8.700 | MeanLoss(100it):  0.07456 | MeanQ:  48.246 | GradNorm:  0.6049\n",
      "[cartpole] Ep 770 Steps 8218 RecentMean 8.70 Eps 0.919\n",
      "[DEBUG] Step   8223 | Ep  771 | Eps 0.919 | MeanReward(10ep):   8.500 | MeanLoss(100it):  0.07462 | MeanQ:  48.223 | GradNorm:  0.4251\n",
      "[DEBUG] Step   8233 | Ep  772 | Eps 0.919 | MeanReward(10ep):   8.400 | MeanLoss(100it):  0.07462 | MeanQ:  48.192 | GradNorm:  0.5326\n",
      "[DEBUG] Step   8254 | Ep  773 | Eps 0.918 | MeanReward(10ep):   9.900 | MeanLoss(100it):  0.07513 | MeanQ:  48.206 | GradNorm:  0.6644\n",
      "[DEBUG] Step   8267 | Ep  774 | Eps 0.918 | MeanReward(10ep):  10.200 | MeanLoss(100it):  0.07475 | MeanQ:  48.201 | GradNorm:  0.4256\n",
      "[DEBUG] Step   8283 | Ep  775 | Eps 0.918 | MeanReward(10ep):  10.800 | MeanLoss(100it):  0.07544 | MeanQ:  48.189 | GradNorm:  0.3735\n",
      "[DEBUG] Step   8298 | Ep  776 | Eps 0.918 | MeanReward(10ep):  10.300 | MeanLoss(100it):  0.07528 | MeanQ:  48.221 | GradNorm:  0.6092\n",
      "[DEBUG] Step   8305 | Ep  777 | Eps 0.918 | MeanReward(10ep):  10.300 | MeanLoss(100it):  0.07501 | MeanQ:  48.236 | GradNorm:  0.1463\n",
      "[DEBUG] Step   8311 | Ep  778 | Eps 0.918 | MeanReward(10ep):   9.900 | MeanLoss(100it):  0.07496 | MeanQ:  48.227 | GradNorm:  0.4968\n",
      "[DEBUG] Step   8340 | Ep  779 | Eps 0.917 | MeanReward(10ep):  12.000 | MeanLoss(100it):  0.07447 | MeanQ:  48.090 | GradNorm:  0.3404\n",
      "[DEBUG] Step   8347 | Ep  780 | Eps 0.917 | MeanReward(10ep):  11.900 | MeanLoss(100it):  0.07443 | MeanQ:  48.116 | GradNorm:  0.5924\n",
      "[cartpole] Ep 780 Steps 8347 RecentMean 11.90 Eps 0.917\n",
      "Eval @ Ep 780 and 8347 steps => mean return 41.00\n",
      "[DEBUG] Step   8384 | Ep  781 | Eps 0.917 | MeanReward(10ep):  15.100 | MeanLoss(100it):  0.07390 | MeanQ:  47.828 | GradNorm:  0.3789\n",
      "[DEBUG] Step   8389 | Ep  782 | Eps 0.917 | MeanReward(10ep):  14.600 | MeanLoss(100it):  0.07400 | MeanQ:  48.178 | GradNorm:  0.3799\n",
      "[DEBUG] Step   8400 | Ep  783 | Eps 0.917 | MeanReward(10ep):  13.600 | MeanLoss(100it):  0.07389 | MeanQ:  48.184 | GradNorm:  0.8731\n",
      "[DEBUG] Step   8411 | Ep  784 | Eps 0.917 | MeanReward(10ep):  13.400 | MeanLoss(100it):  0.07380 | MeanQ:  48.192 | GradNorm:  0.4475\n",
      "[DEBUG] Step   8416 | Ep  785 | Eps 0.917 | MeanReward(10ep):  12.300 | MeanLoss(100it):  0.07346 | MeanQ:  48.176 | GradNorm:  0.2974\n",
      "[DEBUG] Step   8425 | Ep  786 | Eps 0.917 | MeanReward(10ep):  11.700 | MeanLoss(100it):  0.07341 | MeanQ:  48.143 | GradNorm:  0.4864\n",
      "[DEBUG] Step   8431 | Ep  787 | Eps 0.917 | MeanReward(10ep):  11.600 | MeanLoss(100it):  0.07338 | MeanQ:  48.129 | GradNorm:  0.2187\n",
      "[DEBUG] Step   8437 | Ep  788 | Eps 0.916 | MeanReward(10ep):  11.600 | MeanLoss(100it):  0.07349 | MeanQ:  48.073 | GradNorm:  0.2831\n",
      "[DEBUG] Step   8447 | Ep  789 | Eps 0.916 | MeanReward(10ep):   9.700 | MeanLoss(100it):  0.07325 | MeanQ:  48.025 | GradNorm:  0.3700\n",
      "[DEBUG] Step   8456 | Ep  790 | Eps 0.916 | MeanReward(10ep):   9.900 | MeanLoss(100it):  0.07336 | MeanQ:  48.036 | GradNorm:  0.3372\n",
      "[cartpole] Ep 790 Steps 8456 RecentMean 9.90 Eps 0.916\n",
      "[DEBUG] Step   8463 | Ep  791 | Eps 0.916 | MeanReward(10ep):   6.900 | MeanLoss(100it):  0.07323 | MeanQ:  48.032 | GradNorm:  0.4868\n",
      "[DEBUG] Step   8473 | Ep  792 | Eps 0.916 | MeanReward(10ep):   7.400 | MeanLoss(100it):  0.07290 | MeanQ:  48.019 | GradNorm:  0.4179\n",
      "[DEBUG] Step   8480 | Ep  793 | Eps 0.916 | MeanReward(10ep):   7.000 | MeanLoss(100it):  0.07271 | MeanQ:  48.023 | GradNorm:  0.4277\n",
      "[DEBUG] Step   8494 | Ep  794 | Eps 0.916 | MeanReward(10ep):   7.300 | MeanLoss(100it):  0.07224 | MeanQ:  48.024 | GradNorm:  0.2674\n",
      "[DEBUG] Step   8500 | Ep  795 | Eps 0.916 | MeanReward(10ep):   7.400 | MeanLoss(100it):  0.07204 | MeanQ:  47.993 | GradNorm:  0.1677\n",
      "[DEBUG] Step   8506 | Ep  796 | Eps 0.916 | MeanReward(10ep):   7.100 | MeanLoss(100it):  0.07200 | MeanQ:  47.974 | GradNorm:  0.2481\n",
      "[DEBUG] Step   8533 | Ep  797 | Eps 0.916 | MeanReward(10ep):   9.200 | MeanLoss(100it):  0.07298 | MeanQ:  47.942 | GradNorm:  0.2086\n",
      "[DEBUG] Step   8549 | Ep  798 | Eps 0.915 | MeanReward(10ep):  10.200 | MeanLoss(100it):  0.07241 | MeanQ:  47.879 | GradNorm:  0.3869\n",
      "[DEBUG] Step   8582 | Ep  799 | Eps 0.915 | MeanReward(10ep):  12.500 | MeanLoss(100it):  0.07273 | MeanQ:  47.972 | GradNorm:  1.1772\n",
      "[DEBUG] Step   8591 | Ep  800 | Eps 0.915 | MeanReward(10ep):  12.500 | MeanLoss(100it):  0.07302 | MeanQ:  47.915 | GradNorm:  0.3897\n",
      "[cartpole] Ep 800 Steps 8591 RecentMean 12.50 Eps 0.915\n",
      "Eval @ Ep 800 and 8591 steps => mean return 175.00\n",
      "[DEBUG] Step   8596 | Ep  801 | Eps 0.915 | MeanReward(10ep):  12.300 | MeanLoss(100it):  0.07293 | MeanQ:  47.505 | GradNorm:  0.4022\n",
      "[DEBUG] Step   8620 | Ep  802 | Eps 0.915 | MeanReward(10ep):  13.700 | MeanLoss(100it):  0.07288 | MeanQ:  47.794 | GradNorm:  0.3082\n",
      "[DEBUG] Step   8627 | Ep  803 | Eps 0.915 | MeanReward(10ep):  13.700 | MeanLoss(100it):  0.07296 | MeanQ:  47.792 | GradNorm:  0.6333\n",
      "[DEBUG] Step   8635 | Ep  804 | Eps 0.915 | MeanReward(10ep):  13.100 | MeanLoss(100it):  0.07333 | MeanQ:  47.798 | GradNorm:  0.6809\n",
      "[DEBUG] Step   8648 | Ep  805 | Eps 0.914 | MeanReward(10ep):  13.800 | MeanLoss(100it):  0.07319 | MeanQ:  47.797 | GradNorm:  0.4287\n",
      "[DEBUG] Step   8658 | Ep  806 | Eps 0.914 | MeanReward(10ep):  14.200 | MeanLoss(100it):  0.07289 | MeanQ:  47.793 | GradNorm:  0.4642\n",
      "[DEBUG] Step   8664 | Ep  807 | Eps 0.914 | MeanReward(10ep):  12.100 | MeanLoss(100it):  0.07279 | MeanQ:  47.762 | GradNorm:  0.5151\n",
      "[DEBUG] Step   8671 | Ep  808 | Eps 0.914 | MeanReward(10ep):  11.200 | MeanLoss(100it):  0.07290 | MeanQ:  47.748 | GradNorm:  0.2136\n",
      "[DEBUG] Step   8715 | Ep  809 | Eps 0.914 | MeanReward(10ep):  12.300 | MeanLoss(100it):  0.07265 | MeanQ:  47.761 | GradNorm:  0.8270\n",
      "[DEBUG] Step   8736 | Ep  810 | Eps 0.914 | MeanReward(10ep):  13.500 | MeanLoss(100it):  0.07304 | MeanQ:  47.767 | GradNorm:  0.7344\n",
      "[cartpole] Ep 810 Steps 8736 RecentMean 13.50 Eps 0.914\n",
      "[DEBUG] Step   8755 | Ep  811 | Eps 0.913 | MeanReward(10ep):  14.900 | MeanLoss(100it):  0.07318 | MeanQ:  47.813 | GradNorm:  0.3058\n",
      "[DEBUG] Step   8767 | Ep  812 | Eps 0.913 | MeanReward(10ep):  13.700 | MeanLoss(100it):  0.07380 | MeanQ:  47.813 | GradNorm:  0.3545\n",
      "[DEBUG] Step   8776 | Ep  813 | Eps 0.913 | MeanReward(10ep):  13.900 | MeanLoss(100it):  0.07367 | MeanQ:  47.875 | GradNorm:  0.3464\n",
      "[DEBUG] Step   8805 | Ep  814 | Eps 0.913 | MeanReward(10ep):  16.000 | MeanLoss(100it):  0.07382 | MeanQ:  47.854 | GradNorm:  0.9037\n",
      "[DEBUG] Step   8820 | Ep  815 | Eps 0.913 | MeanReward(10ep):  16.200 | MeanLoss(100it):  0.07434 | MeanQ:  47.804 | GradNorm:  0.3585\n",
      "[DEBUG] Step   8832 | Ep  816 | Eps 0.913 | MeanReward(10ep):  16.400 | MeanLoss(100it):  0.07417 | MeanQ:  47.786 | GradNorm:  0.5660\n",
      "[DEBUG] Step   8846 | Ep  817 | Eps 0.912 | MeanReward(10ep):  17.200 | MeanLoss(100it):  0.07431 | MeanQ:  47.760 | GradNorm:  0.3040\n",
      "[DEBUG] Step   8858 | Ep  818 | Eps 0.912 | MeanReward(10ep):  17.700 | MeanLoss(100it):  0.07465 | MeanQ:  47.760 | GradNorm:  0.9466\n",
      "[DEBUG] Step   8872 | Ep  819 | Eps 0.912 | MeanReward(10ep):  14.700 | MeanLoss(100it):  0.07483 | MeanQ:  47.759 | GradNorm:  0.4968\n",
      "[DEBUG] Step   8878 | Ep  820 | Eps 0.912 | MeanReward(10ep):  13.200 | MeanLoss(100it):  0.07502 | MeanQ:  47.751 | GradNorm:  0.3685\n",
      "[cartpole] Ep 820 Steps 8878 RecentMean 13.20 Eps 0.912\n",
      "Eval @ Ep 820 and 8878 steps => mean return 37.00\n",
      "[DEBUG] Step   8897 | Ep  821 | Eps 0.912 | MeanReward(10ep):  13.200 | MeanLoss(100it):  0.07588 | MeanQ:  47.436 | GradNorm:  0.5923\n",
      "[DEBUG] Step   8911 | Ep  822 | Eps 0.912 | MeanReward(10ep):  13.400 | MeanLoss(100it):  0.07573 | MeanQ:  47.866 | GradNorm:  0.3579\n",
      "[DEBUG] Step   8919 | Ep  823 | Eps 0.912 | MeanReward(10ep):  13.300 | MeanLoss(100it):  0.07566 | MeanQ:  47.862 | GradNorm:  0.4691\n",
      "[DEBUG] Step   8928 | Ep  824 | Eps 0.912 | MeanReward(10ep):  11.300 | MeanLoss(100it):  0.07501 | MeanQ:  47.806 | GradNorm:  0.3807\n",
      "[DEBUG] Step   8935 | Ep  825 | Eps 0.912 | MeanReward(10ep):  10.500 | MeanLoss(100it):  0.07533 | MeanQ:  47.797 | GradNorm:  0.2836\n",
      "[DEBUG] Step   8947 | Ep  826 | Eps 0.911 | MeanReward(10ep):  10.500 | MeanLoss(100it):  0.07530 | MeanQ:  47.825 | GradNorm:  0.3463\n",
      "[DEBUG] Step   8952 | Ep  827 | Eps 0.911 | MeanReward(10ep):   9.600 | MeanLoss(100it):  0.07510 | MeanQ:  47.868 | GradNorm:  0.6041\n",
      "[DEBUG] Step   8961 | Ep  828 | Eps 0.911 | MeanReward(10ep):   9.300 | MeanLoss(100it):  0.07548 | MeanQ:  47.861 | GradNorm:  0.4700\n",
      "[DEBUG] Step   8969 | Ep  829 | Eps 0.911 | MeanReward(10ep):   8.700 | MeanLoss(100it):  0.07549 | MeanQ:  47.817 | GradNorm:  0.4918\n",
      "[DEBUG] Step   8977 | Ep  830 | Eps 0.911 | MeanReward(10ep):   8.900 | MeanLoss(100it):  0.07557 | MeanQ:  47.799 | GradNorm:  0.2914\n",
      "[cartpole] Ep 830 Steps 8977 RecentMean 8.90 Eps 0.911\n",
      "[DEBUG] Step   8991 | Ep  831 | Eps 0.911 | MeanReward(10ep):   8.400 | MeanLoss(100it):  0.07526 | MeanQ:  47.829 | GradNorm:  0.4589\n",
      "[DEBUG] Step   9003 | Ep  832 | Eps 0.911 | MeanReward(10ep):   8.200 | MeanLoss(100it):  0.07532 | MeanQ:  47.857 | GradNorm:  0.5034\n",
      "[DEBUG] Step   9016 | Ep  833 | Eps 0.911 | MeanReward(10ep):   8.700 | MeanLoss(100it):  0.07466 | MeanQ:  47.815 | GradNorm:  0.3190\n",
      "[DEBUG] Step   9026 | Ep  834 | Eps 0.911 | MeanReward(10ep):   8.800 | MeanLoss(100it):  0.07463 | MeanQ:  47.847 | GradNorm:  1.1540\n",
      "[DEBUG] Step   9035 | Ep  835 | Eps 0.911 | MeanReward(10ep):   9.000 | MeanLoss(100it):  0.07432 | MeanQ:  47.925 | GradNorm:  0.1930\n",
      "[DEBUG] Step   9047 | Ep  836 | Eps 0.910 | MeanReward(10ep):   9.000 | MeanLoss(100it):  0.07444 | MeanQ:  47.947 | GradNorm:  0.8015\n",
      "[DEBUG] Step   9057 | Ep  837 | Eps 0.910 | MeanReward(10ep):   9.500 | MeanLoss(100it):  0.07432 | MeanQ:  47.824 | GradNorm:  0.3737\n",
      "[DEBUG] Step   9068 | Ep  838 | Eps 0.910 | MeanReward(10ep):   9.700 | MeanLoss(100it):  0.07451 | MeanQ:  47.762 | GradNorm:  0.8417\n",
      "[DEBUG] Step   9080 | Ep  839 | Eps 0.910 | MeanReward(10ep):  10.100 | MeanLoss(100it):  0.07434 | MeanQ:  47.853 | GradNorm:  0.2266\n",
      "[DEBUG] Step   9091 | Ep  840 | Eps 0.910 | MeanReward(10ep):  10.400 | MeanLoss(100it):  0.07410 | MeanQ:  47.897 | GradNorm:  0.4654\n",
      "[cartpole] Ep 840 Steps 9091 RecentMean 10.40 Eps 0.910\n",
      "Eval @ Ep 840 and 9091 steps => mean return 275.00\n",
      "[DEBUG] Step   9097 | Ep  841 | Eps 0.910 | MeanReward(10ep):   9.600 | MeanLoss(100it):  0.07420 | MeanQ:  48.115 | GradNorm:  0.7244\n",
      "[DEBUG] Step   9107 | Ep  842 | Eps 0.910 | MeanReward(10ep):   9.400 | MeanLoss(100it):  0.07422 | MeanQ:  47.822 | GradNorm:  0.6096\n",
      "[DEBUG] Step   9117 | Ep  843 | Eps 0.910 | MeanReward(10ep):   9.100 | MeanLoss(100it):  0.07380 | MeanQ:  47.780 | GradNorm:  0.4328\n",
      "[DEBUG] Step   9123 | Ep  844 | Eps 0.910 | MeanReward(10ep):   8.700 | MeanLoss(100it):  0.07386 | MeanQ:  47.786 | GradNorm:  0.2941\n",
      "[DEBUG] Step   9133 | Ep  845 | Eps 0.910 | MeanReward(10ep):   8.800 | MeanLoss(100it):  0.07376 | MeanQ:  47.828 | GradNorm:  0.2880\n",
      "[DEBUG] Step   9148 | Ep  846 | Eps 0.909 | MeanReward(10ep):   9.100 | MeanLoss(100it):  0.07425 | MeanQ:  47.932 | GradNorm:  1.1188\n",
      "[DEBUG] Step   9156 | Ep  847 | Eps 0.909 | MeanReward(10ep):   8.900 | MeanLoss(100it):  0.07428 | MeanQ:  47.862 | GradNorm:  0.9796\n",
      "[DEBUG] Step   9166 | Ep  848 | Eps 0.909 | MeanReward(10ep):   8.800 | MeanLoss(100it):  0.07393 | MeanQ:  47.811 | GradNorm:  0.6054\n",
      "[DEBUG] Step   9203 | Ep  849 | Eps 0.909 | MeanReward(10ep):  11.300 | MeanLoss(100it):  0.07471 | MeanQ:  47.869 | GradNorm:  0.2643\n",
      "[DEBUG] Step   9222 | Ep  850 | Eps 0.909 | MeanReward(10ep):  12.100 | MeanLoss(100it):  0.07357 | MeanQ:  47.866 | GradNorm:  0.5473\n",
      "[cartpole] Ep 850 Steps 9222 RecentMean 12.10 Eps 0.909\n",
      "[DEBUG] Step   9230 | Ep  851 | Eps 0.909 | MeanReward(10ep):  12.300 | MeanLoss(100it):  0.07327 | MeanQ:  47.920 | GradNorm:  0.4021\n",
      "[DEBUG] Step   9238 | Ep  852 | Eps 0.909 | MeanReward(10ep):  12.100 | MeanLoss(100it):  0.07289 | MeanQ:  47.927 | GradNorm:  0.2148\n",
      "[DEBUG] Step   9249 | Ep  853 | Eps 0.908 | MeanReward(10ep):  12.200 | MeanLoss(100it):  0.07213 | MeanQ:  47.885 | GradNorm:  0.2167\n",
      "[DEBUG] Step   9257 | Ep  854 | Eps 0.908 | MeanReward(10ep):  12.400 | MeanLoss(100it):  0.07151 | MeanQ:  47.828 | GradNorm:  0.5485\n",
      "[DEBUG] Step   9280 | Ep  855 | Eps 0.908 | MeanReward(10ep):  13.700 | MeanLoss(100it):  0.07035 | MeanQ:  47.845 | GradNorm:  0.4853\n",
      "[DEBUG] Step   9286 | Ep  856 | Eps 0.908 | MeanReward(10ep):  12.800 | MeanLoss(100it):  0.07030 | MeanQ:  47.842 | GradNorm:  0.3345\n",
      "[DEBUG] Step   9292 | Ep  857 | Eps 0.908 | MeanReward(10ep):  12.600 | MeanLoss(100it):  0.06951 | MeanQ:  47.856 | GradNorm:  0.2661\n",
      "[DEBUG] Step   9301 | Ep  858 | Eps 0.908 | MeanReward(10ep):  12.500 | MeanLoss(100it):  0.06931 | MeanQ:  47.876 | GradNorm:  0.2141\n",
      "[DEBUG] Step   9310 | Ep  859 | Eps 0.908 | MeanReward(10ep):   9.700 | MeanLoss(100it):  0.06931 | MeanQ:  47.889 | GradNorm:  0.3295\n",
      "[DEBUG] Step   9324 | Ep  860 | Eps 0.908 | MeanReward(10ep):   9.200 | MeanLoss(100it):  0.06948 | MeanQ:  47.814 | GradNorm:  0.2782\n",
      "[cartpole] Ep 860 Steps 9324 RecentMean 9.20 Eps 0.908\n",
      "Eval @ Ep 860 and 9324 steps => mean return 80.00\n",
      "[DEBUG] Step   9334 | Ep  861 | Eps 0.908 | MeanReward(10ep):   9.400 | MeanLoss(100it):  0.06948 | MeanQ:  47.441 | GradNorm:  0.1629\n",
      "[DEBUG] Step   9341 | Ep  862 | Eps 0.908 | MeanReward(10ep):   9.300 | MeanLoss(100it):  0.06959 | MeanQ:  47.813 | GradNorm:  0.2282\n",
      "[DEBUG] Step   9347 | Ep  863 | Eps 0.907 | MeanReward(10ep):   8.800 | MeanLoss(100it):  0.06953 | MeanQ:  47.837 | GradNorm:  0.5207\n",
      "[DEBUG] Step   9363 | Ep  864 | Eps 0.907 | MeanReward(10ep):   9.600 | MeanLoss(100it):  0.06963 | MeanQ:  47.810 | GradNorm:  0.3755\n",
      "[DEBUG] Step   9388 | Ep  865 | Eps 0.907 | MeanReward(10ep):   9.800 | MeanLoss(100it):  0.07032 | MeanQ:  47.853 | GradNorm:  0.6532\n",
      "[DEBUG] Step   9410 | Ep  866 | Eps 0.907 | MeanReward(10ep):  11.400 | MeanLoss(100it):  0.07107 | MeanQ:  47.659 | GradNorm:  0.4741\n",
      "[DEBUG] Step   9428 | Ep  867 | Eps 0.907 | MeanReward(10ep):  12.600 | MeanLoss(100it):  0.07136 | MeanQ:  47.731 | GradNorm:  0.2985\n",
      "[DEBUG] Step   9443 | Ep  868 | Eps 0.907 | MeanReward(10ep):  13.200 | MeanLoss(100it):  0.07167 | MeanQ:  47.724 | GradNorm:  0.9990\n",
      "[DEBUG] Step   9451 | Ep  869 | Eps 0.906 | MeanReward(10ep):  13.100 | MeanLoss(100it):  0.07191 | MeanQ:  47.645 | GradNorm:  0.1514\n",
      "[DEBUG] Step   9473 | Ep  870 | Eps 0.906 | MeanReward(10ep):  13.900 | MeanLoss(100it):  0.07253 | MeanQ:  47.677 | GradNorm:  0.5194\n",
      "[cartpole] Ep 870 Steps 9473 RecentMean 13.90 Eps 0.906\n",
      "[DEBUG] Step   9489 | Ep  871 | Eps 0.906 | MeanReward(10ep):  14.500 | MeanLoss(100it):  0.07295 | MeanQ:  47.720 | GradNorm:  0.6296\n",
      "[DEBUG] Step   9495 | Ep  872 | Eps 0.906 | MeanReward(10ep):  14.400 | MeanLoss(100it):  0.07298 | MeanQ:  47.688 | GradNorm:  0.4559\n",
      "[DEBUG] Step   9523 | Ep  873 | Eps 0.906 | MeanReward(10ep):  16.600 | MeanLoss(100it):  0.07376 | MeanQ:  47.564 | GradNorm:  1.1187\n",
      "[DEBUG] Step   9532 | Ep  874 | Eps 0.906 | MeanReward(10ep):  15.900 | MeanLoss(100it):  0.07358 | MeanQ:  47.606 | GradNorm:  0.3093\n",
      "[DEBUG] Step   9538 | Ep  875 | Eps 0.906 | MeanReward(10ep):  14.000 | MeanLoss(100it):  0.07370 | MeanQ:  47.593 | GradNorm:  0.6888\n",
      "[DEBUG] Step   9557 | Ep  876 | Eps 0.905 | MeanReward(10ep):  13.700 | MeanLoss(100it):  0.07345 | MeanQ:  47.490 | GradNorm:  0.2721\n",
      "[DEBUG] Step   9564 | Ep  877 | Eps 0.905 | MeanReward(10ep):  12.600 | MeanLoss(100it):  0.07408 | MeanQ:  47.489 | GradNorm:  0.6613\n",
      "[DEBUG] Step   9579 | Ep  878 | Eps 0.905 | MeanReward(10ep):  12.600 | MeanLoss(100it):  0.07417 | MeanQ:  47.421 | GradNorm:  0.2932\n",
      "[DEBUG] Step   9591 | Ep  879 | Eps 0.905 | MeanReward(10ep):  13.000 | MeanLoss(100it):  0.07465 | MeanQ:  47.423 | GradNorm:  0.4890\n",
      "[DEBUG] Step   9597 | Ep  880 | Eps 0.905 | MeanReward(10ep):  11.400 | MeanLoss(100it):  0.07467 | MeanQ:  47.468 | GradNorm:  0.6973\n",
      "[cartpole] Ep 880 Steps 9597 RecentMean 11.40 Eps 0.905\n",
      "Eval @ Ep 880 and 9597 steps => mean return 109.00\n",
      "[DEBUG] Step   9602 | Ep  881 | Eps 0.905 | MeanReward(10ep):  10.300 | MeanLoss(100it):  0.07456 | MeanQ:  47.136 | GradNorm:  0.4043\n",
      "[DEBUG] Step   9633 | Ep  882 | Eps 0.905 | MeanReward(10ep):  12.800 | MeanLoss(100it):  0.07372 | MeanQ:  47.521 | GradNorm:  0.6154\n",
      "[DEBUG] Step   9662 | Ep  883 | Eps 0.904 | MeanReward(10ep):  12.900 | MeanLoss(100it):  0.07358 | MeanQ:  47.628 | GradNorm:  0.3727\n",
      "[DEBUG] Step   9671 | Ep  884 | Eps 0.904 | MeanReward(10ep):  12.900 | MeanLoss(100it):  0.07322 | MeanQ:  47.583 | GradNorm:  0.5688\n",
      "[DEBUG] Step   9680 | Ep  885 | Eps 0.904 | MeanReward(10ep):  13.200 | MeanLoss(100it):  0.07301 | MeanQ:  47.600 | GradNorm:  0.4176\n",
      "[DEBUG] Step   9695 | Ep  886 | Eps 0.904 | MeanReward(10ep):  12.800 | MeanLoss(100it):  0.07295 | MeanQ:  47.625 | GradNorm:  0.5311\n",
      "[DEBUG] Step   9701 | Ep  887 | Eps 0.904 | MeanReward(10ep):  12.700 | MeanLoss(100it):  0.07274 | MeanQ:  47.659 | GradNorm:  0.5722\n",
      "[DEBUG] Step   9709 | Ep  888 | Eps 0.904 | MeanReward(10ep):  12.000 | MeanLoss(100it):  0.07278 | MeanQ:  47.649 | GradNorm:  0.7400\n",
      "[DEBUG] Step   9718 | Ep  889 | Eps 0.904 | MeanReward(10ep):  11.700 | MeanLoss(100it):  0.07272 | MeanQ:  47.617 | GradNorm:  0.4183\n",
      "[DEBUG] Step   9725 | Ep  890 | Eps 0.904 | MeanReward(10ep):  11.800 | MeanLoss(100it):  0.07288 | MeanQ:  47.586 | GradNorm:  0.5821\n",
      "[cartpole] Ep 890 Steps 9725 RecentMean 11.80 Eps 0.904\n",
      "[DEBUG] Step   9740 | Ep  891 | Eps 0.904 | MeanReward(10ep):  12.800 | MeanLoss(100it):  0.07297 | MeanQ:  47.685 | GradNorm:  0.1164\n",
      "[DEBUG] Step   9746 | Ep  892 | Eps 0.904 | MeanReward(10ep):  10.300 | MeanLoss(100it):  0.07288 | MeanQ:  47.704 | GradNorm:  0.4131\n",
      "[DEBUG] Step   9752 | Ep  893 | Eps 0.903 | MeanReward(10ep):   8.000 | MeanLoss(100it):  0.07298 | MeanQ:  47.659 | GradNorm:  1.1182\n",
      "[DEBUG] Step   9766 | Ep  894 | Eps 0.903 | MeanReward(10ep):   8.500 | MeanLoss(100it):  0.07308 | MeanQ:  47.588 | GradNorm:  0.3205\n",
      "[DEBUG] Step   9774 | Ep  895 | Eps 0.903 | MeanReward(10ep):   8.400 | MeanLoss(100it):  0.07286 | MeanQ:  47.607 | GradNorm:  0.4450\n",
      "[DEBUG] Step   9792 | Ep  896 | Eps 0.903 | MeanReward(10ep):   8.700 | MeanLoss(100it):  0.07186 | MeanQ:  47.682 | GradNorm:  0.4083\n",
      "[DEBUG] Step   9801 | Ep  897 | Eps 0.903 | MeanReward(10ep):   9.000 | MeanLoss(100it):  0.07181 | MeanQ:  47.601 | GradNorm:  0.6354\n",
      "[DEBUG] Step   9808 | Ep  898 | Eps 0.903 | MeanReward(10ep):   8.900 | MeanLoss(100it):  0.07168 | MeanQ:  47.504 | GradNorm:  0.6242\n",
      "[DEBUG] Step   9814 | Ep  899 | Eps 0.903 | MeanReward(10ep):   8.600 | MeanLoss(100it):  0.07181 | MeanQ:  47.480 | GradNorm:  0.4713\n",
      "[DEBUG] Step   9826 | Ep  900 | Eps 0.903 | MeanReward(10ep):   9.100 | MeanLoss(100it):  0.07181 | MeanQ:  47.528 | GradNorm:  0.4185\n",
      "[cartpole] Ep 900 Steps 9826 RecentMean 9.10 Eps 0.903\n",
      "Eval @ Ep 900 and 9826 steps => mean return 75.00\n",
      "[DEBUG] Step   9838 | Ep  901 | Eps 0.903 | MeanReward(10ep):   8.800 | MeanLoss(100it):  0.07168 | MeanQ:  47.170 | GradNorm:  0.4253\n",
      "[DEBUG] Step   9844 | Ep  902 | Eps 0.903 | MeanReward(10ep):   8.800 | MeanLoss(100it):  0.07117 | MeanQ:  47.483 | GradNorm:  0.2109\n",
      "[DEBUG] Step   9857 | Ep  903 | Eps 0.902 | MeanReward(10ep):   9.500 | MeanLoss(100it):  0.07152 | MeanQ:  47.462 | GradNorm:  0.2608\n",
      "[DEBUG] Step   9864 | Ep  904 | Eps 0.902 | MeanReward(10ep):   8.800 | MeanLoss(100it):  0.07162 | MeanQ:  47.476 | GradNorm:  0.4518\n",
      "[DEBUG] Step   9877 | Ep  905 | Eps 0.902 | MeanReward(10ep):   9.300 | MeanLoss(100it):  0.07217 | MeanQ:  47.489 | GradNorm:  0.8976\n",
      "[DEBUG] Step   9883 | Ep  906 | Eps 0.902 | MeanReward(10ep):   8.100 | MeanLoss(100it):  0.07241 | MeanQ:  47.471 | GradNorm:  0.7355\n",
      "[DEBUG] Step   9892 | Ep  907 | Eps 0.902 | MeanReward(10ep):   8.100 | MeanLoss(100it):  0.07256 | MeanQ:  47.427 | GradNorm:  0.5782\n",
      "[DEBUG] Step   9901 | Ep  908 | Eps 0.902 | MeanReward(10ep):   8.300 | MeanLoss(100it):  0.07270 | MeanQ:  47.470 | GradNorm:  0.1473\n",
      "[DEBUG] Step   9912 | Ep  909 | Eps 0.902 | MeanReward(10ep):   8.800 | MeanLoss(100it):  0.07249 | MeanQ:  47.516 | GradNorm:  0.7995\n",
      "[DEBUG] Step   9918 | Ep  910 | Eps 0.902 | MeanReward(10ep):   8.200 | MeanLoss(100it):  0.07222 | MeanQ:  47.509 | GradNorm:  0.4194\n",
      "[cartpole] Ep 910 Steps 9918 RecentMean 8.20 Eps 0.902\n",
      "[DEBUG] Step   9928 | Ep  911 | Eps 0.902 | MeanReward(10ep):   8.000 | MeanLoss(100it):  0.07197 | MeanQ:  47.461 | GradNorm:  0.5312\n",
      "[DEBUG] Step   9943 | Ep  912 | Eps 0.902 | MeanReward(10ep):   8.900 | MeanLoss(100it):  0.07169 | MeanQ:  47.491 | GradNorm:  0.6290\n",
      "[DEBUG] Step   9957 | Ep  913 | Eps 0.901 | MeanReward(10ep):   9.000 | MeanLoss(100it):  0.07126 | MeanQ:  47.569 | GradNorm:  0.3330\n",
      "[DEBUG] Step   9965 | Ep  914 | Eps 0.901 | MeanReward(10ep):   9.100 | MeanLoss(100it):  0.07078 | MeanQ:  47.564 | GradNorm:  0.2163\n",
      "[DEBUG] Step   9980 | Ep  915 | Eps 0.901 | MeanReward(10ep):   9.300 | MeanLoss(100it):  0.07016 | MeanQ:  47.510 | GradNorm:  0.6674\n",
      "[DEBUG] Step  10009 | Ep  916 | Eps 0.901 | MeanReward(10ep):  11.600 | MeanLoss(100it):  0.07000 | MeanQ:  47.505 | GradNorm:  0.2466\n",
      "[DEBUG] Step  10015 | Ep  917 | Eps 0.901 | MeanReward(10ep):  11.300 | MeanLoss(100it):  0.06993 | MeanQ:  47.481 | GradNorm:  0.1862\n",
      "[DEBUG] Step  10027 | Ep  918 | Eps 0.901 | MeanReward(10ep):  11.600 | MeanLoss(100it):  0.07040 | MeanQ:  47.445 | GradNorm:  0.7632\n",
      "[DEBUG] Step  10033 | Ep  919 | Eps 0.901 | MeanReward(10ep):  11.100 | MeanLoss(100it):  0.07052 | MeanQ:  47.500 | GradNorm:  0.2885\n",
      "[DEBUG] Step  10051 | Ep  920 | Eps 0.901 | MeanReward(10ep):  12.300 | MeanLoss(100it):  0.07081 | MeanQ:  47.554 | GradNorm:  0.1860\n",
      "[cartpole] Ep 920 Steps 10051 RecentMean 12.30 Eps 0.901\n",
      "Eval @ Ep 920 and 10051 steps => mean return 76.00\n",
      "[DEBUG] Step  10066 | Ep  921 | Eps 0.900 | MeanReward(10ep):  12.800 | MeanLoss(100it):  0.07089 | MeanQ:  47.269 | GradNorm:  0.5121\n",
      "[DEBUG] Step  10084 | Ep  922 | Eps 0.900 | MeanReward(10ep):  13.100 | MeanLoss(100it):  0.07131 | MeanQ:  47.635 | GradNorm:  0.5260\n",
      "[DEBUG] Step  10090 | Ep  923 | Eps 0.900 | MeanReward(10ep):  12.300 | MeanLoss(100it):  0.07126 | MeanQ:  47.629 | GradNorm:  0.3572\n",
      "[DEBUG] Step  10095 | Ep  924 | Eps 0.900 | MeanReward(10ep):  12.000 | MeanLoss(100it):  0.07150 | MeanQ:  47.641 | GradNorm:  0.7081\n",
      "[DEBUG] Step  10100 | Ep  925 | Eps 0.900 | MeanReward(10ep):  11.000 | MeanLoss(100it):  0.07167 | MeanQ:  47.636 | GradNorm:  0.2251\n",
      "[DEBUG] Step  10124 | Ep  926 | Eps 0.900 | MeanReward(10ep):  10.500 | MeanLoss(100it):  0.07224 | MeanQ:  47.539 | GradNorm:  0.1852\n",
      "[DEBUG] Step  10136 | Ep  927 | Eps 0.900 | MeanReward(10ep):  11.100 | MeanLoss(100it):  0.07215 | MeanQ:  47.493 | GradNorm:  0.2571\n",
      "[DEBUG] Step  10148 | Ep  928 | Eps 0.900 | MeanReward(10ep):  11.100 | MeanLoss(100it):  0.07243 | MeanQ:  47.456 | GradNorm:  0.2001\n",
      "[DEBUG] Step  10165 | Ep  929 | Eps 0.899 | MeanReward(10ep):  12.200 | MeanLoss(100it):  0.07238 | MeanQ:  47.480 | GradNorm:  0.4732\n",
      "[DEBUG] Step  10171 | Ep  930 | Eps 0.899 | MeanReward(10ep):  11.000 | MeanLoss(100it):  0.07247 | MeanQ:  47.476 | GradNorm:  0.3279\n",
      "[cartpole] Ep 930 Steps 10171 RecentMean 11.00 Eps 0.899\n",
      "[DEBUG] Step  10178 | Ep  931 | Eps 0.899 | MeanReward(10ep):  10.200 | MeanLoss(100it):  0.07271 | MeanQ:  47.447 | GradNorm:  0.1692\n",
      "[DEBUG] Step  10195 | Ep  932 | Eps 0.899 | MeanReward(10ep):  10.100 | MeanLoss(100it):  0.07366 | MeanQ:  47.549 | GradNorm:  0.3773\n",
      "[DEBUG] Step  10201 | Ep  933 | Eps 0.899 | MeanReward(10ep):  10.100 | MeanLoss(100it):  0.07379 | MeanQ:  47.611 | GradNorm:  0.3184\n",
      "[DEBUG] Step  10208 | Ep  934 | Eps 0.899 | MeanReward(10ep):  10.300 | MeanLoss(100it):  0.07380 | MeanQ:  47.655 | GradNorm:  0.5192\n",
      "[DEBUG] Step  10215 | Ep  935 | Eps 0.899 | MeanReward(10ep):  10.500 | MeanLoss(100it):  0.07374 | MeanQ:  47.661 | GradNorm:  0.3743\n",
      "[DEBUG] Step  10221 | Ep  936 | Eps 0.899 | MeanReward(10ep):   8.700 | MeanLoss(100it):  0.07321 | MeanQ:  47.643 | GradNorm:  0.5250\n",
      "[DEBUG] Step  10228 | Ep  937 | Eps 0.899 | MeanReward(10ep):   8.200 | MeanLoss(100it):  0.07315 | MeanQ:  47.600 | GradNorm:  0.4890\n",
      "[DEBUG] Step  10238 | Ep  938 | Eps 0.899 | MeanReward(10ep):   8.000 | MeanLoss(100it):  0.07310 | MeanQ:  47.533 | GradNorm:  0.1954\n",
      "[DEBUG] Step  10266 | Ep  939 | Eps 0.898 | MeanReward(10ep):   9.100 | MeanLoss(100it):  0.07221 | MeanQ:  47.661 | GradNorm:  0.6846\n",
      "[DEBUG] Step  10272 | Ep  940 | Eps 0.898 | MeanReward(10ep):   9.100 | MeanLoss(100it):  0.07184 | MeanQ:  47.572 | GradNorm:  0.6907\n",
      "[cartpole] Ep 940 Steps 10272 RecentMean 9.10 Eps 0.898\n",
      "Eval @ Ep 940 and 10272 steps => mean return 41.00\n",
      "[DEBUG] Step  10282 | Ep  941 | Eps 0.898 | MeanReward(10ep):   9.400 | MeanLoss(100it):  0.07156 | MeanQ:  47.145 | GradNorm:  0.4282\n",
      "[DEBUG] Step  10293 | Ep  942 | Eps 0.898 | MeanReward(10ep):   8.800 | MeanLoss(100it):  0.07183 | MeanQ:  47.530 | GradNorm:  0.7911\n",
      "[DEBUG] Step  10314 | Ep  943 | Eps 0.898 | MeanReward(10ep):  10.300 | MeanLoss(100it):  0.07202 | MeanQ:  47.796 | GradNorm:  1.1564\n",
      "[DEBUG] Step  10333 | Ep  944 | Eps 0.898 | MeanReward(10ep):  11.500 | MeanLoss(100it):  0.07250 | MeanQ:  47.603 | GradNorm:  0.9443\n",
      "[DEBUG] Step  10339 | Ep  945 | Eps 0.898 | MeanReward(10ep):  11.400 | MeanLoss(100it):  0.07256 | MeanQ:  47.625 | GradNorm:  0.8920\n",
      "[DEBUG] Step  10350 | Ep  946 | Eps 0.898 | MeanReward(10ep):  11.900 | MeanLoss(100it):  0.07245 | MeanQ:  47.848 | GradNorm:  0.9641\n",
      "[DEBUG] Step  10359 | Ep  947 | Eps 0.897 | MeanReward(10ep):  12.100 | MeanLoss(100it):  0.07245 | MeanQ:  47.931 | GradNorm:  1.2072\n",
      "[DEBUG] Step  10367 | Ep  948 | Eps 0.897 | MeanReward(10ep):  11.900 | MeanLoss(100it):  0.07234 | MeanQ:  47.882 | GradNorm:  0.5305\n",
      "[DEBUG] Step  10377 | Ep  949 | Eps 0.897 | MeanReward(10ep):  10.100 | MeanLoss(100it):  0.07207 | MeanQ:  47.774 | GradNorm:  0.6980\n",
      "[DEBUG] Step  10384 | Ep  950 | Eps 0.897 | MeanReward(10ep):  10.200 | MeanLoss(100it):  0.07194 | MeanQ:  47.764 | GradNorm:  0.8611\n",
      "[cartpole] Ep 950 Steps 10384 RecentMean 10.20 Eps 0.897\n",
      "[DEBUG] Step  10400 | Ep  951 | Eps 0.897 | MeanReward(10ep):  10.800 | MeanLoss(100it):  0.07162 | MeanQ:  47.933 | GradNorm:  0.3806\n",
      "[DEBUG] Step  10415 | Ep  952 | Eps 0.897 | MeanReward(10ep):  11.200 | MeanLoss(100it):  0.07206 | MeanQ:  47.896 | GradNorm:  0.7332\n",
      "[DEBUG] Step  10422 | Ep  953 | Eps 0.897 | MeanReward(10ep):   9.800 | MeanLoss(100it):  0.07190 | MeanQ:  47.863 | GradNorm:  0.3150\n",
      "[DEBUG] Step  10448 | Ep  954 | Eps 0.897 | MeanReward(10ep):  10.500 | MeanLoss(100it):  0.07136 | MeanQ:  47.849 | GradNorm:  0.3950\n",
      "[DEBUG] Step  10456 | Ep  955 | Eps 0.896 | MeanReward(10ep):  10.700 | MeanLoss(100it):  0.07160 | MeanQ:  47.878 | GradNorm:  0.4847\n",
      "[DEBUG] Step  10465 | Ep  956 | Eps 0.896 | MeanReward(10ep):  10.500 | MeanLoss(100it):  0.07156 | MeanQ:  47.939 | GradNorm:  0.5637\n",
      "[DEBUG] Step  10471 | Ep  957 | Eps 0.896 | MeanReward(10ep):  10.200 | MeanLoss(100it):  0.07163 | MeanQ:  47.960 | GradNorm:  0.3463\n",
      "[DEBUG] Step  10490 | Ep  958 | Eps 0.896 | MeanReward(10ep):  11.300 | MeanLoss(100it):  0.07140 | MeanQ:  48.009 | GradNorm:  0.3225\n",
      "[DEBUG] Step  10508 | Ep  959 | Eps 0.896 | MeanReward(10ep):  12.100 | MeanLoss(100it):  0.07083 | MeanQ:  47.962 | GradNorm:  0.3640\n",
      "[DEBUG] Step  10521 | Ep  960 | Eps 0.896 | MeanReward(10ep):  12.700 | MeanLoss(100it):  0.07066 | MeanQ:  47.941 | GradNorm:  0.4335\n",
      "[cartpole] Ep 960 Steps 10521 RecentMean 12.70 Eps 0.896\n",
      "Eval @ Ep 960 and 10521 steps => mean return 87.00\n",
      "[DEBUG] Step  10534 | Ep  961 | Eps 0.896 | MeanReward(10ep):  12.400 | MeanLoss(100it):  0.07104 | MeanQ:  47.566 | GradNorm:  0.2933\n",
      "[DEBUG] Step  10544 | Ep  962 | Eps 0.896 | MeanReward(10ep):  11.900 | MeanLoss(100it):  0.07065 | MeanQ:  47.865 | GradNorm:  0.2977\n",
      "[DEBUG] Step  10554 | Ep  963 | Eps 0.896 | MeanReward(10ep):  12.200 | MeanLoss(100it):  0.07057 | MeanQ:  47.844 | GradNorm:  0.5183\n",
      "[DEBUG] Step  10562 | Ep  964 | Eps 0.895 | MeanReward(10ep):  10.400 | MeanLoss(100it):  0.07016 | MeanQ:  47.853 | GradNorm:  0.3976\n",
      "[DEBUG] Step  10576 | Ep  965 | Eps 0.895 | MeanReward(10ep):  11.000 | MeanLoss(100it):  0.07019 | MeanQ:  47.909 | GradNorm:  0.7375\n",
      "[DEBUG] Step  10584 | Ep  966 | Eps 0.895 | MeanReward(10ep):  10.900 | MeanLoss(100it):  0.06965 | MeanQ:  47.869 | GradNorm:  0.5093\n",
      "[DEBUG] Step  10596 | Ep  967 | Eps 0.895 | MeanReward(10ep):  11.500 | MeanLoss(100it):  0.06950 | MeanQ:  47.795 | GradNorm:  0.3114\n",
      "[DEBUG] Step  10607 | Ep  968 | Eps 0.895 | MeanReward(10ep):  10.700 | MeanLoss(100it):  0.06925 | MeanQ:  47.786 | GradNorm:  0.2562\n",
      "[DEBUG] Step  10617 | Ep  969 | Eps 0.895 | MeanReward(10ep):   9.900 | MeanLoss(100it):  0.06904 | MeanQ:  47.781 | GradNorm:  0.3186\n",
      "[DEBUG] Step  10623 | Ep  970 | Eps 0.895 | MeanReward(10ep):   9.200 | MeanLoss(100it):  0.06930 | MeanQ:  47.778 | GradNorm:  0.1543\n",
      "[cartpole] Ep 970 Steps 10623 RecentMean 9.20 Eps 0.895\n",
      "[DEBUG] Step  10634 | Ep  971 | Eps 0.895 | MeanReward(10ep):   9.000 | MeanLoss(100it):  0.06916 | MeanQ:  47.740 | GradNorm:  0.4917\n",
      "[DEBUG] Step  10660 | Ep  972 | Eps 0.894 | MeanReward(10ep):  10.600 | MeanLoss(100it):  0.06923 | MeanQ:  47.797 | GradNorm:  0.8424\n",
      "[DEBUG] Step  10665 | Ep  973 | Eps 0.894 | MeanReward(10ep):  10.100 | MeanLoss(100it):  0.06929 | MeanQ:  47.790 | GradNorm:  0.4513\n",
      "[DEBUG] Step  10703 | Ep  974 | Eps 0.894 | MeanReward(10ep):  13.100 | MeanLoss(100it):  0.06795 | MeanQ:  47.801 | GradNorm:  0.8739\n",
      "[DEBUG] Step  10711 | Ep  975 | Eps 0.894 | MeanReward(10ep):  12.500 | MeanLoss(100it):  0.06790 | MeanQ:  47.795 | GradNorm:  0.4173\n",
      "[DEBUG] Step  10729 | Ep  976 | Eps 0.894 | MeanReward(10ep):  13.500 | MeanLoss(100it):  0.06741 | MeanQ:  47.826 | GradNorm:  0.3170\n",
      "[DEBUG] Step  10736 | Ep  977 | Eps 0.894 | MeanReward(10ep):  13.000 | MeanLoss(100it):  0.06705 | MeanQ:  47.854 | GradNorm:  0.4587\n",
      "[DEBUG] Step  10745 | Ep  978 | Eps 0.894 | MeanReward(10ep):  12.800 | MeanLoss(100it):  0.06665 | MeanQ:  47.875 | GradNorm:  0.3444\n",
      "[DEBUG] Step  10754 | Ep  979 | Eps 0.894 | MeanReward(10ep):  12.700 | MeanLoss(100it):  0.06655 | MeanQ:  47.844 | GradNorm:  0.6516\n",
      "[DEBUG] Step  10770 | Ep  980 | Eps 0.893 | MeanReward(10ep):  13.700 | MeanLoss(100it):  0.06624 | MeanQ:  47.880 | GradNorm:  0.8668\n",
      "[cartpole] Ep 980 Steps 10770 RecentMean 13.70 Eps 0.893\n",
      "Eval @ Ep 980 and 10770 steps => mean return 128.00\n",
      "[DEBUG] Step  10787 | Ep  981 | Eps 0.893 | MeanReward(10ep):  14.300 | MeanLoss(100it):  0.06594 | MeanQ:  47.618 | GradNorm:  0.5864\n",
      "[DEBUG] Step  10818 | Ep  982 | Eps 0.893 | MeanReward(10ep):  14.800 | MeanLoss(100it):  0.06647 | MeanQ:  47.759 | GradNorm:  0.6392\n",
      "[DEBUG] Step  10825 | Ep  983 | Eps 0.893 | MeanReward(10ep):  15.000 | MeanLoss(100it):  0.06694 | MeanQ:  47.732 | GradNorm:  0.4079\n",
      "[DEBUG] Step  10842 | Ep  984 | Eps 0.893 | MeanReward(10ep):  12.900 | MeanLoss(100it):  0.06788 | MeanQ:  47.681 | GradNorm:  0.6108\n",
      "[DEBUG] Step  10850 | Ep  985 | Eps 0.893 | MeanReward(10ep):  12.900 | MeanLoss(100it):  0.06824 | MeanQ:  47.631 | GradNorm:  0.2096\n",
      "[DEBUG] Step  10861 | Ep  986 | Eps 0.892 | MeanReward(10ep):  12.200 | MeanLoss(100it):  0.06860 | MeanQ:  47.658 | GradNorm:  0.4042\n",
      "[DEBUG] Step  10870 | Ep  987 | Eps 0.892 | MeanReward(10ep):  12.400 | MeanLoss(100it):  0.06866 | MeanQ:  47.693 | GradNorm:  0.2917\n",
      "[DEBUG] Step  10890 | Ep  988 | Eps 0.892 | MeanReward(10ep):  13.500 | MeanLoss(100it):  0.06911 | MeanQ:  47.746 | GradNorm:  0.3317\n",
      "[DEBUG] Step  10898 | Ep  989 | Eps 0.892 | MeanReward(10ep):  13.400 | MeanLoss(100it):  0.06937 | MeanQ:  47.732 | GradNorm:  0.4422\n",
      "[DEBUG] Step  10907 | Ep  990 | Eps 0.892 | MeanReward(10ep):  12.700 | MeanLoss(100it):  0.06940 | MeanQ:  47.687 | GradNorm:  0.3601\n",
      "[cartpole] Ep 990 Steps 10907 RecentMean 12.70 Eps 0.892\n",
      "[DEBUG] Step  10921 | Ep  991 | Eps 0.892 | MeanReward(10ep):  12.400 | MeanLoss(100it):  0.06915 | MeanQ:  47.728 | GradNorm:  0.2901\n",
      "[DEBUG] Step  10927 | Ep  992 | Eps 0.892 | MeanReward(10ep):   9.900 | MeanLoss(100it):  0.06915 | MeanQ:  47.736 | GradNorm:  0.5106\n",
      "[DEBUG] Step  10942 | Ep  993 | Eps 0.892 | MeanReward(10ep):  10.700 | MeanLoss(100it):  0.06863 | MeanQ:  47.724 | GradNorm:  0.6415\n",
      "[DEBUG] Step  10949 | Ep  994 | Eps 0.892 | MeanReward(10ep):   9.700 | MeanLoss(100it):  0.06846 | MeanQ:  47.675 | GradNorm:  0.3255\n",
      "[DEBUG] Step  10961 | Ep  995 | Eps 0.891 | MeanReward(10ep):  10.100 | MeanLoss(100it):  0.06855 | MeanQ:  47.619 | GradNorm:  0.3001\n",
      "[DEBUG] Step  10980 | Ep  996 | Eps 0.891 | MeanReward(10ep):  10.900 | MeanLoss(100it):  0.06785 | MeanQ:  47.724 | GradNorm:  0.3020\n",
      "[DEBUG] Step  11012 | Ep  997 | Eps 0.891 | MeanReward(10ep):  13.200 | MeanLoss(100it):  0.06808 | MeanQ:  47.549 | GradNorm:  0.7386\n",
      "[DEBUG] Step  11025 | Ep  998 | Eps 0.891 | MeanReward(10ep):  12.500 | MeanLoss(100it):  0.06836 | MeanQ:  47.581 | GradNorm:  0.2768\n",
      "[DEBUG] Step  11031 | Ep  999 | Eps 0.891 | MeanReward(10ep):  12.300 | MeanLoss(100it):  0.06837 | MeanQ:  47.578 | GradNorm:  0.3754\n",
      "[DEBUG] Step  11040 | Ep 1000 | Eps 0.891 | MeanReward(10ep):  12.300 | MeanLoss(100it):  0.06881 | MeanQ:  47.545 | GradNorm:  0.2870\n",
      "[cartpole] Ep 1000 Steps 11040 RecentMean 12.30 Eps 0.891\n",
      "Eval @ Ep 1000 and 11040 steps => mean return 123.00\n",
      "[DEBUG] Step  11066 | Ep 1001 | Eps 0.890 | MeanReward(10ep):  13.500 | MeanLoss(100it):  0.06883 | MeanQ:  47.220 | GradNorm:  0.2600\n",
      "[DEBUG] Step  11082 | Ep 1002 | Eps 0.890 | MeanReward(10ep):  14.500 | MeanLoss(100it):  0.06849 | MeanQ:  47.599 | GradNorm:  0.1724\n",
      "[DEBUG] Step  11092 | Ep 1003 | Eps 0.890 | MeanReward(10ep):  14.000 | MeanLoss(100it):  0.06858 | MeanQ:  47.537 | GradNorm:  0.4383\n",
      "[DEBUG] Step  11127 | Ep 1004 | Eps 0.890 | MeanReward(10ep):  16.800 | MeanLoss(100it):  0.06939 | MeanQ:  47.518 | GradNorm:  0.9516\n",
      "[DEBUG] Step  11133 | Ep 1005 | Eps 0.890 | MeanReward(10ep):  16.200 | MeanLoss(100it):  0.06939 | MeanQ:  47.458 | GradNorm:  0.4207\n",
      "[DEBUG] Step  11145 | Ep 1006 | Eps 0.890 | MeanReward(10ep):  15.500 | MeanLoss(100it):  0.06983 | MeanQ:  47.471 | GradNorm:  0.7477\n",
      "[DEBUG] Step  11157 | Ep 1007 | Eps 0.890 | MeanReward(10ep):  13.500 | MeanLoss(100it):  0.07035 | MeanQ:  47.559 | GradNorm:  0.4026\n",
      "[DEBUG] Step  11165 | Ep 1008 | Eps 0.889 | MeanReward(10ep):  13.000 | MeanLoss(100it):  0.07048 | MeanQ:  47.540 | GradNorm:  0.7617\n",
      "[DEBUG] Step  11202 | Ep 1009 | Eps 0.889 | MeanReward(10ep):  16.100 | MeanLoss(100it):  0.07120 | MeanQ:  47.601 | GradNorm:  0.3793\n",
      "[DEBUG] Step  11210 | Ep 1010 | Eps 0.889 | MeanReward(10ep):  16.000 | MeanLoss(100it):  0.07077 | MeanQ:  47.695 | GradNorm:  0.5497\n",
      "[cartpole] Ep 1010 Steps 11210 RecentMean 16.00 Eps 0.889\n",
      "[DEBUG] Step  11224 | Ep 1011 | Eps 0.889 | MeanReward(10ep):  14.800 | MeanLoss(100it):  0.07008 | MeanQ:  47.614 | GradNorm:  0.4069\n",
      "[DEBUG] Step  11232 | Ep 1012 | Eps 0.889 | MeanReward(10ep):  14.000 | MeanLoss(100it):  0.06961 | MeanQ:  47.581 | GradNorm:  0.2723\n",
      "[DEBUG] Step  11240 | Ep 1013 | Eps 0.889 | MeanReward(10ep):  13.800 | MeanLoss(100it):  0.06912 | MeanQ:  47.644 | GradNorm:  0.6057\n",
      "[DEBUG] Step  11251 | Ep 1014 | Eps 0.889 | MeanReward(10ep):  11.400 | MeanLoss(100it):  0.06862 | MeanQ:  47.732 | GradNorm:  0.4236\n",
      "[DEBUG] Step  11259 | Ep 1015 | Eps 0.889 | MeanReward(10ep):  11.600 | MeanLoss(100it):  0.06795 | MeanQ:  47.763 | GradNorm:  0.8683\n",
      "[DEBUG] Step  11267 | Ep 1016 | Eps 0.888 | MeanReward(10ep):  11.200 | MeanLoss(100it):  0.06776 | MeanQ:  47.701 | GradNorm:  0.1350\n",
      "[DEBUG] Step  11277 | Ep 1017 | Eps 0.888 | MeanReward(10ep):  11.000 | MeanLoss(100it):  0.06715 | MeanQ:  47.681 | GradNorm:  0.6343\n",
      "[DEBUG] Step  11283 | Ep 1018 | Eps 0.888 | MeanReward(10ep):  10.800 | MeanLoss(100it):  0.06715 | MeanQ:  47.702 | GradNorm:  0.5064\n",
      "[DEBUG] Step  11289 | Ep 1019 | Eps 0.888 | MeanReward(10ep):   7.700 | MeanLoss(100it):  0.06711 | MeanQ:  47.720 | GradNorm:  0.5098\n",
      "[DEBUG] Step  11318 | Ep 1020 | Eps 0.888 | MeanReward(10ep):   9.800 | MeanLoss(100it):  0.06678 | MeanQ:  47.621 | GradNorm:  0.1823\n",
      "[cartpole] Ep 1020 Steps 11318 RecentMean 9.80 Eps 0.888\n",
      "Eval @ Ep 1020 and 11318 steps => mean return 93.00\n",
      "[DEBUG] Step  11326 | Ep 1021 | Eps 0.888 | MeanReward(10ep):   9.200 | MeanLoss(100it):  0.06714 | MeanQ:  47.289 | GradNorm:  0.6114\n",
      "[DEBUG] Step  11343 | Ep 1022 | Eps 0.888 | MeanReward(10ep):  10.100 | MeanLoss(100it):  0.06753 | MeanQ:  47.659 | GradNorm:  0.8897\n",
      "[DEBUG] Step  11352 | Ep 1023 | Eps 0.888 | MeanReward(10ep):  10.200 | MeanLoss(100it):  0.06813 | MeanQ:  47.609 | GradNorm:  0.5230\n",
      "[DEBUG] Step  11370 | Ep 1024 | Eps 0.887 | MeanReward(10ep):  10.900 | MeanLoss(100it):  0.06822 | MeanQ:  47.628 | GradNorm:  0.2210\n",
      "[DEBUG] Step  11390 | Ep 1025 | Eps 0.887 | MeanReward(10ep):  12.100 | MeanLoss(100it):  0.06875 | MeanQ:  47.654 | GradNorm:  0.5340\n",
      "[DEBUG] Step  11397 | Ep 1026 | Eps 0.887 | MeanReward(10ep):  12.000 | MeanLoss(100it):  0.06834 | MeanQ:  47.635 | GradNorm:  0.4006\n",
      "[DEBUG] Step  11407 | Ep 1027 | Eps 0.887 | MeanReward(10ep):  12.000 | MeanLoss(100it):  0.06845 | MeanQ:  47.641 | GradNorm:  0.0892\n",
      "[DEBUG] Step  11417 | Ep 1028 | Eps 0.887 | MeanReward(10ep):  12.400 | MeanLoss(100it):  0.06841 | MeanQ:  47.667 | GradNorm:  0.4197\n",
      "[DEBUG] Step  11425 | Ep 1029 | Eps 0.887 | MeanReward(10ep):  12.600 | MeanLoss(100it):  0.06827 | MeanQ:  47.625 | GradNorm:  0.1695\n",
      "[DEBUG] Step  11438 | Ep 1030 | Eps 0.887 | MeanReward(10ep):  11.000 | MeanLoss(100it):  0.06813 | MeanQ:  47.580 | GradNorm:  0.2639\n",
      "[cartpole] Ep 1030 Steps 11438 RecentMean 11.00 Eps 0.887\n",
      "[DEBUG] Step  11446 | Ep 1031 | Eps 0.887 | MeanReward(10ep):  11.000 | MeanLoss(100it):  0.06796 | MeanQ:  47.583 | GradNorm:  0.4873\n",
      "[DEBUG] Step  11455 | Ep 1032 | Eps 0.887 | MeanReward(10ep):  10.200 | MeanLoss(100it):  0.06793 | MeanQ:  47.628 | GradNorm:  0.3097\n",
      "[DEBUG] Step  11464 | Ep 1033 | Eps 0.887 | MeanReward(10ep):  10.200 | MeanLoss(100it):  0.06790 | MeanQ:  47.606 | GradNorm:  0.9561\n",
      "[DEBUG] Step  11473 | Ep 1034 | Eps 0.886 | MeanReward(10ep):   9.300 | MeanLoss(100it):  0.06821 | MeanQ:  47.500 | GradNorm:  0.2343\n",
      "[DEBUG] Step  11484 | Ep 1035 | Eps 0.886 | MeanReward(10ep):   8.400 | MeanLoss(100it):  0.06850 | MeanQ:  47.506 | GradNorm:  0.6287\n",
      "[DEBUG] Step  11497 | Ep 1036 | Eps 0.886 | MeanReward(10ep):   9.000 | MeanLoss(100it):  0.06898 | MeanQ:  47.622 | GradNorm:  0.3013\n",
      "[DEBUG] Step  11502 | Ep 1037 | Eps 0.886 | MeanReward(10ep):   8.500 | MeanLoss(100it):  0.06898 | MeanQ:  47.650 | GradNorm:  0.1746\n",
      "[DEBUG] Step  11513 | Ep 1038 | Eps 0.886 | MeanReward(10ep):   8.600 | MeanLoss(100it):  0.06880 | MeanQ:  47.708 | GradNorm:  0.4523\n",
      "[DEBUG] Step  11530 | Ep 1039 | Eps 0.886 | MeanReward(10ep):   9.500 | MeanLoss(100it):  0.06790 | MeanQ:  47.680 | GradNorm:  0.5959\n",
      "[DEBUG] Step  11540 | Ep 1040 | Eps 0.886 | MeanReward(10ep):   9.200 | MeanLoss(100it):  0.06785 | MeanQ:  47.710 | GradNorm:  0.7656\n",
      "[cartpole] Ep 1040 Steps 11540 RecentMean 9.20 Eps 0.886\n",
      "Eval @ Ep 1040 and 11540 steps => mean return 106.00\n",
      "[DEBUG] Step  11550 | Ep 1041 | Eps 0.886 | MeanReward(10ep):   9.400 | MeanLoss(100it):  0.06740 | MeanQ:  47.430 | GradNorm:  0.1507\n",
      "[DEBUG] Step  11566 | Ep 1042 | Eps 0.886 | MeanReward(10ep):  10.100 | MeanLoss(100it):  0.06682 | MeanQ:  47.809 | GradNorm:  0.2675\n",
      "[DEBUG] Step  11581 | Ep 1043 | Eps 0.885 | MeanReward(10ep):  10.700 | MeanLoss(100it):  0.06673 | MeanQ:  47.772 | GradNorm:  0.3582\n",
      "[DEBUG] Step  11595 | Ep 1044 | Eps 0.885 | MeanReward(10ep):  11.200 | MeanLoss(100it):  0.06637 | MeanQ:  47.776 | GradNorm:  0.7099\n",
      "[DEBUG] Step  11603 | Ep 1045 | Eps 0.885 | MeanReward(10ep):  10.900 | MeanLoss(100it):  0.06608 | MeanQ:  47.783 | GradNorm:  0.3275\n",
      "[DEBUG] Step  11618 | Ep 1046 | Eps 0.885 | MeanReward(10ep):  11.100 | MeanLoss(100it):  0.06650 | MeanQ:  47.774 | GradNorm:  0.2863\n",
      "[DEBUG] Step  11630 | Ep 1047 | Eps 0.885 | MeanReward(10ep):  11.800 | MeanLoss(100it):  0.06671 | MeanQ:  47.737 | GradNorm:  0.1558\n",
      "[DEBUG] Step  11640 | Ep 1048 | Eps 0.885 | MeanReward(10ep):  11.700 | MeanLoss(100it):  0.06702 | MeanQ:  47.707 | GradNorm:  0.1579\n",
      "[DEBUG] Step  11646 | Ep 1049 | Eps 0.885 | MeanReward(10ep):  10.600 | MeanLoss(100it):  0.06711 | MeanQ:  47.702 | GradNorm:  0.1504\n",
      "[DEBUG] Step  11659 | Ep 1050 | Eps 0.885 | MeanReward(10ep):  10.900 | MeanLoss(100it):  0.06741 | MeanQ:  47.666 | GradNorm:  0.3196\n",
      "[cartpole] Ep 1050 Steps 11659 RecentMean 10.90 Eps 0.885\n",
      "[DEBUG] Step  11672 | Ep 1051 | Eps 0.884 | MeanReward(10ep):  11.200 | MeanLoss(100it):  0.06779 | MeanQ:  47.699 | GradNorm:  0.2313\n",
      "[DEBUG] Step  11678 | Ep 1052 | Eps 0.884 | MeanReward(10ep):  10.200 | MeanLoss(100it):  0.06782 | MeanQ:  47.717 | GradNorm:  0.4565\n",
      "[DEBUG] Step  11692 | Ep 1053 | Eps 0.884 | MeanReward(10ep):  10.100 | MeanLoss(100it):  0.06773 | MeanQ:  47.728 | GradNorm:  0.2936\n",
      "[DEBUG] Step  11700 | Ep 1054 | Eps 0.884 | MeanReward(10ep):   9.500 | MeanLoss(100it):  0.06781 | MeanQ:  47.716 | GradNorm:  0.4731\n",
      "[DEBUG] Step  11707 | Ep 1055 | Eps 0.884 | MeanReward(10ep):   9.400 | MeanLoss(100it):  0.06791 | MeanQ:  47.709 | GradNorm:  0.3011\n",
      "[DEBUG] Step  11725 | Ep 1056 | Eps 0.884 | MeanReward(10ep):   9.700 | MeanLoss(100it):  0.06751 | MeanQ:  47.735 | GradNorm:  0.2956\n",
      "[DEBUG] Step  11731 | Ep 1057 | Eps 0.884 | MeanReward(10ep):   9.100 | MeanLoss(100it):  0.06724 | MeanQ:  47.755 | GradNorm:  0.0786\n",
      "[DEBUG] Step  11741 | Ep 1058 | Eps 0.884 | MeanReward(10ep):   9.100 | MeanLoss(100it):  0.06693 | MeanQ:  47.766 | GradNorm:  0.0739\n",
      "[DEBUG] Step  11754 | Ep 1059 | Eps 0.884 | MeanReward(10ep):   9.800 | MeanLoss(100it):  0.06686 | MeanQ:  47.736 | GradNorm:  0.3930\n",
      "[DEBUG] Step  11770 | Ep 1060 | Eps 0.883 | MeanReward(10ep):  10.100 | MeanLoss(100it):  0.06669 | MeanQ:  47.637 | GradNorm:  0.5161\n",
      "[cartpole] Ep 1060 Steps 11770 RecentMean 10.10 Eps 0.883\n",
      "Eval @ Ep 1060 and 11770 steps => mean return 313.00\n",
      "[DEBUG] Step  11778 | Ep 1061 | Eps 0.883 | MeanReward(10ep):   9.600 | MeanLoss(100it):  0.06648 | MeanQ:  47.905 | GradNorm:  0.4264\n",
      "[DEBUG] Step  11784 | Ep 1062 | Eps 0.883 | MeanReward(10ep):   9.600 | MeanLoss(100it):  0.06629 | MeanQ:  47.715 | GradNorm:  0.4861\n",
      "[DEBUG] Step  11793 | Ep 1063 | Eps 0.883 | MeanReward(10ep):   9.100 | MeanLoss(100it):  0.06598 | MeanQ:  47.740 | GradNorm:  0.1103\n",
      "[DEBUG] Step  11810 | Ep 1064 | Eps 0.883 | MeanReward(10ep):  10.000 | MeanLoss(100it):  0.06537 | MeanQ:  47.686 | GradNorm:  0.2564\n",
      "[DEBUG] Step  11833 | Ep 1065 | Eps 0.883 | MeanReward(10ep):  11.600 | MeanLoss(100it):  0.06566 | MeanQ:  47.648 | GradNorm:  0.5690\n",
      "[DEBUG] Step  11839 | Ep 1066 | Eps 0.883 | MeanReward(10ep):  10.400 | MeanLoss(100it):  0.06569 | MeanQ:  47.627 | GradNorm:  0.3662\n",
      "[DEBUG] Step  11846 | Ep 1067 | Eps 0.883 | MeanReward(10ep):  10.500 | MeanLoss(100it):  0.06581 | MeanQ:  47.624 | GradNorm:  0.1656\n",
      "[DEBUG] Step  11872 | Ep 1068 | Eps 0.882 | MeanReward(10ep):  12.100 | MeanLoss(100it):  0.06530 | MeanQ:  47.637 | GradNorm:  0.6476\n",
      "[DEBUG] Step  11885 | Ep 1069 | Eps 0.882 | MeanReward(10ep):  12.100 | MeanLoss(100it):  0.06472 | MeanQ:  47.667 | GradNorm:  0.3240\n",
      "[DEBUG] Step  11893 | Ep 1070 | Eps 0.882 | MeanReward(10ep):  11.300 | MeanLoss(100it):  0.06451 | MeanQ:  47.667 | GradNorm:  0.6404\n",
      "[cartpole] Ep 1070 Steps 11893 RecentMean 11.30 Eps 0.882\n",
      "[DEBUG] Step  11901 | Ep 1071 | Eps 0.882 | MeanReward(10ep):  11.300 | MeanLoss(100it):  0.06442 | MeanQ:  47.656 | GradNorm:  0.2871\n",
      "[DEBUG] Step  11913 | Ep 1072 | Eps 0.882 | MeanReward(10ep):  11.900 | MeanLoss(100it):  0.06506 | MeanQ:  47.614 | GradNorm:  0.7360\n",
      "[DEBUG] Step  11931 | Ep 1073 | Eps 0.882 | MeanReward(10ep):  12.800 | MeanLoss(100it):  0.06613 | MeanQ:  47.635 | GradNorm:  0.5783\n",
      "[DEBUG] Step  11937 | Ep 1074 | Eps 0.882 | MeanReward(10ep):  11.700 | MeanLoss(100it):  0.06669 | MeanQ:  47.637 | GradNorm:  0.9724\n",
      "[DEBUG] Step  11953 | Ep 1075 | Eps 0.882 | MeanReward(10ep):  11.000 | MeanLoss(100it):  0.06746 | MeanQ:  47.532 | GradNorm:  0.3574\n",
      "[DEBUG] Step  11964 | Ep 1076 | Eps 0.882 | MeanReward(10ep):  11.500 | MeanLoss(100it):  0.06820 | MeanQ:  47.614 | GradNorm:  0.7728\n",
      "[DEBUG] Step  11988 | Ep 1077 | Eps 0.881 | MeanReward(10ep):  13.200 | MeanLoss(100it):  0.06880 | MeanQ:  47.603 | GradNorm:  1.0828\n",
      "[DEBUG] Step  11994 | Ep 1078 | Eps 0.881 | MeanReward(10ep):  11.200 | MeanLoss(100it):  0.06871 | MeanQ:  47.540 | GradNorm:  0.3267\n",
      "[DEBUG] Step  12003 | Ep 1079 | Eps 0.881 | MeanReward(10ep):  10.800 | MeanLoss(100it):  0.06924 | MeanQ:  47.484 | GradNorm:  0.8467\n",
      "[DEBUG] Step  12008 | Ep 1080 | Eps 0.881 | MeanReward(10ep):  10.500 | MeanLoss(100it):  0.06971 | MeanQ:  47.438 | GradNorm:  0.5628\n",
      "[cartpole] Ep 1080 Steps 12008 RecentMean 10.50 Eps 0.881\n",
      "Eval @ Ep 1080 and 12008 steps => mean return 147.00\n",
      "[DEBUG] Step  12023 | Ep 1081 | Eps 0.881 | MeanReward(10ep):  11.200 | MeanLoss(100it):  0.07064 | MeanQ:  47.055 | GradNorm:  0.7456\n",
      "[DEBUG] Step  12035 | Ep 1082 | Eps 0.881 | MeanReward(10ep):  11.200 | MeanLoss(100it):  0.07125 | MeanQ:  47.309 | GradNorm:  0.3642\n",
      "[DEBUG] Step  12056 | Ep 1083 | Eps 0.881 | MeanReward(10ep):  11.500 | MeanLoss(100it):  0.07247 | MeanQ:  47.246 | GradNorm:  0.4843\n",
      "[DEBUG] Step  12073 | Ep 1084 | Eps 0.880 | MeanReward(10ep):  12.600 | MeanLoss(100it):  0.07333 | MeanQ:  47.219 | GradNorm:  0.3092\n",
      "[DEBUG] Step  12078 | Ep 1085 | Eps 0.880 | MeanReward(10ep):  11.500 | MeanLoss(100it):  0.07370 | MeanQ:  47.228 | GradNorm:  0.7633\n",
      "[DEBUG] Step  12102 | Ep 1086 | Eps 0.880 | MeanReward(10ep):  12.800 | MeanLoss(100it):  0.07518 | MeanQ:  47.355 | GradNorm:  0.5643\n",
      "[DEBUG] Step  12111 | Ep 1087 | Eps 0.880 | MeanReward(10ep):  11.300 | MeanLoss(100it):  0.07490 | MeanQ:  47.393 | GradNorm:  0.3982\n",
      "[DEBUG] Step  12119 | Ep 1088 | Eps 0.880 | MeanReward(10ep):  11.500 | MeanLoss(100it):  0.07434 | MeanQ:  47.420 | GradNorm:  0.5339\n",
      "[DEBUG] Step  12128 | Ep 1089 | Eps 0.880 | MeanReward(10ep):  11.500 | MeanLoss(100it):  0.07400 | MeanQ:  47.453 | GradNorm:  0.3213\n",
      "[DEBUG] Step  12149 | Ep 1090 | Eps 0.880 | MeanReward(10ep):  13.100 | MeanLoss(100it):  0.07327 | MeanQ:  47.389 | GradNorm:  0.3397\n",
      "[cartpole] Ep 1090 Steps 12149 RecentMean 13.10 Eps 0.880\n",
      "[DEBUG] Step  12164 | Ep 1091 | Eps 0.880 | MeanReward(10ep):  13.100 | MeanLoss(100it):  0.07334 | MeanQ:  47.472 | GradNorm:  0.3296\n",
      "[DEBUG] Step  12181 | Ep 1092 | Eps 0.879 | MeanReward(10ep):  13.600 | MeanLoss(100it):  0.07335 | MeanQ:  47.432 | GradNorm:  0.4383\n",
      "[DEBUG] Step  12188 | Ep 1093 | Eps 0.879 | MeanReward(10ep):  12.200 | MeanLoss(100it):  0.07324 | MeanQ:  47.400 | GradNorm:  0.3111\n",
      "[DEBUG] Step  12195 | Ep 1094 | Eps 0.879 | MeanReward(10ep):  11.200 | MeanLoss(100it):  0.07322 | MeanQ:  47.399 | GradNorm:  0.2632\n",
      "[DEBUG] Step  12201 | Ep 1095 | Eps 0.879 | MeanReward(10ep):  11.300 | MeanLoss(100it):  0.07326 | MeanQ:  47.392 | GradNorm:  0.2351\n",
      "[DEBUG] Step  12215 | Ep 1096 | Eps 0.879 | MeanReward(10ep):  10.300 | MeanLoss(100it):  0.07341 | MeanQ:  47.311 | GradNorm:  0.6187\n",
      "[DEBUG] Step  12222 | Ep 1097 | Eps 0.879 | MeanReward(10ep):  10.100 | MeanLoss(100it):  0.07357 | MeanQ:  47.273 | GradNorm:  0.4574\n",
      "[DEBUG] Step  12231 | Ep 1098 | Eps 0.879 | MeanReward(10ep):  10.200 | MeanLoss(100it):  0.07371 | MeanQ:  47.281 | GradNorm:  0.6500\n",
      "[DEBUG] Step  12239 | Ep 1099 | Eps 0.879 | MeanReward(10ep):  10.100 | MeanLoss(100it):  0.07404 | MeanQ:  47.318 | GradNorm:  0.5815\n",
      "[DEBUG] Step  12245 | Ep 1100 | Eps 0.879 | MeanReward(10ep):   8.600 | MeanLoss(100it):  0.07427 | MeanQ:  47.330 | GradNorm:  0.5084\n",
      "[cartpole] Ep 1100 Steps 12245 RecentMean 8.60 Eps 0.879\n",
      "Eval @ Ep 1100 and 12245 steps => mean return 150.00\n",
      "[DEBUG] Step  12255 | Ep 1101 | Eps 0.879 | MeanReward(10ep):   8.100 | MeanLoss(100it):  0.07445 | MeanQ:  46.943 | GradNorm:  0.4418\n",
      "[DEBUG] Step  12263 | Ep 1102 | Eps 0.879 | MeanReward(10ep):   7.200 | MeanLoss(100it):  0.07479 | MeanQ:  47.217 | GradNorm:  0.8202\n",
      "[DEBUG] Step  12279 | Ep 1103 | Eps 0.878 | MeanReward(10ep):   8.100 | MeanLoss(100it):  0.07515 | MeanQ:  47.328 | GradNorm:  0.4242\n",
      "[DEBUG] Step  12285 | Ep 1104 | Eps 0.878 | MeanReward(10ep):   8.000 | MeanLoss(100it):  0.07544 | MeanQ:  47.420 | GradNorm:  0.7081\n",
      "[DEBUG] Step  12303 | Ep 1105 | Eps 0.878 | MeanReward(10ep):   9.200 | MeanLoss(100it):  0.07571 | MeanQ:  47.286 | GradNorm:  0.1246\n",
      "[DEBUG] Step  12315 | Ep 1106 | Eps 0.878 | MeanReward(10ep):   9.000 | MeanLoss(100it):  0.07603 | MeanQ:  47.202 | GradNorm:  0.5166\n",
      "[DEBUG] Step  12321 | Ep 1107 | Eps 0.878 | MeanReward(10ep):   8.900 | MeanLoss(100it):  0.07592 | MeanQ:  47.253 | GradNorm:  0.7872\n",
      "[DEBUG] Step  12331 | Ep 1108 | Eps 0.878 | MeanReward(10ep):   9.000 | MeanLoss(100it):  0.07616 | MeanQ:  47.311 | GradNorm:  0.2313\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 108\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[38;5;66;03m# Training\u001b[39;00m\n\u001b[1;32m    107\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m total_steps \u001b[38;5;241m%\u001b[39m cfg\u001b[38;5;241m.\u001b[39mtrain_freq \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 108\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mq_net\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_net\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreplay\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    109\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m loss \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    110\u001b[0m         logs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlosses\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mappend(loss)\n",
      "Cell \u001b[0;32mIn[10], line 68\u001b[0m, in \u001b[0;36mtrain_step\u001b[0;34m(q_net, target_net, optimizer, replay, cfg)\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m     67\u001b[0m         td_errors \u001b[38;5;241m=\u001b[39m (q_sa \u001b[38;5;241m-\u001b[39m target)\u001b[38;5;241m.\u001b[39mabs()\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[0;32m---> 68\u001b[0m     \u001b[43mreplay\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate_priorities\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhandle_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtd_errors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mfloat\u001b[39m(loss\u001b[38;5;241m.\u001b[39mitem())\n",
      "Cell \u001b[0;32mIn[5], line 148\u001b[0m, in \u001b[0;36mPrioritizedReplayBuffer.update_priorities\u001b[0;34m(self, leaf_idx, td_errors)\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_priority \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_priority, \u001b[38;5;28mfloat\u001b[39m(new_p\u001b[38;5;241m.\u001b[39mmax()))\n\u001b[1;32m    147\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m li, p \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(leaf_idx\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mint\u001b[39m), new_p\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mfloat\u001b[39m)):\n\u001b[0;32m--> 148\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtree\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate_leaf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mli\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[5], line 50\u001b[0m, in \u001b[0;36mSumTree.update_leaf\u001b[0;34m(self, leaf_idx, priority)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mupdate_leaf\u001b[39m(\u001b[38;5;28mself\u001b[39m, leaf_idx: \u001b[38;5;28mint\u001b[39m, priority: \u001b[38;5;28mfloat\u001b[39m):\n\u001b[0;32m---> 50\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mleaf_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpriority\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[5], line 22\u001b[0m, in \u001b[0;36mSumTree.update\u001b[0;34m(self, tree_idx, priority)\u001b[0m\n\u001b[1;32m     20\u001b[0m change \u001b[38;5;241m=\u001b[39m priority \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtree[tree_idx]\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtree[tree_idx] \u001b[38;5;241m=\u001b[39m priority\n\u001b[0;32m---> 22\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m tree_idx \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     23\u001b[0m     tree_idx \u001b[38;5;241m=\u001b[39m (tree_idx \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtree[tree_idx] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m change\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import dataclasses\n",
    "\n",
    "def save_checkpoint(path, q_net, optimizer, cfg, stage_name: str):\n",
    "    torch.save({\n",
    "        'model': q_net.state_dict(),\n",
    "        'optimizer': optimizer.state_dict(),\n",
    "        'config': dataclasses.asdict(cfg),\n",
    "        'stage': stage_name,\n",
    "    }, path)\n",
    "    print('Saved checkpoint:', path)\n",
    "\n",
    "\n",
    "# === Training ===\n",
    "all_stage_logs = {}\n",
    "\n",
    "q_net = None\n",
    "target_net = None\n",
    "optimizer = None\n",
    "stage_index = 0\n",
    "\n",
    "# Make directories for outputs\n",
    "os.makedirs(\"logs\", exist_ok=True)\n",
    "os.makedirs(\"checkpoints\", exist_ok=True)\n",
    "\n",
    "for stage_env_name in cfg.curriculum:\n",
    "    print(f\"\\n=== Stage {stage_index+1}: {stage_env_name} ===\")\n",
    "    env = make_env(stage_env_name, seed=cfg.seed + stage_index)()\n",
    "    eval_env = make_env(stage_env_name, seed=cfg.seed + 100 + stage_index)()\n",
    "\n",
    "    q_net = build_q_network(env, cfg).to(DEVICE)\n",
    "    # if cfg.use_noisy_layer:\n",
    "    #     q_net.reset_noise()\n",
    "    #     target_net.reset_noise()\n",
    "    target_net = build_q_network(env, cfg).to(DEVICE)\n",
    "    hard_update(target_net, q_net)\n",
    "    optimizer = optim.Adam(q_net.parameters(), lr=cfg.lr)\n",
    "\n",
    "    if cfg.use_per:\n",
    "        replay = PrioritizedReplayBuffer(\n",
    "            capacity=cfg.buffer_capacity,\n",
    "            obs_shape=env.single_observation_space.shape,\n",
    "            device=DEVICE,\n",
    "            cfg = cfg\n",
    "        )\n",
    "        print(\"[INFO] Using PrioritizedReplayBuffer\")\n",
    "    else:\n",
    "        replay = ReplayBuffer(cfg.buffer_capacity, env.single_observation_space.shape, DEVICE)\n",
    "        print(\"[INFO] Using uniform ReplayBuffer\")\n",
    "\n",
    "    logs = {\n",
    "        'episode_rewards': [],\n",
    "        'losses': [],\n",
    "        'eps_history': [],\n",
    "        'eval': [],\n",
    "    }\n",
    "\n",
    "    obs, _ = env.reset()\n",
    "    ep_reward = 0\n",
    "    ep_steps = 0\n",
    "    episode_counter = 0\n",
    "    recent_ep_returns = []\n",
    "    total_steps = 0\n",
    "\n",
    "    trajectory_buffer = deque(maxlen=cfg.n_steps)\n",
    "\n",
    "    print('Starting training...')\n",
    "    while total_steps < cfg.max_steps_per_env:\n",
    "        if cfg.use_noisy_layer:\n",
    "            epsilon = 0.0  # No epsilon-greedy when using noisy networks\n",
    "        else:\n",
    "            epsilon = epsilon_by_step(total_steps, cfg)\n",
    "        action = select_action(q_net, obs, epsilon, env)\n",
    "        next_obs, reward, terminated, truncated, info = env.step(action)\n",
    "        reward = process_reward(reward, cfg)\n",
    "        reward = float(np.asarray(reward).reshape(-1)[0])\n",
    "        done = bool(np.asarray(terminated).reshape(-1)[0] or np.asarray(truncated).reshape(-1)[0])\n",
    "\n",
    "        trajectory_buffer.append((obs.copy(), int(action), reward, next_obs.copy(), done))\n",
    "\n",
    "        if len(trajectory_buffer) == cfg.n_steps or done:\n",
    "            n_step_reward = 0\n",
    "\n",
    "            # N-step return calculation\n",
    "            for i, (_, _, reward, _, _) in enumerate(trajectory_buffer):\n",
    "                n_step_reward += (cfg.gamma ** i) * reward\n",
    "            first_obs, first_action, _, _, first_done = trajectory_buffer[0]\n",
    "            _, _, _, last_next_obs, _ = trajectory_buffer[-1]  # Get the last next_obs (n steps ahead)\n",
    "\n",
    "            # Push n-step transition to replay buffer\n",
    "            replay.push(first_obs, first_action, n_step_reward, last_next_obs, first_done)\n",
    "\n",
    "            # Reset buffer if done before filling all n steps\n",
    "            if done:\n",
    "                trajectory_buffer.clear()\n",
    "\n",
    "        obs = next_obs.copy()\n",
    "        ep_reward += reward\n",
    "        ep_steps += 1\n",
    "        total_steps += 1\n",
    "\n",
    "        # Training\n",
    "        if total_steps % cfg.train_freq == 0:\n",
    "            loss = train_step(q_net, target_net, optimizer, replay, cfg)\n",
    "            if loss is not None:\n",
    "                logs['losses'].append(loss)\n",
    "\n",
    "        # Target update\n",
    "        if total_steps % cfg.target_update_interval == 0:\n",
    "            hard_update(target_net, q_net)\n",
    "\n",
    "        logs['eps_history'].append(epsilon)\n",
    "\n",
    "        if done or ep_steps >= cfg.max_episode_len:\n",
    "            logs['episode_rewards'].append(ep_reward)\n",
    "            recent_ep_returns.append(ep_reward)\n",
    "            episode_counter += 1\n",
    "\n",
    "            # --- DEBUG PRINT ---\n",
    "            mean_loss = np.mean(logs['losses'][-100:]) if logs['losses'] else 0.0\n",
    "            mean_reward = np.mean(recent_ep_returns[-cfg.log_interval_episodes:]) if recent_ep_returns else 0.0\n",
    "            with torch.no_grad():\n",
    "                q_sample = q_net(torch.FloatTensor(obs).unsqueeze(0).to(DEVICE))\n",
    "                mean_q = q_sample.mean().item()\n",
    "            grads = [p.grad.norm().item() for p in q_net.parameters() if p.grad is not None]\n",
    "            grad_norm = np.mean(grads) if grads else 0.0\n",
    "            print(f\"[DEBUG] Step {total_steps:6d} | Ep {episode_counter:4d} | \"\n",
    "                  f\"Eps {epsilon:.3f} | MeanReward(10ep): {mean_reward:7.3f} | \"\n",
    "                  f\"MeanLoss(100it): {mean_loss:8.5f} | \"\n",
    "                  f\"MeanQ: {mean_q:7.3f} | GradNorm: {grad_norm:7.4f}\")\n",
    "\n",
    "            if episode_counter % cfg.log_interval_episodes == 0:\n",
    "                mean_recent = np.mean(recent_ep_returns[-cfg.log_interval_episodes:])\n",
    "                print(f\"[{stage_env_name}] Ep {episode_counter} Steps {total_steps} \"\n",
    "                      f\"RecentMean {mean_recent:.2f} Eps {epsilon:.3f}\")\n",
    "\n",
    "            obs, _ = env.reset()\n",
    "            ep_reward = 0\n",
    "            ep_steps = 0\n",
    "\n",
    "            # Periodic evaluation (every 2*log_interval episodes if we have progress)\n",
    "            if episode_counter % (cfg.log_interval_episodes * 2) == 0:\n",
    "                eval_ret = evaluate(eval_env, q_net, cfg, cfg.eval_episodes)\n",
    "                logs['eval'].append((total_steps, eval_ret))\n",
    "                print(f\"Eval @ Ep {episode_counter} and {total_steps} steps => mean return {eval_ret:.2f}\")\n",
    "\n",
    "    # --- Save logs and model after each stage ---\n",
    "    all_stage_logs[stage_env_name] = logs\n",
    "\n",
    "    # Save training logs\n",
    "    df = pd.DataFrame({\n",
    "        'episode_rewards': logs['episode_rewards'],\n",
    "        'losses': logs['losses'][:len(logs['episode_rewards'])],\n",
    "        'eps_history': logs['eps_history'][:len(logs['episode_rewards'])],\n",
    "    })\n",
    "    train_csv = f\"logs/{stage_env_name}_training_log.csv\"\n",
    "    df.to_csv(train_csv, index=False)\n",
    "    print(f\"Saved training data → {train_csv}\")\n",
    "\n",
    "    # Save evaluation results\n",
    "    eval_df = pd.DataFrame(logs['eval'], columns=['total_steps', 'eval_return'])\n",
    "    eval_csv = f\"logs/{stage_env_name}_eval_log.csv\"\n",
    "    eval_df.to_csv(eval_csv, index=False)\n",
    "    print(f\"Saved eval data → {eval_csv}\")\n",
    "\n",
    "    # Save model checkpoint\n",
    "    checkpoint_path = f\"checkpoints/{stage_env_name}_final.pt\"\n",
    "    save_checkpoint(checkpoint_path, q_net, optimizer, cfg, stage_env_name)\n",
    "\n",
    "    stage_index += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Visualization Per Stage\n",
    "helper code for plots, feel free to modify or use different tools, e.g. WandB.\n",
    "Plots: Episode rewards, Epsilon schedule, Loss curve, and Evaluation returns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def moving_average(x, w):\n",
    "    # Ensure x is a flattened 1D numpy array to handle lists like [[1],[2],[3]]\n",
    "    x = np.array(x).flatten()\n",
    "    if len(x) < w:\n",
    "        return x\n",
    "    return np.convolve(x, np.ones(w)/w, mode='valid')\n",
    "\n",
    "for stage, logs in all_stage_logs.items():\n",
    "    rewards = logs['episode_rewards']\n",
    "    losses = logs['losses']\n",
    "    eps_hist = logs['eps_history']\n",
    "    evals = logs['eval']\n",
    "    print(f\"\\n=== {stage} ===\")\n",
    "    plt.figure(figsize=(14,4))\n",
    "    # Rewards\n",
    "    plt.subplot(1,3,1)\n",
    "    plt.title(f'{stage} Rewards')\n",
    "    plt.plot(rewards, alpha=0.5, label='Return')\n",
    "    ma = moving_average(rewards, 100)\n",
    "    if len(ma) != len(rewards):\n",
    "        plt.plot(range(len(rewards)-len(ma), len(rewards)), ma, label='MA(20)')\n",
    "    plt.legend()\n",
    "    plt.xlabel('Episode')\n",
    "\n",
    "    # Epsilon\n",
    "    plt.subplot(1,3,2)\n",
    "    plt.title('Epsilon')\n",
    "    plt.plot(eps_hist)\n",
    "    plt.xlabel('Step')\n",
    "\n",
    "    # Losses\n",
    "    plt.subplot(1,3,3)\n",
    "    plt.title('Loss (smoothed)')\n",
    "    if len(losses) > 0:\n",
    "        lma = moving_average(losses, 200)\n",
    "        plt.plot(losses, alpha=0.3)\n",
    "        plt.plot(range(len(losses)-len(lma), len(losses)), lma, label='MA(200)')\n",
    "        plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    if len(evals) > 0:\n",
    "        xs = [x for x,_ in evals]\n",
    "        ys = [y for _,y in evals]\n",
    "        plt.figure()\n",
    "        plt.title(f'{stage} Evaluation Returns')\n",
    "        plt.plot(xs, ys, marker='o')\n",
    "        plt.xlabel('Env Steps')\n",
    "        plt.ylabel('Mean Return (greedy)')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Group Project: Extensions and Analysis\n",
    "\n",
    "This assignment is a group project. After familiarizing yourselves with the baseline DQN code, your group will divide the following implementation tasks. Once each part is complete, you will merge your code and collaborate on a final report that analyzes and compares the performance of each extension.\n",
    "\n",
    "---\n",
    "### **Part 1: Individual Implementation Tasks**\n",
    "\n",
    "Each group member must implement one of the following sets of extensions. You will need to modify the core components (`MLPQ`, `ReplayBuffer`, `train_step`) to support these new algorithms, using the `cfg` object to toggle them on and off.\n",
    "\n",
    "---\n",
    "#### **Extension A: Double DQN & Dueling DQN**\n",
    "\n",
    "**Assigned to:** *[Student A Name Here]*\n",
    "\n",
    "This set of tasks focuses on improving the Q-value estimation and network architecture.\n",
    "\n",
    "**TODO (A1): Implement Double DQN**\n",
    "In the `train_step` function, implement the logic inside the `if cfg.use_double_dqn:` block. This involves decoupling action selection from evaluation to mitigate overestimation bias by using the **online network** to select the best next action and the **target network** to evaluate its value.\n",
    "\n",
    "**TODO (A2): Implement Dueling DQN**\n",
    "In the `MLPQ` class, modify the `__init__` and `forward` methods to support a dueling architecture. You will need to create separate `value_head` and `advantage_head` layers and combine their outputs using the formula: `Q(s,a) = V(s) + (A(s,a) - mean(A(s,:)))`.\n",
    "\n",
    "---\n",
    "#### **Extension B: Prioritized Experience Replay (PER)**\n",
    "\n",
    "**Assigned to:** *[Student B Name Here]*\n",
    "\n",
    "This task replaces the uniform `ReplayBuffer` with a more intelligent sampling strategy that prioritizes \"surprising\" transitions.\n",
    "\n",
    "**TODO (B1): Implement `PrioritizedReplayBuffer`**\n",
    "Create a new `PrioritizedReplayBuffer` class. This class must manage transition priorities (based on TD-error), sample transitions according to these priorities, and compute importance sampling (IS) weights to correct for the biased sampling. Implementing a **SumTree** is the standard approach for this.\n",
    "\n",
    "**TODO (B2): Integrate PER into `train_step`**\n",
    "Modify the training loop to use your new buffer. In `train_step`, you must use the IS weights to scale the loss for each transition and then call a method on your buffer to update the priorities of the sampled transitions with their new TD-errors.\n",
    "\n",
    "---\n",
    "#### **Extension C: N-Step Returns & Noisy Networks**\n",
    "\n",
    "**Assigned to:** *[Student C Name Here]*\n",
    "\n",
    "This set of tasks focuses on improving the TD target and the agent's exploration strategy.\n",
    "\n",
    "**TODO (C1): Implement N-Step Returns**\n",
    "Modify the training loop and `train_step` to use N-step returns. This involves temporarily storing the last `N` transitions to calculate the discounted N-step reward (`R_n`) and updating the target formula in `train_step` to use `γ^N` for bootstrapping.\n",
    "\n",
    "**TODO (C2): Implement Noisy Networks**\n",
    "Replace epsilon-greedy exploration with learned exploration. Create a custom `NoisyLinear` PyTorch layer that adds parametric noise to its weights. Replace the final linear layers of your `MLPQ` with this new layer and disable epsilon-greedy exploration in the main loop when this feature is active.\n",
    "\n",
    "---\n",
    "### **Optional Extension**\n",
    "\n",
    "#### **Vectorized Environments**\n",
    "For a deeper challenge, modify the entire pipeline to support vectorized environments. This involves changing `cfg.vector_envs` to > 1 and refactoring the training loop to handle batched operations for action selection, environment stepping, and episode tracking. This is a highly effective method for speeding up training but requires careful management of parallel data streams.\n",
    "\n",
    "---\n",
    "### **Part 2: Final Report (Group Task)**\n",
    "\n",
    "Your group's final submission should be a report that includes the following:\n",
    "\n",
    "**1. Experimental Analysis:**\n",
    "*   Run experiments on the Comparing environment comparing your baseline DQN against each of the implemented extensions (A, B, and C) and all extension combined on the challenge environment.\n",
    "*   Generate and include plots for each experiment (Episode Rewards, Loss, etc.).\n",
    "*  For the replay buffer size and N-Step Returns, test out 2-4 different values and reflect on the difference in performance. You may change other hyperparameters as needed to get good performance, also put the highlights in the report.\n",
    "*   Analyze the plots: Reflect how you can see the improvements for each extension. Which extension provided the biggest performance boost or the most stable training? Justify your claims with evidence.\n",
    "\n",
    "**2. Conceptual Questions:**\n",
    "\n",
    "*   **Q1:** Explain \"maximization bias\" in Q-learning. How does your **Double DQN** implementation address it?\n",
    "*   **Q2:** What is the theoretical motivation for the **Dueling DQN** architecture? Why is the special averaging mechanism important?\n",
    "*   **Q3:** Why is uniform sampling from the replay buffer inefficient? How do the **importance sampling weights** in **PER** correct for the biased sampling you introduced?\n",
    "*   **Q4:** Explain the difference between epsilon-greedy exploration and the exploration provided by **Noisy Networks**. What is an advantage of the latter?\n",
    "*   **Q5:** How does changing 'N' in **N-Step Returns** affect the bias-variance trade-off in your Q-learning updates?\n",
    "\n",
    "**3. Reflection:**\n",
    "*   Briefly discuss the biggest challenge your group faced during implementation and how you solved it.\n",
    "*   Were some of the results unexpected, if so in what way?\n",
    "*   If you implemented the vectorized environments, describe the performance improvement you observed in terms of wall-clock time.\n",
    "\n",
    "**4. Code Submission:**\n",
    "*   Submit your complete code with all extensions implemented. Ensure it is well-commented and organized.\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize agent\n",
    "helper code to visualize your trained agent in the notebook. You may modify this code as needed or visualize it in a different way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyvirtualdisplay import Display\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display, clear_output\n",
    "import time\n",
    "import os\n",
    "import pyscreenshot as ImageGrab\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# --- Visualization Setup (from our debugging) ---\n",
    "seed = 0\n",
    "steps_to_show = 1000 # Your original value\n",
    "game_width = 576\n",
    "game_height = 330\n",
    "bbox = (0, 0, game_width, game_height)\n",
    "\n",
    "# Initialize variables to None before the try block for safe cleanup\n",
    "vdisplay = None\n",
    "env = None\n",
    "\n",
    "original_dir = os.getcwd()\n",
    "pufferlib_dir = '/opt/pufferlib'\n",
    "# -------------------------------------------------\n",
    "def first_obs(x):\n",
    "    x = np.asarray(x)\n",
    "    return x[0] if x.ndim > 1 else x\n",
    "\n",
    "# This is your agent's decision-making function, it's perfect as is.\n",
    "def policy_net(st):\n",
    "    return q_net(st)\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def greedy_action(state_np):\n",
    "    st = torch.as_tensor(state_np, dtype=torch.float32, device=DEVICE).unsqueeze(0)\n",
    "    q = policy_net(st)\n",
    "    print(q)\n",
    "    return int(q.argmax(dim=1).item())\n",
    "\n",
    "# The robust 'try...finally' block for safe execution and cleanup\n",
    "try:\n",
    "    vdisplay = Display(visible=False, size=(game_width, game_height))\n",
    "    vdisplay.start()\n",
    "\n",
    "    # os.chdir(pufferlib_dir)\n",
    "    # env = breakout.Breakout(num_envs=1, render_mode='human', seed=seed)\n",
    "    # env = cartpole.Cartpole(num_envs=1, render_mode='human', seed=seed)\n",
    "    env = cartpole.Cartpole(num_envs=1, render_mode='human', seed=seed)\n",
    "    \n",
    "    fig, ax = plt.subplots(1)\n",
    "\n",
    "    current_directory = os.getcwd()\n",
    "    print(f\"Current directory: {current_directory}\")\n",
    "    \n",
    "    # List all files and directories in the current directory\n",
    "    contents = os.listdir(current_directory)\n",
    "    print(\"Contents of the directory:\")\n",
    "    for item in contents:\n",
    "        print(item)\n",
    "\n",
    "    q_net = build_q_network(env, cfg).to(DEVICE)\n",
    "    optimizer = torch.optim.Adam(q_net.parameters(), lr=cfg.lr)  # Placeholder optimizer for load_checkpoint\n",
    "    checkpoint_path = 'checkpoint_cartpole_stage0.pth'  # Adjust path to your saved checkpoint\n",
    "    if os.path.exists(checkpoint_path):\n",
    "        print(f\"File found at: {checkpoint_path}\")\n",
    "    \n",
    "    load_checkpoint(checkpoint_path, q_net, optimizer)  # Load the trained model\n",
    "\n",
    "    raise Error\n",
    "    \n",
    "    q_net.eval()  # Set to evaluation mode\n",
    "\n",
    "    # --- Your Agent's Logic Starts Here ---\n",
    "    obs, _ = env.reset(seed)\n",
    "    state = first_obs(obs).astype(np.float32)\n",
    "    episode_return = 0.0\n",
    "\n",
    "    print(\"Starting agent visualization... Press 'Stop' to interrupt cleanly.\")\n",
    "    print(\"IMPORTANT: After interrupting, you MUST restart the kernel before running this cell again.\")\n",
    "\n",
    "    for t in range(steps_to_show):\n",
    "        # The inner 'try...except' for clean interruption\n",
    "        try:\n",
    "            # Get action from your policy instead of random\n",
    "            # q_net = build_q_network(env, cfg).to(DEVICE)\n",
    "            a = greedy_action(state)\n",
    "\n",
    "            # Step the environment\n",
    "            obs, reward, term, trunc, _info = env.step(a)\n",
    "\n",
    "            # --- Apply the Visualization Pipeline ---\n",
    "            # 1. Render the game to the invisible virtual display\n",
    "            env.render()\n",
    "\n",
    "            # 2. Grab a cropped screenshot of the virtual display\n",
    "            frame = ImageGrab.grab(bbox=bbox)\n",
    "\n",
    "            # 3. Display the screenshot in the notebook\n",
    "            clear_output(wait=True)\n",
    "            ax.imshow(frame)\n",
    "            ax.axis('off')\n",
    "            display(fig)\n",
    "\n",
    "            time.sleep(0.01)\n",
    "            # --- End of Visualization Pipeline ---\n",
    "\n",
    "            # Continue with your agent's logic\n",
    "            state = first_obs(obs).astype(np.float32)\n",
    "            episode_return += float(first_obs(reward))\n",
    "            done = bool(first_obs(term)) or bool(first_obs(trunc))\n",
    "            if done:\n",
    "                print(\"Episode finished.\")\n",
    "                break\n",
    "\n",
    "        except KeyboardInterrupt:\n",
    "            print(\"\\n\\nInterrupted by user. The kernel is now in an unstable state and must be restarted.\")\n",
    "            break\n",
    "\n",
    "    print(f'Greedy policy return: {episode_return:.2f}')\n",
    "\n",
    "finally:\n",
    "    # This block is ALWAYS executed, ensuring a safe shutdown.\n",
    "    print(\"Cleaning up resources...\")\n",
    "    os.chdir(original_dir)\n",
    "\n",
    "    if env is not None:\n",
    "        env.close()\n",
    "\n",
    "    time.sleep(0.5)\n",
    "\n",
    "    if vdisplay is not None:\n",
    "        vdisplay.stop()\n",
    "    print(\"Cleanup complete. If you interrupted the cell, please restart the kernel now.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(path, q_net, optimizer, cfg: Config, stage_name: str):\n",
    "    torch.save({\n",
    "        'model': q_net.state_dict(),\n",
    "        'optimizer': optimizer.state_dict(),\n",
    "        'config': dataclasses.asdict(cfg),\n",
    "        'stage': stage_name,\n",
    "    }, path)\n",
    "    print('Saved checkpoint:', path)\n",
    "\n",
    "def load_checkpoint(path, q_net, optimizer):\n",
    "    data = torch.load(path, map_location=DEVICE)\n",
    "    q_net.load_state_dict(data['model'])\n",
    "    optimizer.load_state_dict(data['optimizer'])\n",
    "    print('Loaded checkpoint from', path)\n",
    "    print(data['config'])\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "deepnote_notebook_id": "43ce95ddb5754e06b99d62b9ae367324",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
