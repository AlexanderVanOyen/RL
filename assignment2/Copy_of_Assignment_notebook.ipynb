{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7fSfdduoCipS"
   },
   "source": [
    "# **Disclaimer:**\n",
    "This assignment must be completed solely by the members of your group. Sharing of code between groups is strictly prohibited. However, you are allowed to discuss general solution approaches or share publicly available resources with members of other groups. Therefore, clearly indicate which public resources you consulted and/or copied code from. Any plagiarism between groups will result in the initiation of a fraud procedure with the director of education."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jrU2rypaT3HI"
   },
   "source": [
    "# **Introduction**\n",
    "\n",
    "In this assignment, you will train a Reinforcement Learning (RL) agent to address a Residential Energy Management problem.\n",
    "\n",
    "The goal is for your agent to control a battery and optimize the electricity costs for a residential household. You will submit your trained agent, which will first be evaluated using an evaluation dataset (different from what you have). Following that, your agent will be deployed in a real-world setting: the [HomeLab](https://idlab.ugent.be/resources/homelab). After deployment, you will receive data detailing the performance of your agent in both environments. Finally, you will write a brief essay analyzing and discussing the results. \\\n",
    "Moreover, we wrote a set of questions / tasks you can fill in the notebook. Each question will contribute to the final grade you will obtain from this assignment.\n",
    "\n",
    "The task involves managing energy for a residential household equipped with a Photovoltaic (PV) system and an Energy Storage System (ESS), specifically a battery. In the context of the global shift toward decarbonizing energy sources, these types of challenges are becoming increasingly significant. Reinforcement Learning (RL) has proven to be a promising technique for addressing them.\n",
    "\n",
    "The setup includes a household where residents consume electricity daily, resulting in a unique consumption profile. Electricity costs are based on a dynamic price profile, with prices varying by the hour. When the household generates more energy (from the PV system or battery) than it consumes, the excess will be sold back to the grid, although the selling price is typically much lower than the buying price.\n",
    "\n",
    "Your objective is to train an RL agent to control the battery and minimize electricity costs. While this might initially seem straightforward (e.g., charge the battery when prices are low and discharge it when prices are high), there are several challenges that complicate the problem:\n",
    "\n",
    "*   **Uncertainty in household consumption**: Future consumption is difficult to predict. While models based on inhabitants' habits can provide reasonable estimates[1], there is always a stochastic element involved.\n",
    "*   **Variability in PV production**: PV energy generation depends on weather conditions, adding another layer of unpredictability.\n",
    "*   **Dynamic price profiles**: Although, for simplicity, we assume a single price profile repeats daily in this assignment, real-world scenarios often involve prices that vary from day to day.\n",
    "*   **Physical constraints**: The control of the battery and the accuracy of data from the household (such as PV production and overall consumption) are not perfect. While these issues are ignored in the simulation, they will become evident in the real-world results from HomeLab after your agent has been deployed.\n",
    "\n",
    "Additionally, although not considered in this assignment, more complex setups may include assets like heat pumps or electric vehicle chargers. These cases may require an extended RL framework, potentially involving multi-agent systems.\n",
    "\n",
    "In the assignment, you will use an agent based on the [PPO](https://arxiv.org/pdf/1707.06347) algorithm. There are some libraries that have already implemented the algorithm, the most famous being [stable-baselines3](https://stable-baselines3.readthedocs.io/en/master/#). However, in this assignment, you will write the algorithm code yourself. To help you with the task, we provide you with most of the code you will need to develop a PPO agent.\n",
    "\n",
    "\\\n",
    "\n",
    "---\n",
    "\n",
    "\\\n",
    "\n",
    "[1] e.g., by observing that they usually leave for work from 9:00 AM to 18:00 PM and cook at 19:00 PM, we can obtain a sensible model of the expected consumption\n",
    "\n",
    "\\\n",
    "\n",
    "---\n",
    "\n",
    "\\\n",
    "\n",
    "\n",
    "Let's now delve into the assignment details!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TO9Xp6MyU7Pm"
   },
   "source": [
    "# **Support Classes**\n",
    "\n",
    "In this assignment, we provide the essential code to help you train a PPO agent for a Home Energy Management (HEM) problem. To begin, download the material.zip file available in the course resources. This file contains the necessary code and information to complete the assignment.\n",
    "\n",
    "After downloading, extract the .py files and upload them into the 'Files' section of this Colab notebook. Next, run the code snippet below, which will import all required external libraries, along with the support classes from the files you just uploaded.\n",
    "\n",
    "We advise you **not** to change the code we provided you in such files. Any changes in those files that hinder our evaluation of your agents might result in your submission receiving a lower grade.\n",
    "\n",
    "**N.B:** For training the neural networks used in the PPO algorithm, we require that you use the PyTorch library included in the provided files. Using other deep learning libraries (e.g., TensorFlow) could cause compatibility issues when evaluating your agent.\n",
    "\n",
    "If you require additional standard Python packages for your work, feel free to import them in the following cell. (by standard Python packages we consider the ones included in the Google colab environment. Avoid using any 'pip install' call in this notebook) \\\n",
    "In case you want to download the notebook and run it locally, you can recreate the colab environment by installing the same versions in your local machine. The list of packages' versions can be seen through the command `!pip freeze` executed below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-07T14:29:28.309323400Z",
     "start_time": "2025-08-07T14:29:26.821495400Z"
    },
    "id": "fK2DxHt3lZ-d"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.distributions import MultivariateNormal\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "\n",
    "sys.path.insert(0, 'Environment.py')\n",
    "sys.path.insert(0, 'nn_utils.py')\n",
    "\n",
    "from Environment import BaseEnvironment\n",
    "from nn_utils import GenericNeuralNetwork\n",
    "\n",
    "\n",
    "\n",
    "# Extra imports:\n",
    "# import ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-07T14:29:29.849193200Z",
     "start_time": "2025-08-07T14:29:28.161613800Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ns2IaYlPFzoI",
    "outputId": "c44767a9-7714-4d8e-980c-f311e5dfb804"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "anyio==4.11.0\n",
      "argon2-cffi==25.1.0\n",
      "argon2-cffi-bindings==25.1.0\n",
      "arrow==1.4.0\n",
      "asttokens==3.0.0\n",
      "async-lru==2.0.5\n",
      "attrs==25.4.0\n",
      "babel==2.17.0\n",
      "beautifulsoup4==4.14.2\n",
      "bleach==6.2.0\n",
      "certifi==2025.10.5\n",
      "cffi==2.0.0\n",
      "charset-normalizer==3.4.4\n",
      "comm==0.2.3\n",
      "contourpy==1.3.3\n",
      "cycler==0.12.1\n",
      "debugpy==1.8.17\n",
      "decorator==5.2.1\n",
      "defusedxml==0.7.1\n",
      "executing==2.2.1\n",
      "fastjsonschema==2.21.2\n",
      "filelock==3.20.0\n",
      "fonttools==4.60.1\n",
      "fqdn==1.5.1\n",
      "fsspec==2025.10.0\n",
      "h11==0.16.0\n",
      "httpcore==1.0.9\n",
      "httpx==0.28.1\n",
      "idna==3.11\n",
      "ipykernel==7.0.1\n",
      "ipython==9.6.0\n",
      "ipython_pygments_lexers==1.1.1\n",
      "isoduration==20.11.0\n",
      "jedi==0.19.2\n",
      "Jinja2==3.1.6\n",
      "json5==0.12.1\n",
      "jsonpointer==3.0.0\n",
      "jsonschema==4.25.1\n",
      "jsonschema-specifications==2025.9.1\n",
      "jupyter-events==0.12.0\n",
      "jupyter-lsp==2.3.0\n",
      "jupyter_client==8.6.3\n",
      "jupyter_core==5.9.1\n",
      "jupyter_server==2.17.0\n",
      "jupyter_server_terminals==0.5.3\n",
      "jupyterlab==4.4.9\n",
      "jupyterlab_pygments==0.3.0\n",
      "jupyterlab_server==2.27.3\n",
      "kiwisolver==1.4.9\n",
      "lark==1.3.0\n",
      "MarkupSafe==3.0.3\n",
      "matplotlib==3.10.7\n",
      "matplotlib-inline==0.1.7\n",
      "mistune==3.1.4\n",
      "mpmath==1.3.0\n",
      "nbclient==0.10.2\n",
      "nbconvert==7.16.6\n",
      "nbformat==5.10.4\n",
      "nest-asyncio==1.6.0\n",
      "networkx==3.5\n",
      "notebook==7.4.7\n",
      "notebook_shim==0.2.4\n",
      "numpy==2.3.4\n",
      "nvidia-cublas-cu12==12.8.4.1\n",
      "nvidia-cuda-cupti-cu12==12.8.90\n",
      "nvidia-cuda-nvrtc-cu12==12.8.93\n",
      "nvidia-cuda-runtime-cu12==12.8.90\n",
      "nvidia-cudnn-cu12==9.10.2.21\n",
      "nvidia-cufft-cu12==11.3.3.83\n",
      "nvidia-cufile-cu12==1.13.1.3\n",
      "nvidia-curand-cu12==10.3.9.90\n",
      "nvidia-cusolver-cu12==11.7.3.90\n",
      "nvidia-cusparse-cu12==12.5.8.93\n",
      "nvidia-cusparselt-cu12==0.7.1\n",
      "nvidia-nccl-cu12==2.27.5\n",
      "nvidia-nvjitlink-cu12==12.8.93\n",
      "nvidia-nvshmem-cu12==3.3.20\n",
      "nvidia-nvtx-cu12==12.8.90\n",
      "packaging==25.0\n",
      "pandas==2.3.3\n",
      "pandocfilters==1.5.1\n",
      "parso==0.8.5\n",
      "pexpect==4.9.0\n",
      "pillow==12.0.0\n",
      "platformdirs==4.5.0\n",
      "prometheus_client==0.23.1\n",
      "prompt_toolkit==3.0.52\n",
      "psutil==7.1.1\n",
      "ptyprocess==0.7.0\n",
      "pure_eval==0.2.3\n",
      "pycparser==2.23\n",
      "Pygments==2.19.2\n",
      "pyparsing==3.2.5\n",
      "python-dateutil==2.9.0.post0\n",
      "python-json-logger==4.0.0\n",
      "pytz==2025.2\n",
      "PyYAML==6.0.3\n",
      "pyzmq==27.1.0\n",
      "referencing==0.37.0\n",
      "requests==2.32.5\n",
      "rfc3339-validator==0.1.4\n",
      "rfc3986-validator==0.1.1\n",
      "rfc3987-syntax==1.1.0\n",
      "rpds-py==0.27.1\n",
      "Send2Trash==1.8.3\n",
      "setuptools==80.9.0\n",
      "six==1.17.0\n",
      "sniffio==1.3.1\n",
      "soupsieve==2.8\n",
      "stack-data==0.6.3\n",
      "sympy==1.14.0\n",
      "terminado==0.18.1\n",
      "tinycss2==1.4.0\n",
      "torch==2.9.0\n",
      "tornado==6.5.2\n",
      "traitlets==5.14.3\n",
      "triton==3.5.0\n",
      "typing_extensions==4.15.0\n",
      "tzdata==2025.2\n",
      "uri-template==1.3.0\n",
      "urllib3==2.5.0\n",
      "wcwidth==0.2.14\n",
      "webcolors==24.11.1\n",
      "webencodings==0.5.1\n",
      "websocket-client==1.9.0\n"
     ]
    }
   ],
   "source": [
    "!pip freeze"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bs9cACy0W98v"
   },
   "source": [
    "# **Environment**\n",
    "\n",
    "Now that you imported the classes from the files we provided you, you can start building an RL framework. RL agents learn how to obtain optimal actions by interacting with an environment, modeled by a Markovian Decision Process. The class 'BaseEnvironment' contained in the file 'Environment.py' is exactly that."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6u2f4MZPas1l"
   },
   "source": [
    "Let's initialize a BaseEnvironment class:\n",
    "\n",
    "First, you need to fix some hyperparameters defining the details of the environment.\n",
    "- `power_capacity` is the max. power the battery is physically capable of injecting (or withdrawing). It is expressed in kW\n",
    "- `energy_capacity` is the max. energy the battery is physically allowed to store. It is expressed in kWh\n",
    "\n",
    "The battery used in the evaluations has a power capacity of 4kW and an energy capacity of 8kWh. Therefore, the agent you upload should be trained with these capacity values.\n",
    "\n",
    "You will also need to load the 'public_data_dict.pkl' we provided you. The loaded file is a dictionary containing historical data on PV production and household consumption for the weekdays of almost a whole year. Every instance of the `BaseEnvironment` requires as input a data dictionary with the same format we provided you with. \\\n",
    "Further details of the environment are contained in the documentation we provided in the .zip file. **Please read it carefully**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-07T14:29:30.323048600Z",
     "start_time": "2025-08-07T14:29:29.654541700Z"
    },
    "id": "feTJXf2YbNGX"
   },
   "outputs": [],
   "source": [
    "# HYPERPARAMETERS:\n",
    "power_capacity = 4\n",
    "energy_capacity = 8\n",
    "\n",
    "### Environment initialization\n",
    "with open('public_data_dict.pkl', 'rb') as f:\n",
    "    data_dict = pickle.load(f)\n",
    "\n",
    "env = BaseEnvironment(power_capacity = power_capacity,\n",
    "                      energy_capacity = energy_capacity,\n",
    "                      data = data_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X-7qjiyJfDrF"
   },
   "source": [
    "\n",
    "In machine learning, it is common practice to divide a dataset into at least two parts:\n",
    "\n",
    "- A training set, used to train the model;\n",
    "- A validation set, which remains completely unseen by the model during training and is used to assess its performance during and after training.\n",
    "\n",
    "This division is not always allowed in RL. For example, in board game environments such as chess you do not necessarily have historical data to divide. Hence, in cases like that it is not possible to separate into different sets. \\\n",
    "However, in problems like this where historical data is available, you can (and should) divide sets to ensure the agent is not simply overfitting on the training set. Below we suggest a possible division that is based on the data available and the conditions in which the agent will be evaluated.\n",
    "\n",
    "Additionally, the policy of an agent during training often differs from its policy during evaluation. For instance, when evaluating an agent, it may be beneficial to minimize the effect of stochastic events by standardizing evaluation conditions. One way to achieve this is to consistently use the agent's greedy actions during evaluation and ensure that the environment always starts with the same initial states."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-07T14:29:30.323048600Z",
     "start_time": "2025-08-07T14:29:30.084326600Z"
    },
    "id": "Q2YAYx80UP3n"
   },
   "outputs": [],
   "source": [
    "train_length = int(len(data_dict) * 0.8)\n",
    "\n",
    "train_set_indexes = list(range(0, train_length))\n",
    "test_set_indexes = list(range(train_length, len(data_dict)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MdY_9cI-g4NC"
   },
   "source": [
    "# **PPO Agent**\n",
    "\n",
    "Now, it is time to start writing the PPO code. In the .zip file, we provided you with some utility classes that will help you train and use a PyTorch neural network. \\\n",
    "Moreover, to help you with the task of implementing a PPO algorithm, we provided you with most of the code structure. \\\n",
    "Below, you can find the class of the PPO agent you will use in the assignment. First, we ask you to **carefully read the code** and understand it as much as possible. The code misses some parts that you will need to fill in. In particular, you need to complete the following methods:\n",
    "- `train`: Perform `num_iterations` training steps. In each training step, perform `num_batches` rollouts and train both the actor and the critic on the obtained data. In this method, 2 portions needs to be completed. First, you will need to calculate the estimated advantage values `A_k`. For stability reasons, it is good practice to normalize these values by scaling them into a more manageable interval (e.g., ~[-1, 1]); how can you do so? If you do not scale the estimations, what do you notice in your training curve? Then, you need to implement the PPO loss for the actor. For that, it can be handy to use the torch function `torch.clamp`.\n",
    "- `calculate_value_logprobs`: Given the batched states and actions, it returns the estimated values of the batched states (from the critic network) and the logarithmic probabilities of the batched actions in relation to the current actor distribution. Check the documentation of the `MultivariateNormal` class, as it will be quite useful for implementing this function.\n",
    "- `sample_action`: Given the input `state`, it returns the corresponding action from the actor, and the logarithmic probability of the action in relation to the current actor distribution. There is also a boolean input `explore`. If the boolean is set to `True`, the output is an action sampled from the actor probability distribution. Else, if the boolean is set to `False`, the action will be the most likely one from the actor probability distribution. Check the documentation of the `MultivariateNormal` class, as it will be quite useful for implementing this function.\n",
    "- `rollout`: This function perform `num_batches` rollout. Each rollout correspond to the execution of one episode sampling from the actor policy distribution. You can use the `sample_action` method for that. Part of the rollout loop is already provided. You will need to complete it by properly filling the batch lists (`batch_action`, `batch_states`, `batch_rewards`, and `batch_log_probs`) with their corresponding values.\n",
    "\n",
    "**Important: The agent and critic should be working and training with normalized (i.e., scaled) states, actions, and rewards. Make sure to implement the following code sections accordingly. When evaluating your agent, we will assume your actor neural network takes the normalized state as input and returns the normalized actions. We will normalize the states and de-normalize the actions accordingly to the environment documentation.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-15T08:26:19.741849800Z",
     "start_time": "2025-10-15T08:26:18.728734Z"
    },
    "id": "0OCeK4hal35c"
   },
   "outputs": [],
   "source": [
    "class PPOHyperparameters():\n",
    "\n",
    "    def __init__(self, num_batches: int, sampling_variance: float, gamma: float, updates_per_iteration: int, clip_value: float, actor_lr: float, critic_lr: float, ):\n",
    "        self.num_batches = num_batches\n",
    "\n",
    "        # Variance of the multivariate Normal distribution used to sample exploratory actions\n",
    "        self.sampling_variance = sampling_variance\n",
    "\n",
    "        # Discount factor\n",
    "        self.gamma = gamma\n",
    "\n",
    "        # Number of updates in each training iteration\n",
    "        self.updates_per_iteration = updates_per_iteration\n",
    "\n",
    "        # Epsilon used in the clip for training\n",
    "        self.clip_value = clip_value\n",
    "\n",
    "        # Networks Learning Rate\n",
    "        self.actor_lr = actor_lr\n",
    "        self.critic_lr = critic_lr\n",
    "\n",
    "class PPOAgent:\n",
    "\n",
    "    def __init__(self, env, state_dim, action_dim, actor: nn.Module, critic: nn.Module, hyperparameters: PPOHyperparameters):\n",
    "\n",
    "        self.env = env\n",
    "\n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "\n",
    "        self.hyperparameters = hyperparameters\n",
    "\n",
    "        self.actor = actor\n",
    "        self.actor_optimizer = torch.optim.Adam(self.actor.parameters(), lr=self.hyperparameters.actor_lr)\n",
    "\n",
    "        self.critic = critic\n",
    "        self.critic_optimizer = torch.optim.Adam(self.critic.parameters(), lr=self.hyperparameters.critic_lr)\n",
    "\n",
    "        self.cov_var = torch.full(size=(self.action_dim,), fill_value = self.hyperparameters.sampling_variance)\n",
    "        self.cov_mat = torch.diag(self.cov_var) # This will be used to sample actions from the agent distribution. The higher the variance, the wider the gaussian distribution.\n",
    "\n",
    "\n",
    "    def train(self, num_iterations:int = 200):\n",
    "        for iteration in range(num_iterations):\n",
    "            batch_states, batch_actions, batch_log_probs, batch_cumulative_rewards, batch_rewards = self.rollout()\n",
    "\n",
    "            V, _ = self.calculate_value_logprobs(batch_states=batch_states, batch_actions=batch_actions)\n",
    "            V = V.detach()\n",
    "\n",
    "            # Exercise: Calculate the Advantage and normalize it\n",
    "            # A_k = ...\n",
    "            advantages = batch_cumulative_rewards - V\n",
    "            # normalize\n",
    "            advantages = (advantages - advantages.mean()) / (advantages.std(unbiased=False) + 1e-8)\n",
    "\n",
    "            for _ in range(self.hyperparameters.updates_per_iteration):\n",
    "                current_V, current_log_probs = self.calculate_value_logprobs(batch_states=batch_states, batch_actions=batch_actions)\n",
    "\n",
    "                probability_ratios = torch.exp(current_log_probs - batch_log_probs)\n",
    "\n",
    "                # Exercise: Implement the PPO loss from the probability_ratios variable\n",
    "                # actor_loss = ...\n",
    "                clip_eps = self.hyperparameters.clip_value\n",
    "                unclipped_obj = probability_ratios * advantages\n",
    "                clipped_ratios = torch.clamp(probability_ratios, 1.0 - clip_eps, 1.0 + clip_eps)\n",
    "                clipped_obj = clipped_ratios * advantages\n",
    "\n",
    "                # we minimize loss, so take negative of objective and mean\n",
    "                actor_loss = -torch.mean(torch.min(unclipped_obj, clipped_obj))\n",
    "\n",
    "                \n",
    "                self.actor_optimizer.zero_grad()\n",
    "                actor_loss.backward(retain_graph=True)\n",
    "                self.actor_optimizer.step()\n",
    "\n",
    "                critic_loss = torch.nn.MSELoss()(current_V, batch_cumulative_rewards.squeeze(-1))\n",
    "                self.critic_optimizer.zero_grad()\n",
    "                critic_loss.backward()\n",
    "                self.critic_optimizer.step()\n",
    "\n",
    "\n",
    "    def calculate_value_logprobs(self, batch_states, batch_actions):\n",
    "        \"\"\"\n",
    "        batch_states: tensor (N, state_dim) -- normalized\n",
    "        batch_actions: tensor (N, action_dim) or (N,) but in training you used unsqueeze so it's shape (N,1)\n",
    "        returns: V (N,), log_probs (N,)\n",
    "        \"\"\"\n",
    "        # Value estimates\n",
    "        V = self.critic(batch_states).squeeze(-1)  # shape (N,)\n",
    "\n",
    "        # Actor predicted mean for actions (normalized actions)\n",
    "        action_means = self.actor(batch_states)  # shape (N, action_dim)\n",
    "\n",
    "        # Ensure batch_actions is shaped (N, action_dim)\n",
    "        if batch_actions.dim() == 1:\n",
    "            actions = batch_actions.unsqueeze(-1)\n",
    "        else:\n",
    "            actions = batch_actions\n",
    "        actions = actions.view(action_means.shape)  # align shapes\n",
    "\n",
    "        # Create multivariate normal with diagonal covariance (same for all samples)\n",
    "        cov_mat = self.cov_mat.to(action_means.device)\n",
    "        dist = MultivariateNormal(loc=action_means, covariance_matrix=cov_mat)\n",
    "\n",
    "        # log_prob returns (N,) for multivariate\n",
    "        log_probs = dist.log_prob(actions)\n",
    "\n",
    "        return V, log_probs\n",
    "\n",
    "        \n",
    "    def sample_action(self, state, explore=False):\n",
    "        \"\"\"\n",
    "        state: raw environment state (list/array)\n",
    "        explore: if True sample from distribution; otherwise return mean (deterministic)\n",
    "        returns:\n",
    "            action (numpy scalar or array) -> normalized action (so actor/critic are trained on normalized actions)\n",
    "            log_prob (torch.tensor scalar)\n",
    "        \"\"\"\n",
    "        # Normalize state for the actor\n",
    "        norm_state = np.array(self.env.scale_state(state), dtype=np.float32)\n",
    "        state_tensor = torch.tensor(norm_state, dtype=torch.float).unsqueeze(0)  # shape (1, state_dim)\n",
    "\n",
    "        # Actor mean (normalized action)\n",
    "        with torch.no_grad():\n",
    "            action_mean = self.actor(state_tensor).squeeze(0)  # shape (action_dim,)\n",
    "\n",
    "        cov_mat = self.cov_mat.to(action_mean.device)\n",
    "        dist = MultivariateNormal(loc=action_mean, covariance_matrix=cov_mat)\n",
    "\n",
    "        if explore:\n",
    "            sampled_action = dist.sample()  # tensor (action_dim,)\n",
    "        else:\n",
    "            sampled_action = action_mean  # deterministic\n",
    "\n",
    "        log_prob = dist.log_prob(sampled_action)  # scalar tensor\n",
    "\n",
    "        # Return numpy for convenience and the log_prob tensor\n",
    "        return sampled_action.cpu().numpy(), log_prob\n",
    "\n",
    "\n",
    "    def compute_cumulative_rewards(self, batch_rewards):\n",
    "        batch_cumulative_rewards = []\n",
    "\n",
    "        for reward_episodes in batch_rewards:  # iterate normally\n",
    "            discounted_reward = 0\n",
    "            for reward in reversed(reward_episodes):\n",
    "                discounted_reward = self.hyperparameters.gamma * discounted_reward + reward\n",
    "                batch_cumulative_rewards.insert(0, discounted_reward)  # insert in front\n",
    "\n",
    "        # Flatten in case some inner lists got in\n",
    "        batch_cumulative_rewards = [float(r.item()) if isinstance(r, np.ndarray) else float(r) for r in batch_cumulative_rewards]\n",
    "\n",
    "        batch_cumulative_rewards = torch.tensor(batch_cumulative_rewards, dtype=torch.float)\n",
    "        return batch_cumulative_rewards\n",
    "\n",
    "\n",
    "    def rollout(self):\n",
    "        # NOTE: Put the correct sizes as comment here\n",
    "        batch_states = []       # (num_batches * timesteps_per_episode, state_dim)\n",
    "        batch_actions = []      # (num_batches * timesteps_per_episode, action_dim)\n",
    "        batch_log_probs = []    # (num_batches * timesteps_per_episode)\n",
    "        batch_rewards = []      # (num_batches, timesteps_per_episode)\n",
    "\n",
    "        for t in range(self.hyperparameters.num_batches):\n",
    "            self.env.reset(np.random.choice(train_set_indexes))   # Train on a random day from the training set\n",
    "            rewards_episode = []\n",
    "\n",
    "            state = self.env.state\n",
    "            done = self.env.done\n",
    "\n",
    "            while not done:\n",
    "                # Sample normalized action from policy\n",
    "                norm_action, log_prob = self.sample_action(state, explore=True)\n",
    "\n",
    "                # Convert normalized → real-world action\n",
    "                real_action = self.env.descale_action(norm_action)\n",
    "\n",
    "                # Step in environment\n",
    "                next_state, reward, done, _ = self.env.step(real_action)\n",
    "\n",
    "                # Normalize state\n",
    "                norm_state = self.env.scale_state(state)\n",
    "                batch_states.append(norm_state)\n",
    "\n",
    "                # Store normalized action (float or array)\n",
    "                batch_actions.append(\n",
    "                    float(norm_action) if np.isscalar(norm_action) or np.shape(norm_action) == ()\n",
    "                    else np.array(norm_action, dtype=float)\n",
    "                )\n",
    "\n",
    "                # Store log probability\n",
    "                batch_log_probs.append(log_prob.detach().cpu().item())\n",
    "\n",
    "                # Scale reward for stability\n",
    "                scaled_reward = self.env.scale_reward(reward)\n",
    "                rewards_episode.append(scaled_reward)\n",
    "\n",
    "                state = next_state\n",
    "\n",
    "            batch_rewards.append(rewards_episode)\n",
    "\n",
    "        # Convert arrays for tensors\n",
    "        batch_states = torch.tensor(np.array(batch_states), dtype=torch.float)\n",
    "        batch_actions_array = np.array(batch_actions)  # or np.stack(batch_actions) if shapes match\n",
    "        batch_actions_tensor = torch.tensor(batch_actions_array, dtype=torch.float).unsqueeze(dim=-1)\n",
    "        batch_log_probs = torch.tensor(batch_log_probs, dtype=torch.float)\n",
    "\n",
    "        # ✅ DO NOT CONVERT batch_rewards TO TENSOR HERE\n",
    "        batch_cumulative_rewards = self.compute_cumulative_rewards(batch_rewards=batch_rewards)\n",
    "\n",
    "        print(type(batch_rewards), len(batch_rewards), type(batch_rewards[0]))\n",
    "\n",
    "        return batch_states, batch_actions_tensor, batch_log_probs, batch_cumulative_rewards, batch_rewards\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def save_agent(self):\n",
    "        with open('actor.pickle', 'wb') as handle:\n",
    "            pickle.dump(self.actor, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "    def load_agent(self):\n",
    "        with open('actor.pickle', 'rb') as handle:\n",
    "            self.actor = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fcyZU_YEo17m"
   },
   "source": [
    "# **Interaction agent-environment**\n",
    "\n",
    "Now that you’ve written the code for the PPO agent, it’s good practice to perform some sanity checks to ensure the agent behaves as expected. But first, we need to implement the code that will serve as the interface between the environment and the agent.\n",
    "\n",
    "Let’s begin by initializing the agent class. Below is an example of hyperparameters you can use. These values are **intentionally** chosen to be suboptimal. Meaning that the code will run with those values, but the agent **will not have good training performance**. \\\n",
    "**Remember to adjust them when you start evaluating the agent's performance.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-07T15:03:00.989773300Z",
     "start_time": "2025-08-07T15:03:00.775085500Z"
    },
    "id": "1ARHDju-tozp"
   },
   "outputs": [],
   "source": [
    "state_dim = 5       # Dimension of each state, do not change\n",
    "action_dim = 1      # Dimension of the actions, do not change\n",
    "\n",
    "actor_nn_params = dict(input_size=state_dim,\n",
    "                       layers=[(4096, 'relu', 0.9), # (number_of_neurons, activation_function, dropout_rate)\n",
    "                               (2048, 'relu', 0.9), ],\n",
    "                       output_size=action_dim,\n",
    "                       activation_final='tanh')\n",
    "actor = GenericNeuralNetwork(params=actor_nn_params)\n",
    "\n",
    "critic_nn_params = dict(input_size=state_dim,\n",
    "                        layers=[(2048, 'relu', 0.9),\n",
    "                                (4096, 'relu', 0.9), ],\n",
    "                        output_size=1,\n",
    "                        activation_final='linear')\n",
    "critic = GenericNeuralNetwork(params=critic_nn_params)\n",
    "\n",
    "hyperparameters = PPOHyperparameters(num_batches=1,                 # Number of episodes per rollout\n",
    "                                     sampling_variance=5.735,       # Variance of the actor gaussian distribution\n",
    "                                     gamma=0.25,                    # Discount factor\n",
    "                                     updates_per_iteration=19345,   # Neural Network backpropagation steps per train() call\n",
    "                                     clip_value=0.9,                # Clip value from the PPO loss\n",
    "                                     actor_lr=30.857,               # Learning rate of the Actor\n",
    "                                     critic_lr=5.12)                # Learning rate of the Critic\n",
    "\n",
    "ppo_agent = PPOAgent(env=env, state_dim=state_dim, action_dim=action_dim, actor=actor, critic=critic, hyperparameters=hyperparameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lqRZvYVKtrhx"
   },
   "source": [
    "# **Training loop**\n",
    "\n",
    "Finally, we are ready to train and evaluate the agents. To do so, you can use the train method of the PPO agent. It is good practice to regularly evaluate the agent after a certain amount of iteration during the training phase. In the evaluation process, the actions applied to the environment should be the greedy  ones (i.e., the action the agent currently believes to be the optimal). We provided you with the structure of an evaluation method, you can fill the missing code and properly use it in your training phase \\\n",
    "As you experiment with this assignment, think of how you can improve the training based on the knowledge you learned in the classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2025-08-07T15:03:38.725830Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 510
    },
    "id": "mXc7dUttkQ_t",
    "is_executing": true,
    "outputId": "19370976-cfb6-49f6-a81e-20946c1cd287"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "<class 'list'> 1 <class 'list'>\n"
     ]
    }
   ],
   "source": [
    "def environment_episode(day_index: int = 0):\n",
    "    '''\n",
    "    Reset and interact with the environment until the end of the selected day.\n",
    "\n",
    "    day_index: Code of the day used in the environment\n",
    "    '''\n",
    "    ppo_agent.env.reset(day_index)\n",
    "    total_reward = 0\n",
    "\n",
    "    while not env.done:\n",
    "        state = ppo_agent.env.state\n",
    "        action, _ = ppo_agent.sample_action(state, explore=False)\n",
    "        action = action.detach().item()\n",
    "        next_state, reward, done, _ = env.step(ppo_agent.env.descale_action(action))\n",
    "\n",
    "        total_reward += reward\n",
    "\n",
    "    return total_reward\n",
    "\n",
    "def evaluate_agent(num_eval_days: int = 10):\n",
    "    \"\"\"\n",
    "    Runs deterministic evaluation episodes on different days and returns\n",
    "    the mean total reward.\n",
    "    \"\"\"\n",
    "    total_rewards = []\n",
    "\n",
    "    # Choose distinct or random days from your training set\n",
    "    for _ in range(num_eval_days):\n",
    "        day_index = np.random.choice(train_set_indexes)\n",
    "        total_reward = environment_episode(day_index)\n",
    "        total_rewards.append(total_reward)\n",
    "\n",
    "    # Compute the average total reward\n",
    "    evaluation_reward = np.mean(total_rewards)\n",
    "    return evaluation_reward\n",
    "\n",
    "num_iteration = 100\n",
    "iteration_per_evaluation = 1\n",
    "\n",
    "for training_iteration in range(1, num_iteration):\n",
    "    print(training_iteration)\n",
    "    ppo_agent.train(num_iterations=1)\n",
    "\n",
    "    if (training_iteration % iteration_per_evaluation) == 0:\n",
    "        reward = evaluate_agent()\n",
    "        print(training_iteration, reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZvHwdQC7snCy"
   },
   "source": [
    "# **Conclusions**\n",
    "\n",
    "Now you have everything you need to start training an effective agent.\n",
    "\n",
    "To proceed further, you can change the hyperparameters, write new functions to keep track of the training process, and produce informative graphs that can help you understand what is going on behind the curtains of the model. \\\n",
    "To double check the code you just wrote, try to train and evaluate the agent on a single day (i.e., using the same data of 1 single day for both training and evaluation). This is a very simple task, hence your agent is supposed to behave well without too many difficulties. For example, try with day number 100. With that day, your agent should be able to achieve a daily total reward that is at least greater than -0.4 (when using a 4kW, 8kWh battery). \\\n",
    "Keep in mind that the algorithm will require some time to get trained. Don't give up on a parameter setting after only a bunch of minutes of training. Sometimes the agent might just need some extra episodes of training to start getting some sense of its actions.\n",
    "\n",
    "**Your final agent needs to be saved using the methods we provided you. The agent you will upload will be evaluated on a private dataset taken out of the data we gave you, and the results will influence the score of your assignment.**\n",
    "\n",
    "Whatever extra code you think you will need to train the agent, please write it below. \\\n",
    "**Moreover, we prepared a set of questions and tasks for you. These are part of your assignment evaluation.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tZZh5BDgt0go"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "X29MlT77t0sh"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O00dLPWct06_"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "wDRkB3hcwVYd",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### **Question 0**\n",
    "\n",
    "As explained before, we ask you to train your agent by working with normalize states, actions, and rewards by scaling their original value in more contained domain ([0, 1] or [-1, 1]). Can you reflect on this, giving your thoughts on why this should be (or should not be) done?\n",
    "\n",
    "Write you brief considerations below: \\\n",
    "... \\\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pEddLv6IoYbV"
   },
   "source": [
    "### **Question 1**\n",
    "Coming out with good hyperparameters is not straightforward. Can you make a list below of the **most important** hyperparameters you had to choose to obtain better results? Give brief intuitions for each. If needed, you can back it up with evaluation results.\n",
    "\n",
    "- hyperparam1: fixed to x because of ...\n",
    "- hyperparam2: fixed to y because of ...\n",
    "- hyperparam3: fixed to z because of ...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PzD1D1QTuete"
   },
   "source": [
    "### **Question 2:**\n",
    "When training an RL agent (or whatever Machine Learning model), a crucial step is to track the learning. Can you produce some informative graphs that tells you how is the training going? What observation can you draw from those?\n",
    "\n",
    "Can you think of some other insightful graphs / information about the agent? For example, what kind of extra metric can you keep track of? Here are some examples:\n",
    "- Probability ratio of the actions during training\n",
    "- KL divergence of the policy before and after each training step\n",
    "\n",
    "Briefly explain what you did and why here:\n",
    "\n",
    "... \\\n",
    "... \\\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WrcED4IYu74o"
   },
   "outputs": [],
   "source": [
    "# Training graphs\n",
    "# ...\n",
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3ToVC-ll7x8J"
   },
   "source": [
    "### **Question 3:**\n",
    "\n",
    "When using RL on a given problem, **you** also need to understand what the problem is about. Following that, it is important to keep track of the agent's performance by not only checking its numerical metrics (e.g., the obtained daily rewards), but also by checking if its actions make sense in the first place. To do so, plotting the policies obtained on a limited interval of time (e.g., a single day) is crucial. Can you write a function that does that? What observations / conclusions can you draw when you plot the agent policies?\n",
    "\n",
    "Can you also think of additional way you can plot the policy without specifically referring to any specific day? In other words, can you generate a plot that describes the general behavior of the learnt policy in relation to some state inputs (not necessarily from the actual dataset; they could be artificially made just for the sake of the plot)?\n",
    "\n",
    "Briefly explain your observations here:\n",
    "\n",
    "... \\\n",
    "... \\\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Jl3viQWd8sbf"
   },
   "outputs": [],
   "source": [
    "# Policy graphs\n",
    "# ...\n",
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "5H2_sJ36wVYd",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### **Question 4.1:**\n",
    "\n",
    "PPO stands in the RL algorithm family of policy gradients methods. These differ from value-based methods such as Deep Q-Network (DQN) in the way the agent obtain the actions.\n",
    "Policy gradient methods have an actor model that directly returns an action after processing an input state. In value-based methods, instead, the actor processes the input state to approximate the Q-function (or the value function) of the problem; the action is then selected based on the approximated Q-function values (for example by argmaxing each Q values). \\\n",
    "Specifically thinking about this problem (home energy management), can you reflect on what major advantages/disadvantages one algorithmic family brings over the other?\n",
    "\n",
    "Briefly explain your observations here:\n",
    "\n",
    "... \\\n",
    "... \\\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "R3LEYNudwVYd",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### **Question 4.2:**\n",
    "\n",
    "In this assignment, you used 2 neural networks as models for both the critic and actor. This is a choice we made when designing this assignment, but it is not mandatory. Other regression architectures/models could have been used. Can you think of any specific reason why we would want to use a different model?  Write some considerations regarding this, with specific focus on the actor model.\n",
    "\n",
    "Briefly explain your observations here:\n",
    "\n",
    "... \\\n",
    "... \\\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ukE3icNGu869"
   },
   "source": [
    "### **Question 5:**\n",
    "\n",
    "One of the benefits of implementing an algorithm from scratch (besides the obvious one of understanding more deeply what the algorithm is about) is the total control over the code you just wrote. This enables you to customize your code to, for example, gain informative insights regarding the algorithm dynamics.\n",
    "There are a few things you can implement in your PPO code to potentially improve its performance. Here we give you some suggestions, but feel free to also check online sources to take additional inspiration.\n",
    "- Entropy regularization\n",
    "- Generalized Advantage Estimation (check [paper](https://arxiv.org/pdf/1506.02438))\n",
    "- Learnable sampling variance\n",
    "- Mini-batch partition fo the rollout data when training\n",
    "- Learning Rate Annealing\n",
    "\n",
    "Can you implement some of these additional features and report their value over the agent training phase? You do not have to implement them all, but even if you do, **please report only 1 or 2 of them**. \\\n",
    "Make a complete report of what differences you noticed using or not using the additional feature you implemented. Consider it as a mini scientific report.\n",
    "\n",
    "... \\\n",
    "... \\\n",
    "... \\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MSy-VUVZwVYd"
   },
   "outputs": [],
   "source": [
    "# Implemented features report\n",
    "# ...\n",
    "# ..."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
