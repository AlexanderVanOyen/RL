{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PTT57uLHmIVf"
   },
   "source": [
    "\n",
    "# **Disclaimer**\n",
    "This assignment must be completed solely by the members of your group. Sharing of code between groups is strictly prohibited. However, you are allowed to discuss general solution approaches or share publicly available resources with members of other groups. Therefore, clearly indicate which public resources you consulted and/or copied code from. Any plagiarism between groups will result in the initiation of a fraud procedure with the director of education. Additionally, don't put the code of this assingnment online due to the license on the code.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "intSkgKwmUZm"
   },
   "source": [
    "# Introduction\n",
    "Welcome to the third assignment of the Reinforcement Learning course! In this project, you will dive deep into the world of cooperative multi-agent reinforcement learning (MARL). Your mission is to implement, extend, and innovate upon the QMix algorithm to train a team of intelligent agents for the Pacman Capture the Flag challenge. You will control two blue Pacman agents, guiding them to work together to capture food while outsmarting their red opponents.\n",
    "\n",
    "This assignment is divided into three distinct parts:\n",
    "1. **Implement the QMix Algorithm**: You will begin by building the core components of the QMix architecture. This involves creating the individual agent networks and the crucial mixing network that enables centralized training. You will then implement the complete training loop and the QMix loss function to bring your agents to life. The goal here is to establish a solid, working implementation.\n",
    "2. **Specialize in an Advanced Technique**: With a functional QMix agent, each student will choose one specific area to focus on for improvement. This allows you to explore a state-of-the-art MARL topic in depth. Your options will include implementing advanced mixing networks (such as QTran or QPLEX), sophisticated exploration strategies (such as count-based exploration), or enhancing individual agents with techniques from the Rainbow paper.\n",
    "3. **Explore and Innovate**: In the final section, you will build upon the insights gained in Part 2. Based on your experiments, you will be able to identify a promising direction for further improvement and implement it. This is an open-ended challenge where you can explore advanced topics such as policy gradient methods (MAPPO), novel exploration techniques, generalization to random maps, or better observation representations for your agents.\n",
    "\n",
    "\n",
    "## Tournaments: Put Your Agents to the Test\n",
    "Throughout the assignment, you will have the opportunity to test your agents' skills in a series of round-robin tournaments against your peers:\n",
    "\n",
    "Three Intermediate Tournaments: These serve as valuable checkpoints to evaluate\n",
    "your models and refine your training strategies. This are the deadlines for submitting your agents:\n",
    "1.\tMonday 30 November before 10 am.\n",
    "2.\tMonday 7 December before 10 am.\n",
    "3.\tFriday 12 December before 1 pm.\n",
    "\n",
    "One Final Tournament: This determines the ultimate winner! The victorious team will earn a permanent place in the RL course Hall of Fame.\n",
    "\n",
    "All tournaments will be held on the bloxCapture.lay map. For detailed information on submission guidelines and deadlines, please refer to the main assignment document. **It is important to note that tournament results will not impact your grade.** However, we appreciate reflecting in the report based on the video footage of the results of the tournaments in your assignment.\n",
    "\n",
    "By the end of this assignment, you will have gained hands-on experience implementing and experimenting with a powerful MARL algorithm, preparing you to tackle complex multi-agent problems.\n",
    "\n",
    "## The Pacman Environment: [PacMan, a capture the flag variant ](https://ai.berkeley.edu/contest.html)\n",
    "\n",
    "The environment you'll be working in is a cooperative, multi-agent variant of the classic Pacman game. It features multiple agents that must coordinate to achieve a shared objective. Each agent receives its own local observations and must act upon them, making this an ideal testbed for cooperative MARL research.\n",
    "The core game logic is adapted from the Berkeley AI contest, and we encourage you to explore the source code to understand its mechanics. To facilitate training, we have wrapped the game in an interface that follows the popular Gym API, making it straightforward to integrate with your deep reinforcement learning algorithms. The game logic was originally developed at UC Berkeley and is not too hard understand if you want to delve a little bit deeper.\n",
    "\n",
    "The provided code includes a display argument for visualizing gameplay. For the best experience, we recommend running the notebook on a local machine to render the game.\n",
    "\n",
    "A separate document contains a detailed breakdown of the environment, including the observation space, action space, and reward function. Please review it carefully before you begin.\n",
    "\n",
    "## Assignment submission\n",
    "\n",
    "For the final submission, you will provide a report detailing your work across all three parts of the assignment. This report is a critical component of the project, as it allows you to document your journey and showcase what you have learned.\n",
    "\n",
    "We place a high value on reflection. We want to see more than just final results as we want to understand your thought process. Explain why you made certain design choices, what challenges you encountered, and what your experiments (incl. the ones that failed) taught you about the algorithms you implemented. Your insights are just as important as the outcomes.\n",
    "\n",
    "To communicate your findings effectively, please use plots and tables to visualize your results. A clear graph of win rates or training rewards often holds more information than a thousand words. While there is no hard page limit, we encourage you to be concise and clear. Focus on creating a well-structured and insightful analysis of your project.\n",
    "\n",
    "We expect you to submit the following things for the final submission:\n",
    "\n",
    "1. The report\n",
    "2. The notebook\n",
    "3. Any custom code you wrote\n",
    "4. Your final agents for the tournament (in the format mentioned in the assignment description PDF)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2IBmjgcct1f9"
   },
   "source": [
    "\n",
    "# **Setup**\n",
    "\n",
    "Before we dive into coding, let's make sure everything is set up correctly.\n",
    "\n",
    "1. Install Dependencies\n",
    "\n",
    "You'll need to install the following libraries to run the notebook. Run the cell below to install them:\n",
    "\n",
    "*   PacMan Capture the Flag: a reinforcement learning environment.\n",
    "*   Packages you'll use throughout the notebook.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "qoP_9-CHfydw"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mhint: Using 'master' as the name for the initial branch. This default branch name\u001b[m\n",
      "\u001b[33mhint: is subject to change. To configure the initial branch name to use in all\u001b[m\n",
      "\u001b[33mhint: of your new repositories, which will suppress this warning, call:\u001b[m\n",
      "\u001b[33mhint:\u001b[m\n",
      "\u001b[33mhint: \tgit config --global init.defaultBranch <name>\u001b[m\n",
      "\u001b[33mhint:\u001b[m\n",
      "\u001b[33mhint: Names commonly chosen instead of 'master' are 'main', 'trunk' and\u001b[m\n",
      "\u001b[33mhint: 'development'. The just-created branch can be renamed via this command:\u001b[m\n",
      "\u001b[33mhint:\u001b[m\n",
      "\u001b[33mhint: \tgit branch -m <name>\u001b[m\n",
      "\u001b[33mhint:\u001b[m\n",
      "\u001b[33mhint: Disable this message with \"git config set advice.defaultBranchName false\"\u001b[m\n",
      "Initialized empty Git repository in /home/drizzy/Downloads/assignment_3/assignment_3/.git/\n",
      "Cloning into 'rl-lab-3-pacman'...\n",
      "remote: Enumerating objects: 812, done.\u001b[K\n",
      "remote: Counting objects: 100% (124/124), done.\u001b[K\n",
      "remote: Compressing objects: 100% (121/121), done.\u001b[K\n",
      "remote: Total 812 (delta 45), reused 0 (delta 0), pack-reused 688 (from 1)\u001b[K\n",
      "Receiving objects: 100% (812/812), 19.26 MiB | 3.33 MiB/s, done.\n",
      "Resolving deltas: 100% (347/347), done.\n",
      "Note: switching to '49fc996c92b3eb87462fb23ce0d351bc33befd1c'.\n",
      "\n",
      "You are in 'detached HEAD' state. You can look around, make experimental\n",
      "changes and commit them, and you can discard any commits you make in this\n",
      "state without impacting any branches by switching back to a branch.\n",
      "\n",
      "If you want to create a new branch to retain commits you create, you may\n",
      "do so (now or later) by using -c with the switch command. Example:\n",
      "\n",
      "  git switch -c <new-branch-name>\n",
      "\n",
      "Or undo this operation with:\n",
      "\n",
      "  git switch -\n",
      "\n",
      "Turn off this advice by setting config variable advice.detachedHead to false\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!git init\n",
    "!git clone https://student:vaAwWR2Kse-jkAMH2_U5@gitlab.ilabt.imec.be/emalomgr/rl-lab-3-pacman.git --branch student_version\n",
    "!mv ./rl-lab-3-pacman/* ./\n",
    "!rm -rf ./rl-lab-3-pacman/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T1-5e7HR3saX"
   },
   "source": [
    "2. Import and Install Necessary Python Libraries\n",
    "\n",
    "Once the dependencies are installed, import the key libraries you’ll need throughout the notebook:\n",
    "\n",
    "> Remark: if you want to run the notebook on your local machine you'll have to install the packages manually. You can use the `requirements.txt` file from the cloned repository and the [PyTorch documentation](https://pytorch.org/get-started/locally/) to install PyTorch (with CUDA support).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PaNmbYmYrdGq"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[2mResolved \u001b[1m1 package\u001b[0m \u001b[2min 475ms\u001b[0m\u001b[0m                                          \u001b[0m\n",
      "\u001b[2K\u001b[2mPrepared \u001b[1m1 package\u001b[0m \u001b[2min 5.84s\u001b[0m\u001b[0m                                              \n",
      "\u001b[2K\u001b[2mInstalled \u001b[1m1 package\u001b[0m \u001b[2min 28ms\u001b[0m\u001b[0m                                 \u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mnumpy\u001b[0m\u001b[2m==2.3.5\u001b[0m\n",
      "\u001b[2K\u001b[2mResolved \u001b[1m25 packages\u001b[0m \u001b[2min 389ms\u001b[0m\u001b[0m                                        \u001b[0m\n",
      "\u001b[2K\u001b[37m⠧\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/2)                                                   "
     ]
    }
   ],
   "source": [
    "!uv pip install numpy\n",
    "!uv pip install torch\n",
    "!uv pip install matplotlib\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from gymPacMan import gymPacMan_parallel_env\n",
    "\n",
    "device = \"cpu\"\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda:0\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "print(f\"Device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "00mnpkAvlOSB",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "layout_name = 'tinyCapture.lay'                       # see 'layouts/' dir for other options\n",
    "layout_path = os.path.join('layouts', layout_name)\n",
    "env = gymPacMan_parallel_env(layout_file=layout_path, # see class def for options\n",
    "                             display=False,\n",
    "                             reward_forLegalAction=True,\n",
    "                             defenceReward=False,\n",
    "                             length=299,\n",
    "                             enemieName = 'randomTeam',\n",
    "                             self_play=False,\n",
    "                             random_layout = False)\n",
    "env.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wjgyVEbp8sow"
   },
   "source": [
    "# **Section 1: QMix Implementation**\n",
    "\n",
    "In this section, you will implement the QMix algorithm to control agents in the PacMan environment. QMix is a powerful algorithm in multi-agent reinforcement learning that allows for centralized training with decentralized execution (CTDE). The key idea behind QMix is to learn a mixing network that combines individual agent Q-values into a global Q-value, which allows agents to make coordinated decisions while still acting independently during execution. The original QMix paper can be found [here](https://arxiv.org/abs/1803.11485) and will come in handy during implementation of the architecture.\n",
    "\n",
    "## QMix Theory Overview\n",
    "\n",
    "QMix is a value-based multi-agent reinforcement learning algorithm designed for cooperative tasks. It addresses the challenge of decentralized control while maintaining a centralized training framework. The key idea is to learn individual Q-values for each agent and combine them into a global Q-value that represents the team's joint policy.\n",
    "\n",
    "Core Concepts:\n",
    "\n",
    "1.\tIndividual Q-Values: Each agent has a separate Q-network that predicts the Q-values for its actions based on its local observations.\n",
    "2.\tGlobal Q-Value: A mixer network aggregates the individual Q-values into a global Q-value, ensuring that the global Q-value is monotonic with respect to individual Q-values. This monotonicity ensures that maximizing the global Q-value aligns with maximizing the individual Q-values.\n",
    "3.\tHypernetworks: QMix uses hypernetworks to generate the weights for the mixer network dynamically. These weights depend on the global state, allowing the mixer network to adapt its behavior based on the team's overall situation.\n",
    "\n",
    "\n",
    "Step-by-Step implementation: You will be implementing QMix step by step, focusing on the following parts:\n",
    "\n",
    "1.\tImplement the individual agent Q-networks.\n",
    "2.\tBuild the mixing network to combine individual Q-values.\n",
    "3.\tSet up the loss function and training loop.\n",
    "4.\tTrain the agents in the PacMan environment.\n",
    "\n",
    "Let's begin!\n",
    "\n",
    "## 1.1   Agent Q-Network Implementation\n",
    "\n",
    "Before implementing the QMix code, it is essential to have a solid baseline for comparison. You are provided with a working implementation of Independent Q-Learning (IQL). Moreover, you can actually run the notebook and add logging and visualization code (e.g. with WandB) right now, and use the performance of the IQL agent(s) as a reference, since it is guaranteed to reach the maximum score (i.e. the amount of food in the layout) on the tinyCapture layout.\n",
    "\n",
    "**Your First Task**: Your first step is to analyze the the provided code and run the IQL agent and analyze its performance. Train it against random opponents on the smaller maps (tinyCapture.lay and smallCapture.lay). Observe its behavior and log its performance (e.g., win rate, score). This analysis will provide a crucial benchmark that will help you evaluate whether your QMix implementation is an improvement.\n",
    "\n",
    "**Evaluate during training**: Track the learning progress of your agents to check whether they are learning or not. A good tool for this is WandB, where you can easily compare different strategies during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QnE1LUgR0hzY"
   },
   "outputs": [],
   "source": [
    "class AgentQNetwork(nn.Module):\n",
    "    def __init__(self, obs_shape, action_dim, hidden_dim=128):\n",
    "        super(AgentQNetwork, self).__init__()\n",
    "\n",
    "        # Convolutional layers\n",
    "        self.conv1 = nn.Conv2d(obs_shape[0], 16, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv3 = nn.Conv2d(32, 32, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "        conv_output_shape = obs_shape[1] * obs_shape[2] * 32 # assuming obs shape (C, H, W)\n",
    "\n",
    "        # Flatten layer\n",
    "        self.flatten = nn.Flatten()\n",
    "\n",
    "        # Fully connected layers\n",
    "        self.fc1 = nn.Linear(conv_output_shape, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, action_dim)\n",
    "\n",
    "    def forward(self, obs):\n",
    "        # Pass through convolutional layers\n",
    "        x = F.relu(self.conv1(obs))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.relu(self.conv3(x))\n",
    "\n",
    "        # Flatten the output\n",
    "        x = self.flatten(x)\n",
    "\n",
    "        x = F.relu(self.fc1(x))\n",
    "\n",
    "        # Output Q-values\n",
    "        q_values = self.fc2(x)\n",
    "        return q_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_H9qtVQS81ds"
   },
   "source": [
    "\t•\tobs_dim: The dimension of the agent’s local observation.\n",
    "\t•\taction_dim: The number of possible actions the agent can take.\n",
    "\n",
    "## 1.2 Mixing Network\n",
    "\n",
    "The mixing network is responsible for combining the individual Q-values from each agent into a global Q-value. The mixing network ensures that the global Q-value is a monotonic function of each agent’s Q-value, which allows the system to maintain decentralized decision-making at runtime.\n",
    "\n",
    "**Task:** Implement the mixing network.\n",
    "\n",
    "The mixing network will take the Q-values of all agents as input and output a single global Q-value. Plot the results and compare with IQL.\n",
    "\n",
    "Try to optimize the QMix parameters but don't spend too much time on this yet.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1aIiC5gD8yZT"
   },
   "outputs": [],
   "source": [
    "class SimpleQMixer(nn.Module):\n",
    "    def __init__(self, n_agents, state_shape):\n",
    "        super(SimpleQMixer, self).__init__()\n",
    "\n",
    "        # Much simpler state processing\n",
    "\n",
    "        # Single layer mixing network\n",
    "\n",
    "        # Initialize close to equal weights\n",
    "\n",
    "    def forward(self, agent_qs, states):\n",
    "\n",
    "        # Simple positive weights\n",
    "\n",
    "        # Simple weighted sum\n",
    "\n",
    "        return q_tot.view(bs, -1, 1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-OUjn7xq88V6"
   },
   "source": [
    "\t•\tstate_dim: The dimension of the global state (available during centralized training).\n",
    "\t•\tn_agents: The number of agents, which determines the number of Q-values being mixed.\n",
    "\t•\tWeights and biases: The weights and biases of the mixing network depend on the global state, ensuring that different states lead to different weightings of agent Q-values.\n",
    "\n",
    "## 1.3 Loss Function and Training Loop\n",
    "\n",
    "The agents need to learn their Q-values by minimizing the Temporal Difference (TD) error. The loss is computed as the difference between the predicted Q-value (from the agent's Q-network) and the target Q-value (computed using the Bellman equation). Note that the huber loss is used for more stability.\n",
    "\n",
    "**Task:** update the training loop for QMix.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H2c2BYMg84p3"
   },
   "outputs": [],
   "source": [
    "\n",
    "def compute_td_loss(agent_q_networks, target_q_networks, batch, weights=None, gamma=0.99, lambda_=0.1):\n",
    "    \"\"\"\n",
    "    Computes the TD loss for QMix training using the Huber loss.\n",
    "\n",
    "    Args:\n",
    "        agent_q_networks (list): List of Q-networks for each agent.\n",
    "        target_q_networks (list): List of target Q-networks for each agent.\n",
    "        batch (tuple): A batch of experiences (states, actions, rewards, next_states, dones).\n",
    "        weights (torch.Tensor): Importance sampling weights (optional).\n",
    "        gamma (float): Discount factor for future rewards.\n",
    "        lambda_ (float): Regularization factor for stability.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Total loss for training.\n",
    "    \"\"\"\n",
    "    states, actions, rewards, next_states, dones = batch\n",
    "\n",
    "    # Convert to tensors and move to device\n",
    "    states = torch.tensor(states, dtype=torch.float32).to(device)\n",
    "    actions = torch.tensor(actions, dtype=torch.long).to(device)\n",
    "    rewards = torch.tensor(rewards, dtype=torch.float32).to(device)\n",
    "    next_states = torch.tensor(next_states, dtype=torch.float32).to(device)\n",
    "    dones = torch.tensor(dones, dtype=torch.float32).to(device)\n",
    "\n",
    "    # Current Q-values for each agent\n",
    "    agent_q_values = []\n",
    "    for agent_index, q_net in enumerate(agent_q_networks):\n",
    "        q_vals = q_net(states[:, agent_index, :, :, :])  # Get Q-values for each agent\n",
    "        agent_q_values.append(\n",
    "            q_vals.gather(dim=1, index=actions[:, agent_index].unsqueeze(1)))  # Select Q-value for taken action\n",
    "    agent_q_values = torch.cat(agent_q_values, dim=1)  # Shape: (batch_size, n_agents)\n",
    "\n",
    "    # Target Q-values using Double DQN\n",
    "    with torch.no_grad():\n",
    "        # Get actions from current Q-networks\n",
    "        next_agent_q_values = []\n",
    "        for agent_index, (q_net, target_net) in enumerate(zip(agent_q_networks, target_q_networks)):\n",
    "            next_q_vals = q_net(next_states[:, agent_index, :, :, :])  # Get Q-values from current network\n",
    "            max_next_actions = next_q_vals.argmax(dim=1, keepdim=True)  # Greedy actions\n",
    "            target_q_vals = target_net(next_states[:, agent_index, :, :, :])  # Get Q-values from target network\\\n",
    "            max_next_q_vals = target_q_vals.gather(1, max_next_actions)\n",
    "            done_mask = dones[:, 0, 0].unsqueeze(1)\n",
    "            filtered_target_q_vals = max_next_q_vals * (1 - done_mask)\n",
    "\n",
    "            next_agent_q_values.append(filtered_target_q_vals)  # Use target Q-values for selected actions\n",
    "        next_agent_q_values = torch.cat(next_agent_q_values, dim=1)  # Shape: (batch_size, n_agents)\n",
    "\n",
    "    # Independent Q-learning target for each agent (all members of the blue team receive the same reward)\n",
    "    target_q = rewards[:, 0, 0].unsqueeze(1) + gamma * next_agent_q_values\n",
    "\n",
    "    # Compute Huber loss, try also with MSE loss\n",
    "    loss_fn = torch.nn.HuberLoss()\n",
    "\n",
    "    loss_agent1 = loss_fn(agent_q_values[:, 0], target_q[:, 0])\n",
    "    loss_agent2 = loss_fn(agent_q_values[:, 1], target_q[:, 1])\n",
    "\n",
    "    return loss_agent1, loss_agent2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4qb_cFvZ-Bz8"
   },
   "source": [
    "## 1.4 Training the QMix Algorithm\n",
    "\n",
    "Now that you have defined the agent Q-networks, the mixing network, and the loss function, it's time to train the agents in the gym environment. Please note that the given IQL implementation uses soft updates, but feel free to use hard updates.\n",
    "\n",
    "**Task:** Implement the training loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OTtfvR9w-BJQ"
   },
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "import random\n",
    "\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, buffer_size=10_000):\n",
    "        self.buffer = deque(maxlen=buffer_size)\n",
    "\n",
    "    def add(self, experience):\n",
    "        self.buffer.append(experience)\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        experiences = random.sample(self.buffer, batch_size)\n",
    "\n",
    "        # Restructure the batch into separate arrays for states, actions, rewards, next_states, and dones\n",
    "        states = np.array([exp[0].cpu().numpy() for exp in experiences], dtype=np.float32)\n",
    "        actions = np.array([exp[1] for exp in experiences], dtype=np.int64)\n",
    "        rewards = np.array([exp[2] for exp in experiences])\n",
    "        next_states = np.array([exp[3] for exp in experiences])\n",
    "        dones = np.array([exp[4] for exp in experiences])\n",
    "\n",
    "        return states, actions, rewards, next_states, dones\n",
    "\n",
    "    def size(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "\n",
    "def epsilon_greedy_action(agent_q_network, state, epsilon, legal_actions):\n",
    "    if random.random() < epsilon:\n",
    "        # Explore: take a random action\n",
    "        action = random.choice(legal_actions)\n",
    "    else:\n",
    "        state = torch.unsqueeze(state.clone().detach(), 0).to(device)\n",
    "        q_values = agent_q_network(state).cpu().detach().numpy()\n",
    "        action = np.random.choice(np.flatnonzero(q_values == q_values.max()))\n",
    "\n",
    "    return action\n",
    "\n",
    "def update_target_network(agent_q_networks, target_q_networks):\n",
    "    for target, source in zip(target_q_networks, agent_q_networks):\n",
    "        target.load_state_dict(source.state_dict())\n",
    "\n",
    "def soft_update_target_network(agent_q_networks, target_q_networks, tau=0.01):\n",
    "    for target, source in zip(target_q_networks, agent_q_networks):\n",
    "        for target_param, source_param in zip(target.parameters(), source.parameters()):\n",
    "            target_param.data.copy_(\n",
    "                tau * source_param.data + (1 - tau) * target_param.data\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5xN1Mc_I0Ok4"
   },
   "outputs": [],
   "source": [
    "# NOTE: this is currently training Independent IQL and\n",
    "#   it's up to you to make the necessary changes in order\n",
    "#       to obtain the QMix architecture and training loop\n",
    "def train_qmix(env, agent_q_networks, target_q_networks, replay_buffer, n_episodes=500,\n",
    "               batch_size=32, gamma=0.95, lr=0.001):\n",
    "    optimizer = optim.Adam([param for net in agent_q_networks for param in net.parameters()], lr=lr)\n",
    "\n",
    "    epsilon = 1                  # Initial exploration probability\n",
    "    epsilon_min = 0.01\n",
    "    epsilon_decay = 0.99\n",
    "    #target_update_frequency = 5 # Hint: in case you want to use hard updates instead\n",
    "    steps_counter = 0\n",
    "    legal_actions = [0, 1, 2, 3, 4]\n",
    "    info = None\n",
    "    agent_indexes = [1, 3]\n",
    "\n",
    "    for episode in range(n_episodes):\n",
    "        done = {agent_id: False for agent_id in agent_indexes}\n",
    "        env.reset()\n",
    "        blue_player1_reward = 0\n",
    "        blue_player2_reward = 0\n",
    "        score = 0\n",
    "        while not all(done.values()):\n",
    "            actions = [-1 for _, _ in enumerate(env.agents)]\n",
    "            states = []\n",
    "            for i, agent_index in enumerate(agent_indexes):\n",
    "                obs_agent = env.get_Observation(agent_index)\n",
    "                state = torch.tensor(obs_agent, dtype=torch.float32).to(device)\n",
    "                states.append(state)\n",
    "                action = epsilon_greedy_action(agent_q_networks[i],\n",
    "                                               state,\n",
    "                                               epsilon,\n",
    "                                               legal_actions=(lambda: info[\"legal_actions\"][agent_index] if info is not None else legal_actions)()\n",
    "                                               )\n",
    "                actions[agent_index] = action\n",
    "\n",
    "\n",
    "\n",
    "            next_states, rewards, terminations, info = env.step(actions)\n",
    "            score -= info[\"score_change\"]\n",
    "            done = {key: value for key, value in terminations.items() if key in agent_indexes}\n",
    "            blue_player1_reward += rewards[1]\n",
    "            blue_player2_reward += rewards[3]\n",
    "\n",
    "            next_states_converted = []\n",
    "            rewards_converted = []\n",
    "            terminations_converted = []\n",
    "            actions_converted = []\n",
    "\n",
    "            for index in agent_indexes:\n",
    "                next_states_converted.append(list(next_states.values())[index])\n",
    "                rewards_converted.append(rewards[index])\n",
    "                terminations_converted.append(terminations[index])\n",
    "                actions_converted.append(actions[index])\n",
    "\n",
    "            next_states_converted = torch.stack(next_states_converted)\n",
    "            states_converted = torch.stack(states)\n",
    "            rewards_converted = [rewards_converted]\n",
    "            terminations_converted = [terminations_converted]\n",
    "            replay_buffer.add(\n",
    "                (states_converted, actions_converted, rewards_converted, next_states_converted, terminations_converted))\n",
    "\n",
    "            if replay_buffer.size() >= batch_size:\n",
    "                batch = replay_buffer.sample(batch_size)\n",
    "                loss1, loss2 = compute_td_loss(agent_q_networks, target_q_networks, batch,\n",
    "                                               gamma=gamma)\n",
    "                # Zero gradients for all optimizers\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # Backpropagate once for all losses\n",
    "                loss1.backward(retain_graph=True)\n",
    "                loss2.backward()\n",
    "\n",
    "                # Update weights\n",
    "                optimizer.step()\n",
    "\n",
    "                # Soft update after each step\n",
    "                soft_update_target_network(agent_q_networks, target_q_networks)\n",
    "\n",
    "        steps_counter += 1\n",
    "        epsilon = max(epsilon_min, epsilon * epsilon_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DK95aBtt0T6U"
   },
   "outputs": [],
   "source": [
    "n_agents = int(len(env.agents) / 2)\n",
    "action_dim_individual_agent = 5  # North, South, East, West, Stop\n",
    "\n",
    "obs_individual_agent = env.get_Observation(0)\n",
    "obs_shape = obs_individual_agent.shape\n",
    "\n",
    "agent_q_networks = [AgentQNetwork(obs_shape=obs_shape, action_dim=action_dim_individual_agent).to(device) for _ in\n",
    "                    range(n_agents)]\n",
    "target_q_networks = [AgentQNetwork(obs_shape=obs_shape, action_dim=action_dim_individual_agent).to(device) for _ in\n",
    "                     range(n_agents)]\n",
    "\n",
    "# Initialize target Q-networks with the same weights as the main Q-networks\n",
    "update_target_network(agent_q_networks, target_q_networks)\n",
    "\n",
    "# Initialize the replay buffer\n",
    "replay_buffer = ReplayBuffer(buffer_size=10_000)\n",
    "\n",
    "# NOTE: initially, this is just IQL!\n",
    "train_qmix(env, agent_q_networks, target_q_networks, replay_buffer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V3qJ2lOZ-Jk6"
   },
   "source": [
    "## 1.5 Reflection Questions\n",
    "\n",
    "Evaluate your final results on \"smallCapture.lay and bloxCapture.lay (for the tournament)\" against random agents and answer the following questions:\n",
    "\n",
    "*  How do your QMix agents improve over time during the training?\n",
    "*  How does the performance of QMix compare to IQL?\n",
    "*  Do you observe different roles for the agents within a team?\n",
    "*  What other reflection questions can you think of yourself?\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "You've now implemented the QMix algorithm for the PacMan capture the flag environment!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J7hGqk6LKeEg"
   },
   "source": [
    "# Section 2: Specializing in Advanced Techniques\n",
    "With a functional QMix agent from Part 1, it's time to push the boundaries. In this section, each student will choose one of the following specialization tracks to implement a specific, advanced improvement.\n",
    "\n",
    "The goal is to move beyond the basics and tackle some of the core challenges in multi-agent learning. All experiments in this section should be conducted on the larger and more complex bloxCapture.lay map.\n",
    "\n",
    "## 2.1 Improvements\n",
    "\n",
    "Each student must select one of the following three tracks.\n",
    "\n",
    "### Student 1: Advanced Mixing Networks (QTran or QPLEX)\n",
    "**The Challenge**: The standard QMix monotonic mixing network is effective but has representational limitations. More advanced architectures can model more complex team dynamics.\n",
    "\n",
    "**Your Tasks**:\n",
    "- Implement an Advanced Mixer: Choose and implement either QTran or QPLEX, which are more powerful alternatives to the standard QMix mixer.\n",
    "- Integrate and Train: Replace the vanilla mixer in your existing architecture with your new implementation.\n",
    "- Analyze and Compare: Rigorously analyze the performance, training time (time per 100k steps for example), training stability, and sample efficiency of your new agent. In your report, compare it directly against the QMix and IQL baseline from Part 1. Does the more expressive network lead to better coordination and higher win rates and average score?\n",
    "### Student 2: Coordinated Exploration with Count-Based Methods\n",
    "**The Challenge**: In complex environments, agents can easily fail to discover optimal strategies if they don't explore the state space effectively. This is especially true in MARL, where coordinated exploration is key.\n",
    "\n",
    "**Your Tasks**:\n",
    "- Implement a tabular Count-Based exploration bonus: Add an exploration bonus to the agents' rewards using a tabular count-based method. This bonus should be inversely related to how often a particular state has been visited.\n",
    "- Start Simple: Begin by defining the \"state\" for the counting mechanism using only the agent's current position (from the observation's position layer).\n",
    "- Experiment and Expand: Improve upon your initial implementation. Choose at least two expansions for your state definition. For example, you could include:\n",
    "   - The enemy food layer.\n",
    "  - The positions of your teammate.\n",
    "  - The positions of enemies.\n",
    "  - A combination of the above, or another creative idea.\n",
    "- Analyze and Reflect:\n",
    "Investigate the impact of the exploration scaling factor (beta). How does it affect the trade-off between exploration and exploitation? Do you see that your policy now acts suboptimally as exploration behavior is induced into the policy, if so how could you avoid this? In your report, reflect on the effectiveness of your chosen state representations. Did the exploration bonus lead to better coverage of the map and the discovery of new strategies?\n",
    "\n",
    "### Student 3: Enhancing the Individual Agents\n",
    "The Challenge: The performance of any multi-agent system is limited by the capabilities of its individual agents. Techniques from single-agent RL can make each agent smarter and more efficient.\n",
    "\n",
    "**Your Tasks** :\n",
    "- Drawing inspiration from Assignment 1, integrate components into your individual agent Q-networks. We do not expect you to implement every component but rather, we want you to use your insights to choose what works best.\n",
    "- Implement these components within your QMix framework. Tune the hyperparameters to make them work well with the new architecture and environment.\n",
    "- Analyze how these improvements affect agent learning. Do they learn faster? Do they achieve a higher final performance? How do the new components interact with the multi-agent credit assignment problem? How do the components differ in performance compared to the single agent setting?\n",
    "\n",
    "## 2.2 Combine your work\n",
    "Combine your work and try to briefly optimize it. Compare the improvements from each student and show how they work together.\n",
    "\n",
    "## 2.2 Analysis and Reflection\n",
    "In your report for this section, you must provide a detailed analysis of your chosen specialization.\n",
    "\n",
    "1. Describe the approach you implemented. Why did you choose this specific method (e.g., QPLEX over QTran)? What were the key steps in your implementation?\n",
    "\n",
    "2. Present clear evidence of your agent's performance. Use plots (e.g., win rate vs. episodes, rewards, loss curves) and tables to compare your improved agent against the vanilla QMix (and IQL) baseline. Is it better? Is it more stable? Does it learn faster? Why is this the case?\n",
    "\n",
    "3. Analyze your agent's behavior. Did it learn the strategies you expected? What are its remaining limitations? If you had more time, what would you try next to overcome these issues?\n",
    "\n",
    "4. What were the biggest challenges you faced during implementation and training? What key insights did you gain about your chosen topic and about MARL in general?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eGslIWIeMWSS"
   },
   "source": [
    "# **Section 3: Experimenting with Your Own Improvements**\n",
    "\n",
    "This section is an open challenge; based on the previous section, choose what you want to improve in your algorithm. The goal is for you to explore different techniques and report on your findings. We encourage you to try out different things. You can go really in depth into one point but make sure to show us that you put a lot of effort into it. You will implement and test this algorithm on the Pacman Capture the Flag environment on the map \"bloxCapture.lay\".\n",
    "\n",
    "This section is open-ended, allowing you to experiment and think critically about the challenges and opportunities in multi-agent learning.\n",
    "## 3.1 Suggested Directions\n",
    "\n",
    "Here are some ideas to get you started. You may choose one of these or propose a completely new direction:\n",
    "1.\tPolicy Gradient Approaches\n",
    "* Implement a multi-agent Proximal Policy Optimization (PPO) or Actor-Critic algorithm.\n",
    "* How do policy gradient methods handle coordination between agents compared to value-based methods like QMix?\n",
    "2.\tCounterfactual Multi-Agent Policy Gradients (COMA)\n",
    "* Explore COMA, which uses counterfactual baselines to address the credit assignment problem.\n",
    "* How does COMA adjust the contribution of each agent to the team’s reward?\n",
    "3.\tModifications to the coordination\n",
    "* Experiment with a different mixer architectures or other coordination approaches.\n",
    "4. Change the observation space\n",
    "* Try to find a better representation of the environment as this could speed up and improve learning.\n",
    "4. Explore if your method works on random maps\n",
    "* Try to train you method on random maps and see if you method can learn a strategy that transfers to new maps.\n",
    "* Reflect on whether your agents can learn and possibly try to implement something to make it work better\n",
    "5. Go deeper into exploration methods\n",
    "* Until now, you used a tabular version of Count-Based exploration. Now explore generalizable methods for which you are allowed to use the package [RLeXplore](https://github.com/RLE-Foundation/RLeXplore) and the code can be imported [here](https://github.com/RLE-Foundation/rllte).\n",
    "* Possibly think about heurstic reward shaping methods that teach certain behavior. However, keep in mind that this can have unintended outcomes.\n",
    "* Do you see a difference when using Potential-Based Reward Shaping? For implementation check the assinment presentation during class.\n",
    "6. Create a training strategy\n",
    "* Last year we didn't see any real improvements with curriculum learning or self play, but maybe you are able to solve it.\n",
    "7. Have an idea for something else, go right ahead!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sajuh8nfMkTW"
   },
   "source": [
    "## 3.2 Reflection Questions\n",
    "\n",
    "After implementing your chosen algorithm, reflect on the following:\n",
    "\n",
    "1. Design choices\n",
    "* Which things did you implement, and why?\n",
    "* Did you get the results you expected?\n",
    "\n",
    "2.\tPerformance\n",
    "* How does your algorithm perform compared to standard QMix and your work from section 2?\n",
    "\n",
    "3.\tStrengths and Weaknesses\n",
    "* What are the strengths of your chosen approach in the multi-agent Pacman environment?\n",
    "* What are the weaknesses or challenges you encountered?\n",
    "\n",
    "4.\tCoordination\n",
    "* Did your algorithm encourage better coordination between agents? Why or why not?\n",
    "\n",
    "5.\tGeneralization\n",
    "* How well do you think your algorithm generalizes to random maps? Did you have to change something to make this work?\n",
    "\n",
    "6. Future work\n",
    "* if you had more time what would you improve or implement and why?\n",
    "\n",
    "What other reflection questions can you think of yourself?\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AkRulQ_cMn-g"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
